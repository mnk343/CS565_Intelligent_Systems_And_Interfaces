{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CS565_Assignment_1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "VoJYNZVcK3LM"
      },
      "source": [
        "from google.colab import drive\n",
        "import nltk\n",
        "# nltk.download('all')\n",
        "file = open('corpus/en_wiki.txt')\n",
        "write_sentence_file_object = open('/content/drive/My Drive/results/english_sentences_approach_1.txt', \"w\")\n",
        "write_word_file_object = open('/content/drive/My Drive/results/english_words_approach_1.txt', \"w\")\n",
        "\n",
        "corpus_text = file.read()\n",
        "sentence_tokenize_english = nltk.tokenize.sent_tokenize(corpus_text)\n",
        "\n",
        "for sentence in sentence_tokenize_english:\n",
        "  write_sentence_file_object.write(sentence)\n",
        "  write_sentence_file_object.write(\"\\n\")\n",
        "\n",
        "  word_tokenize_english = nltk.tokenize.TreebankWordTokenizer().tokenize(sentence)\n",
        "  for word in word_tokenize_english:\n",
        "    write_word_file_object.write(word)\n",
        "    write_word_file_object.write(\"\\n\")\n",
        "\n",
        "write_word_file_object.close()\n",
        "write_sentence_file_object.close()\n",
        "file.close()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oGjN_37KCfj2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7r5QfCyFURM4"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fTwdFkKENJgq"
      },
      "source": [
        "import nltk\n",
        "file = open('corpus/en_wiki.txt')\n",
        "write_sentence_file_object = open('/content/drive/My Drive/results/english_sentences_approach_2.txt', \"w\")\n",
        "write_word_file_object = open('/content/drive/My Drive/results/english_words_approach_2.txt', \"w\")\n",
        "\n",
        "corpus_text = file.read()\n",
        "tokenizer = nltk.data.load('tokenizers/punkt/PY3/english.pickle') \n",
        "sentence_tokenize_english = tokenizer.tokenize(corpus_text) \n",
        "\n",
        "for sentence in sentence_tokenize_english:\n",
        "  write_sentence_file_object.write(sentence)\n",
        "  write_sentence_file_object.write(\"\\n\")\n",
        "\n",
        "  word_tokenize_english = nltk.tokenize.WordPunctTokenizer().tokenize(sentence)\n",
        "  for word in word_tokenize_english:\n",
        "    write_word_file_object.write(word)\n",
        "    write_word_file_object.write(\"\\n\")\n",
        "\n",
        "write_word_file_object.close()\n",
        "write_sentence_file_object.close()\n",
        "file.close()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TLoMD2Vldt8Q"
      },
      "source": [
        "import spacy\n",
        "# nltk.download('all')\n",
        "file = open('corpus/en_wiki.txt')\n",
        "write_sentence_file_object = open('/content/drive/My Drive/results/english_sentences_spaCy_approach_1.txt', \"w\")\n",
        "write_word_file_object = open('/content/drive/My Drive/results/english_words_spaCy_approach_1.txt', \"w\")\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "\n",
        "for line in file:\n",
        "  line = line.rstrip()\n",
        "  sentence_tokenize_english = nlp(line)\n",
        "  for sentence in sentence_tokenize_english.sents:\n",
        "      sentence = str(sentence)\n",
        "      sentence = sentence.rstrip()\n",
        "      write_sentence_file_object.write(sentence)\n",
        "      write_sentence_file_object.write(\"\\n\")\n",
        "\n",
        "      word_tokenize_english = nlp(sentence)\n",
        "      for word in word_tokenize_english:\n",
        "        word = str(word)\n",
        "        if word != \"\":\n",
        "          write_word_file_object.write(word)\n",
        "          write_word_file_object.write(\"\\n\")\n",
        "\n",
        "write_word_file_object.close()\n",
        "write_sentence_file_object.close()\n",
        "file.close()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-7dS_9SFy1eb"
      },
      "source": [
        "# english custom\n",
        "\n",
        "known_abbreviations = [\"dr\" , \"mr\" , \"mrs\" , \"vs\" , \"etc\" , \"jr\" , \"sr\", \"prof\"]\n",
        "def check_if_known_abbreviation(sentence):\n",
        "  last_word = sentence.split()[-1]\n",
        "  last_word = last_word.lower()\n",
        "  last_word = last_word[:-1]\n",
        "\n",
        "  for abbr in known_abbreviations:\n",
        "    if last_word == abbr:\n",
        "      return True\n",
        "  return False\n",
        "\n",
        "def tokenize_words_from_sentence(sentence , file):\n",
        "  word = \"\"\n",
        "  is_single_quotes = 0\n",
        "  sentence.rstrip()\n",
        "  \n",
        "  for char in sentence:\n",
        "    if is_single_quotes == 1:\n",
        "      if char != ' ':\n",
        "        word += char\n",
        "        file.write(word)\n",
        "        file.write(\"\\n\")\n",
        "\n",
        "      else:\n",
        "        file.write(char)\n",
        "        file.write(\"\\n\")\n",
        "      \n",
        "      word = \"\"\n",
        "      is_single_quotes = 0\n",
        "      continue\n",
        "    is_single_quotes = 0\n",
        "    if char == \" \":\n",
        "      word = word.strip()\n",
        "\n",
        "      if( word != \"\" ):\n",
        "        file.write(word)\n",
        "        file.write(\"\\n\") \n",
        "      word = \"\"\n",
        "      continue\n",
        "    \n",
        "    char = char.lower()\n",
        "    if char == '\\'':\n",
        "      if word != \"\":\n",
        "        file.write(word)\n",
        "        file.write(\"\\n\") \n",
        "      word = \"'\"\n",
        "      is_single_quotes = 1\n",
        "    \n",
        "    elif char == '.':\n",
        "      word += char\n",
        "\n",
        "      if check_if_known_abbreviation(word):\n",
        "        file.write(word)\n",
        "        file.write(\"\\n\") \n",
        "      else:\n",
        "        word = word[:-1]\n",
        "        word.split()\n",
        "        if word != \"\":\n",
        "          file.write(word)\n",
        "          file.write(\"\\n\") \n",
        "        file.write(char)\n",
        "        file.write(\"\\n\") \n",
        "      word = \"\"\n",
        "\n",
        "    elif char == ',' or char=='?' or char == ';' or char == '!' or char == '\"':\n",
        "      if word != \"\":\n",
        "        file.write(word)\n",
        "        file.write(\"\\n\") \n",
        "      file.write(char)\n",
        "      file.write(\"\\n\") \n",
        "      word = \"\"\n",
        "\n",
        "    else:\n",
        "      word += char\n",
        "  \n",
        "  if word != \"\":\n",
        "      file.write(word)\n",
        "      file.write(\"\\n\") \n",
        "\n",
        "file = open('corpus/en_wiki.txt')\n",
        "write_sentence_file_object = open('/content/drive/My Drive/results/english_sentences_custom.txt', \"w\")\n",
        "write_word_file_object = open('/content/drive/My Drive/results/english_words_custom.txt', \"w\")\n",
        "\n",
        "for line in file:\n",
        "  line = line.rstrip()\n",
        "  sentence = \"\"\n",
        "  count_quotes = 0\n",
        "  \n",
        "  for char in line:\n",
        "    sentence += char  \n",
        "    if count_quotes == 0 and char == '\"':\n",
        "      count_quotes += 1\n",
        "\n",
        "    elif count_quotes != 0 and char=='\"':\n",
        "      count_quotes -= 1\n",
        "   \n",
        "    elif count_quotes == 0 and char == '.' and check_if_known_abbreviation(sentence):\n",
        "      continue\n",
        "\n",
        "    elif not (count_quotes != 0) and not (char != '.' and char != '!' and char != '?' and char != ':'):\n",
        "      sentence = sentence.strip()\n",
        "      write_sentence_file_object.write(sentence)\n",
        "      write_sentence_file_object.write(\"\\n\")\n",
        "      tokenize_words_from_sentence(sentence , write_word_file_object )\n",
        "      sentence = \"\"\n",
        "      \n",
        "  sentence = sentence.strip()\n",
        "  if sentence != \"\":\n",
        "    write_sentence_file_object.write(sentence)\n",
        "    write_sentence_file_object.write(\"\\n\")\n",
        "    print(sentence)\n",
        "    tokenize_words_from_sentence(sentence , write_word_file_object )\n",
        "    sentence = \"\"\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A20VXrc7FV6o"
      },
      "source": [
        "# hindi custom\n",
        "\n",
        "def tokenize_words_from_sentence(sentence , file):\n",
        "  word = \"\"\n",
        "  sentence.rstrip()\n",
        "  \n",
        "  for char in sentence:\n",
        "\n",
        "    if char == \" \":\n",
        "      word = word.strip()\n",
        "\n",
        "      if( word != \"\" ):\n",
        "        file.write(word)\n",
        "        file.write(\"\\n\") \n",
        "      word = \"\"\n",
        "      continue\n",
        "    \n",
        "    elif char=='(' or char == ')' or char== '.' or char == ',' or char=='?' or char == ';' or char == '!' or char == '\"' or char == '|' or char == '\\'' or char == '%' or char == '$':\n",
        "      word = word.strip()\n",
        "      if word != \"\":\n",
        "        file.write(word)\n",
        "        file.write(\"\\n\") \n",
        "      file.write(char)\n",
        "      file.write(\"\\n\") \n",
        "      word = \"\"\n",
        "\n",
        "    else:\n",
        "      word += char\n",
        "  \n",
        "  if word != \"\":\n",
        "      file.write(word)\n",
        "      file.write(\"\\n\") \n",
        "\n",
        "file = open('hi_wiki.txt')\n",
        "write_sentence_file_object = open('/content/drive/My Drive/results/hindi_sentences_custom.txt', \"w\")\n",
        "write_word_file_object = open('/content/drive/My Drive/results/hindi_words_custom.txt', \"w\")\n",
        "\n",
        "for line in file:\n",
        "  line = line.rstrip()\n",
        "  sentence = \"\"\n",
        "  count_quotes = 0\n",
        "  \n",
        "  for char in line:\n",
        "    sentence += char  \n",
        "    if count_quotes == 0 and char == '\"':\n",
        "      count_quotes += 1\n",
        "\n",
        "    elif count_quotes != 0 and char=='\"':\n",
        "      count_quotes -= 1\n",
        "\n",
        "    elif not (count_quotes != 0) and not (char != '|' and char != '!' and char != '?' and char != ':'):\n",
        "      sentence = sentence.strip()\n",
        "      write_sentence_file_object.write(sentence)\n",
        "      write_sentence_file_object.write(\"\\n\")\n",
        "      tokenize_words_from_sentence(sentence , write_word_file_object )\n",
        "      sentence = \"\"\n",
        "      \n",
        "  sentence = sentence.strip()\n",
        "  if sentence != \"\":\n",
        "    write_sentence_file_object.write(sentence)\n",
        "    write_sentence_file_object.write(\"\\n\")\n",
        "    print(sentence)\n",
        "    tokenize_words_from_sentence(sentence , write_word_file_object )\n",
        "    sentence = \"\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MzI3Czw3m9Ty"
      },
      "source": [
        "!pip install indic-nlp-library\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1OkvI7x9nI1J"
      },
      "source": [
        "from indicnlp.tokenize import sentence_tokenize, indic_tokenize\n",
        "\n",
        "file = open('corpus/hi_wiki.txt')\n",
        "write_sentence_file_object = open('/content/drive/My Drive/results/hindi_sentences_indicnlp.txt', \"w\")\n",
        "write_word_file_object = open('/content/drive/My Drive/results/hindi_words_indicnlp.txt', \"w\")\n",
        "\n",
        "sentences=sentence_tokenize.sentence_split(indic_string, lang='hi')\n",
        "\n",
        "for line in file:\n",
        "  sentence_tokenize_hindi = sentence_tokenize.sentence_split(line, lang='hi')\n",
        "\n",
        "  for sentence in sentence_tokenize_hindi:\n",
        "    write_sentence_file_object.write(sentence)\n",
        "    write_sentence_file_object.write(\"\\n\")\n",
        "\n",
        "    word_tokenize_hindi = indic_tokenize.trivial_tokenize(sentence)\n",
        "    for word in word_tokenize_hindi:\n",
        "      write_word_file_object.write(word)\n",
        "      write_word_file_object.write(\"\\n\")\n",
        "\n",
        "write_word_file_object.close()\n",
        "write_sentence_file_object.close()\n",
        "file.close()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mpBQllKnxnIe"
      },
      "source": [
        "**New** **section**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-BCzRR40tx8I"
      },
      "source": [
        "!pip install stanfordnlp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CywQGuJYukBP"
      },
      "source": [
        "!pip install stanfordnlp \n",
        "import stanfordnlp\n",
        "\n",
        "stanfordnlp.download('hi')\n",
        "\n",
        "hindi_doc = nlp(\"\"\"केंद्र की मोदी सरकार ने शुक्रवार को अपना अंतरिम बजट पेश किया. कार्यवाहक वित्त मंत्री पीयूष गोयल ने अपने बजट में किसान, मजदूर, करदाता, महिला वर्ग समेत हर किसी के लिए बंपर ऐलान किए. हालांकि, बजट के बाद भी टैक्स को लेकर काफी कन्फ्यूजन बना रहा. केंद्र सरकार के इस अंतरिम बजट क्या खास रहा और किसको क्या मिला, आसान भाषा में यहां समझें\"\"\")\n",
        "\n",
        "nlp = stanfordnlp.Pipeline(processors = \"tokenize\")\n",
        "\n",
        "for sentence in hindi_doc.sentences:\n",
        "  print (sentence)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T48ZZva2Ckcb"
      },
      "source": [
        "# frequency distribution\n",
        "!pip install snowballstemmer\n",
        "import snowballstemmer\n",
        "import numpy as np \n",
        "from prettytable import PrettyTable\n",
        "import matplotlib.pyplot as plt  \n",
        "from nltk.stem import PorterStemmer \n",
        "stemmer = PorterStemmer() \n",
        "hindi_stemmer = snowballstemmer.HindiStemmer()\n",
        "is_file_english = 1\n",
        "\n",
        "val = input(\"Enter input file : \")\n",
        "n = input(\"Enter the value of n (1 for unigram, 2 for bigram, 3 for trigram): \")\n",
        "n = int(n)\n",
        "is_stem = input(\"Enter Y to enable stemming: \")\n",
        "is_stem = is_stem.lower()\n",
        "if( is_stem == \"y\"):\n",
        "  is_stem = 1\n",
        "else:\n",
        "  is_stem = 0 \n",
        "if val[0] == 'h':\n",
        "  is_file_english = 0\n",
        "path_to_file = \"/content/drive/My Drive/results/\" + val\n",
        "file = write_word_file_object = open(path_to_file, \"r\")\n",
        "\n",
        "frequency_of_frequency = {}\n",
        "for number in range(1,14):\n",
        "  frequency_of_frequency[number] = 0\n",
        "\n",
        "def generate_n_grams(n , list_ngrams):\n",
        "  ngram = \"\"\n",
        "  ctr = 1\n",
        "  for line in file:\n",
        "    line = line.rstrip()\n",
        "    if line == \",\" or line == \":\" or line == \"?\" or line == \".\" or line == \"'\" or line == \"\\\"\" or line ==\"(\" or line == \")\":\n",
        "      continue\n",
        "\n",
        "    ngram += line\n",
        "    ngram += \" \"\n",
        "\n",
        "    if ctr < n:\n",
        "      ctr += 1\n",
        "    else:\n",
        "      list_ngrams.append(ngram.strip())\n",
        "      ngram = ngram.split(' ',1)[1]\n",
        "\n",
        "def plot_frequency_distribution ( list_ngrams, is_stem ):\n",
        "  frequency = {} \n",
        "  rank = {}\n",
        "  global is_file_english\n",
        "  for ngram in list_ngrams:\n",
        "    if is_stem == 1:\n",
        "      if is_file_english == 0 :\n",
        "        temp = ngram.split()\n",
        "        for i in range(len(temp)):\n",
        "          word = hindi_stemmer.stemWord(temp[i])\n",
        "          temp[i] = word\n",
        "        ngram = \" \".join(temp)\n",
        "      \n",
        "      else:\n",
        "        temp = ngram.split()\n",
        "        for i in range(len(temp)):\n",
        "          word = stemmer.stem(temp[i])\n",
        "          temp[i] = word\n",
        "        ngram = \" \".join(temp)\n",
        "      \n",
        "    if ngram in frequency:\n",
        "      frequency[ngram] += 1\n",
        "    else:\n",
        "      frequency[ngram] = 1\n",
        "    \n",
        "  rank_ngrams = sorted(frequency.items(), key = \n",
        "                      lambda kv:(kv[1], kv[0]) , reverse = True)\n",
        "\n",
        "  ctr = 1\n",
        "  data_for_plot = {}\n",
        "\n",
        "  for pair in rank_ngrams:\n",
        "    if (pair[1] < 11):\n",
        "      frequency_of_frequency[pair[1]] += 1\n",
        "    elif pair[1] >= 11 and pair[1] <= 50:\n",
        "      frequency_of_frequency[11] += 1\n",
        "    elif pair[1] >= 51 and pair[1] <= 100:\n",
        "      frequency_of_frequency[12] += 1\n",
        "    else:\n",
        "      frequency_of_frequency[13] += 1\n",
        "    \n",
        "    rank[pair[0]] = ctr\n",
        "    \n",
        "    if ctr < 100 :\n",
        "      data_for_plot[ctr] = pair[1]\n",
        "\n",
        "    ctr += 1\n",
        "\n",
        "  ranks = list(data_for_plot.keys()) \n",
        "  values = list(data_for_plot.values()) \n",
        "  figure = plt.figure(figsize = (10, 5)) \n",
        "\n",
        "  plt.bar(ranks, values, color ='coral', width=1) \n",
        "  plt.xlabel(\"Ranks of ngrams\") \n",
        "  plt.ylabel(\"Frequency\") \n",
        "  plt.title(\"Frequency distribution of ngrams with top 100 ranks\") \n",
        "  plt.show() \n",
        "\n",
        "  table = PrettyTable()\n",
        "  column_1 = [\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"11-50\",\"51-100\",\">100\"]\n",
        "  table.field_names = [\"Word Frequency\", \"Frequency of frequency\"]\n",
        "  for i in range(0,13):\n",
        "    table.add_row([column_1[i] , frequency_of_frequency[i+1]])\n",
        "  table.align[\"Word Frequency\"] = \"r\"\n",
        "\n",
        "  print(table)\n",
        "\n",
        "  table = PrettyTable()\n",
        "\n",
        "  table.field_names = [\"Word\", \"Frequency\"]\n",
        "  for i in range(0,min(100,len(rank_ngrams))):\n",
        "    table.add_row([rank_ngrams[i][0] ,rank_ngrams[i][1] ])\n",
        "  print(table)\n",
        "\n",
        "  print(\"Most frequent word: \" + rank_ngrams[0][0] + \" with frequency of \" + str(rank_ngrams[0][1] ))\n",
        "  print(\"Least frequent word: \" + rank_ngrams[-1][0] + \" with frequency of \" + str(rank_ngrams[-1][1]) )\n",
        "ngram_list = [] \n",
        "\n",
        "generate_n_grams(n, ngram_list)\n",
        "\n",
        "frequency_ngram_list = {}\n",
        "rank_ngram_list = {}\n",
        "\n",
        "plot_frequency_distribution(ngram_list , is_stem)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xxW3I_AerOyZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XDE8-h0tDvSe"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K9bNTjulDL5X",
        "outputId": "426d7121-8e41-4ef7-882e-0c5fffca7cb3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# likelihood testing\n",
        "import math\n",
        "# len_corpus = 10000\n",
        "\n",
        "val = input(\"Enter input file : \")\n",
        "path_to_file = \"/content/drive/My Drive/results/\" + val\n",
        "file = open(path_to_file, \"r\")\n",
        "filename = \"likelihood_ratio_test\";\n",
        "if( val[0] == 'e' ):\n",
        "  filename += \"english.txt\"\n",
        "else:\n",
        "  filename += \"hindi.txt\"\n",
        "\n",
        "write_word_file_object = open('/content/drive/My Drive/results/' + filename , \"w\")\n",
        "\n",
        "def auxiliary_value( k , n , x):\n",
        "  return math.pow(1-x, n-k) * math.pow(x,k) \n",
        "\n",
        "# print( auxiliary_value( 170 - 1 , 8972 - 1 ,0.018947837717342843  ),10 )\n",
        "\n",
        "def perform_likelihood_ratio_test( collocations_with_confidence , unigram_list , bigram_list, number_unigrams ):\n",
        "\n",
        "  for bigram, frequency in bigram_list.items(): \n",
        "    bigram = bigram.rstrip()\n",
        "    unigrams = bigram.split()\n",
        "    if( len(unigrams) < 2):\n",
        "      continue\n",
        "    first_unigram = unigrams[0]\n",
        "    second_unigram = unigrams[1]\n",
        "    if second_unigram not in unigram_list or first_unigram not in unigram_list:\n",
        "      continue\n",
        "\n",
        "    p = unigram_list[second_unigram] / number_unigrams\n",
        "    p1 = frequency / unigram_list[first_unigram]\n",
        "    p2 = ( unigram_list[second_unigram] - frequency ) / ( number_unigrams - unigram_list[first_unigram] )\n",
        "    # print(unigram_list[second_unigram])\n",
        "    # print(frequency)\n",
        "    # print(number_unigrams)\n",
        "    # print(unigram_list[first_unigram])\n",
        "    # print(p)\n",
        "    # print(\"\\n\")\n",
        "    val = auxiliary_value( frequency ,unigram_list[first_unigram] , p  ) \n",
        "    if val <= 0 :\n",
        "      val = 1\n",
        "      continue\n",
        "    confidence_value = math.log( val ,10)\n",
        "\n",
        "    val = auxiliary_value ( unigram_list[second_unigram] - frequency  , number_unigrams- unigram_list[first_unigram] , p )\n",
        "    if val <= 0 :\n",
        "      val = 1\n",
        "      continue\n",
        "    confidence_value += math.log( val ,10)\n",
        "    \n",
        "    val = auxiliary_value (  frequency , unigram_list[first_unigram] , p1 ) \n",
        "    if val <= 0 :\n",
        "      val = 1\n",
        "      continue\n",
        "    # print(val)\n",
        "    confidence_value -= math.log( val ,10)\n",
        "    \n",
        "    val = auxiliary_value (  unigram_list[second_unigram] - frequency, number_unigrams- unigram_list[first_unigram] , p2 ) \n",
        "    if val <= 0 :\n",
        "      val = 1\n",
        "      continue\n",
        "    \n",
        "    confidence_value -= math.log( val ,10)\n",
        "\n",
        "    confidence_value *= -2\n",
        "    collocations_with_confidence[bigram] = confidence_value\n",
        "    #  log(x) = log L(c12, c1, p) + \n",
        "    #  log L(c2  c12, N - c1, p) - \n",
        "    #  log L(c12 , c1 ,  p1) - \n",
        "    #  log L(c2-c12 , N - c1 , p2)\n",
        "\n",
        "    #  L(k, n, x) = x^k * (1-x)^(n-k)\n",
        "    #  p = c2 / N\n",
        "    #  p1 = c12 / c1\n",
        "    #  p2 = ( c2 - c12 ) / ( N - c1 )\n",
        "\n",
        "def generate_n_grams(n , list_ngrams):\n",
        "  ngram = \"\"\n",
        "  ctr = 1\n",
        "  count_of_words = 0\n",
        "\n",
        "  for line in file:\n",
        "    count_of_words += 1\n",
        "    \n",
        "    line = line.rstrip()\n",
        "    if line == \",\" or line == \":\" or line == \"?\" or line == \".\" or line == \"'\" or line == \"\\\"\" or line ==\"(\" or line == \")\":\n",
        "      continue\n",
        "\n",
        "    ngram += line\n",
        "    ngram += \" \"\n",
        "\n",
        "    if ctr < n:\n",
        "      ctr += 1\n",
        "    else:\n",
        "      list_ngrams.append(ngram.strip())\n",
        "      ngram = ngram.split(' ',1)[1]\n",
        "    # if count_of_words >= len_corpus:\n",
        "    #   break\n",
        "\n",
        "def frequency_ngram ( list_ngrams, frequency_list ):\n",
        "\n",
        "  rank = {}\n",
        "  for ngram in list_ngrams:\n",
        "    if ngram in frequency_list:\n",
        "      frequency_list[ngram] += 1\n",
        "    else:\n",
        "      frequency_list[ngram] = 1\n",
        "    \n",
        "  rank_ngrams = sorted(frequency_list.items(), key = \n",
        "                      lambda kv:(kv[1], kv[0]) , reverse = True)\n",
        "\n",
        "  ctr = 1\n",
        "  for pair in rank_ngrams:\n",
        "    rank[pair[0]] = ctr\n",
        "    ctr += 1\n",
        "\n",
        "unigram_list = []\n",
        "generate_n_grams(1, unigram_list)\n",
        "file.seek(0,0)\n",
        "\n",
        "bigram_list = []\n",
        "generate_n_grams(2, bigram_list)\n",
        "\n",
        "frequency_unigram_list = {}\n",
        "frequency_bigram_list = {}\n",
        "\n",
        "frequency_ngram(unigram_list , frequency_unigram_list)\n",
        "frequency_ngram(bigram_list , frequency_bigram_list)\n",
        "\n",
        "collocations_with_confidence = {}\n",
        "perform_likelihood_ratio_test( collocations_with_confidence, frequency_unigram_list , frequency_bigram_list , len(unigram_list) )\n",
        "\n",
        "collocation = sorted(collocations_with_confidence.items(), key = \n",
        "                      lambda kv:(kv[1], kv[0]) , reverse = True)\n",
        "\n",
        "count_collocations = 0\n",
        "for pair in collocation: \n",
        "    if( pair[1] > 50 ):\n",
        "      count_collocations += 1\n",
        "      write_word_file_object.write(str(pair[0]) + \" \" + str(pair[1]))\n",
        "      write_word_file_object.write(\"\\n\")\n",
        "\n",
        "print( \"NUMBER OF BIGRAMS: \" + str(len(bigram_list)))\n",
        "print( \"NUMBER OF COLLOCATIONS: \" + str(count_collocations))\n",
        "print( \"RATIO: \" +  str(count_collocations/len(bigram_list)) )\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter input file : hindi_words_custom.txt\n",
            "NUMBER OF BIGRAMS: 7601232\n",
            "NUMBER OF COLLOCATIONS: 2882\n",
            "RATIO: 0.00037914906425695204\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o2y9Ni1YYsw3",
        "outputId": "797be4f3-1b53-4228-a4d3-75a2481db8b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "# question 2 to find coverage\n",
        "\n",
        "import snowballstemmer\n",
        "from nltk.stem import PorterStemmer \n",
        "\n",
        "stemmer = PorterStemmer() \n",
        "hindi_stemmer = snowballstemmer.HindiStemmer()\n",
        "\n",
        "is_file_english = 1\n",
        "\n",
        "val = input(\"Enter input file : \")\n",
        "path_to_file = \"/content/drive/My Drive/results/\" + val\n",
        "file = open(path_to_file, \"r\")\n",
        "\n",
        "is_stem = input(\"Enter Y to enable stemming: \")\n",
        "is_stem = is_stem.lower()\n",
        "if is_stem == 'y':\n",
        "  is_stem = 1\n",
        "else:\n",
        "  is_stem = 0\n",
        "\n",
        "def generate_n_grams(n , list_ngrams):\n",
        "  ngram = \"\"\n",
        "  ctr = 1\n",
        "  count_of_words = 0\n",
        "\n",
        "  for line in file:\n",
        "    count_of_words += 1\n",
        "    \n",
        "    line = line.rstrip()\n",
        "    if line == \",\" or line == \":\" or line == \"?\" or line == \".\" or line == \"'\" or line == \"\\\"\" or line ==\"(\" or line == \")\":\n",
        "      continue\n",
        "\n",
        "    ngram += line\n",
        "    ngram += \" \"\n",
        "\n",
        "    if ctr < n:\n",
        "      ctr += 1\n",
        "    else:\n",
        "      list_ngrams.append(ngram.strip())\n",
        "      ngram = ngram.split(' ',1)[1]\n",
        "    \n",
        "def frequency_ngram ( list_ngrams, frequency_list ):\n",
        "  rank = {}\n",
        "  for ngram in list_ngrams:\n",
        "    if is_stem == 1:\n",
        "      if is_file_english == 0 :\n",
        "        temp = ngram.split()\n",
        "        for i in range(len(temp)):\n",
        "          word = hindi_stemmer.stemWord(temp[i])\n",
        "          temp[i] = word\n",
        "        ngram = \" \".join(temp)\n",
        "\n",
        "      else:\n",
        "        temp = ngram.split()\n",
        "        for i in range(len(temp)):\n",
        "          word = stemmer.stem(temp[i])\n",
        "          temp[i] = word\n",
        "        ngram = \" \".join(temp)\n",
        "\n",
        "    if ngram in frequency_list:\n",
        "      frequency_list[ngram] += 1\n",
        "    else:\n",
        "      frequency_list[ngram] = 1\n",
        "    \n",
        "  rank_ngrams = sorted(frequency_list.items(), key = \n",
        "                      lambda kv:(kv[1], kv[0]) , reverse = True)\n",
        "\n",
        "  ctr = 1\n",
        "  for pair in rank_ngrams:\n",
        "    rank[pair[0]] = ctr\n",
        "    ctr += 1\n",
        "\n",
        "def find_coverage( ngram_list , frequency_ugram_list , percentage ):\n",
        "  rank_ngrams = sorted(frequency_ugram_list.items(), key = \n",
        "                      lambda kv:(kv[1], kv[0]) , reverse = True)\n",
        "  sum = 0\n",
        "  number_of_ngrams = 0\n",
        "  percentage /= 100\n",
        "  \n",
        "  for pair in rank_ngrams:\n",
        "    sum += pair[1]\n",
        "    coverage_so_far = sum / len(ngram_list)\n",
        "    number_of_ngrams += 1\n",
        "\n",
        "    if coverage_so_far >= percentage :\n",
        "      return str(number_of_ngrams)\n",
        "    \n",
        "unigram_list = []\n",
        "generate_n_grams(1, unigram_list)\n",
        "file.seek(0,0)\n",
        "\n",
        "bigram_list = []\n",
        "generate_n_grams(2, bigram_list)\n",
        "file.seek(0,0)\n",
        "\n",
        "trigram_list = []\n",
        "generate_n_grams(3, trigram_list)\n",
        "file.seek(0,0)\n",
        "\n",
        "\n",
        "frequency_unigram_list = {}\n",
        "frequency_bigram_list = {}\n",
        "frequency_trigram_list = {}\n",
        "\n",
        "frequency_ngram(unigram_list , frequency_unigram_list)\n",
        "frequency_ngram(bigram_list , frequency_bigram_list)\n",
        "frequency_ngram(trigram_list , frequency_trigram_list)\n",
        "\n",
        "print(\"Number of unigrams to cover 90% of corpora: \" + find_coverage( unigram_list , frequency_unigram_list , 90 ))\n",
        "print(\"Number of bigram to cover 80% of corpora: \" + find_coverage( bigram_list , frequency_bigram_list , 80 ))\n",
        "print(\"Number of trigram to cover 70% of corpora: \" + find_coverage( trigram_list , frequency_trigram_list , 70 ))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter input file : hindi_words_custom.txt\n",
            "Enter Y to enable stemming: n\n",
            "Number of unigrams to cover 90% of corpora: 16310\n",
            "Number of bigram to cover 80% of corpora: 949644\n",
            "Number of trigram to cover 70% of corpora: 3092818\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u1ozPTOixtjk"
      },
      "source": [
        "import re , collections\n",
        "\n",
        "vocabulary_size = 100000\n",
        "number_of_iterations = 15000\n",
        "\n",
        "val = input(\"Enter input file : \")\n",
        "path_to_file = \"/content/drive/My Drive/results/\" + val\n",
        "file = open(path_to_file, \"r\")\n",
        "\n",
        "if( val [0] =='e'):\n",
        "\tval1 = \"english\"\n",
        "else:\n",
        "\tval1 = \"hindi\"\n",
        "\n",
        "writer_file = open(\"/content/drive/My Drive/results/\"+val1 + \"_bpe.txt\", \"w\")\n",
        "\n",
        "def generate_n_grams(n , list_ngrams):\n",
        "  ngram = \"\"\n",
        "  ctr = 1\n",
        "  count_of_words = 0\n",
        "  for line in file:\n",
        "    count_of_words += 1\n",
        "    \n",
        "    line = line.rstrip()\n",
        "    if line == \",\" or line == \":\" or line == \"?\" or line == \".\" or line == \"'\" or line == \"\\\"\" or line ==\"(\" or line == \")\":\n",
        "      continue\n",
        "\n",
        "    ngram += line\n",
        "    ngram += \" \"\n",
        "\n",
        "    if ctr < n:\n",
        "      ctr += 1\n",
        "    else:\n",
        "      list_ngrams.append(ngram.strip())\n",
        "      ngram = ngram.split(' ',1)[1]\n",
        "\n",
        "def frequency_ngram ( list_ngrams, vocab ):\n",
        "\trank = {}\n",
        "\tglobal vocabulary_size\n",
        "\tfrequency_list= {}\n",
        "\tfor ngram in list_ngrams:\n",
        "\t\tif ngram in frequency_list:\n",
        "\t\t\tfrequency_list[ngram] += 1\n",
        "\t\telse:\n",
        "\t\t\tfrequency_list[ngram] = 1\n",
        "\t\t\n",
        "\trank_ngrams = sorted(frequency_list.items(), key = \n",
        "\t\t\t\t\t\t\t\t\t\t\tlambda kv:(kv[1], kv[0]) , reverse = True)\n",
        "\n",
        "\tfor i in range(vocabulary_size):\n",
        "\t\tword = rank_ngrams[i][0]\n",
        "\t\tvocab_word = \"\"\n",
        "\t\tfor char in word:\n",
        "\t\t\tvocab_word += char\n",
        "\t\t\tvocab_word += \" \"\n",
        "\t\tvocab_word += \"~\"\n",
        "\t\tvocab[vocab_word] = rank_ngrams[i][1]\n",
        "\n",
        "def getstats ( vocab ) :\n",
        "\tpairs = collections . defaultdict( int )\n",
        "\tfor word , freq in vocab .items ( ) :\n",
        "\t  symbols = word.split()\n",
        "\t  for i in range ( len(symbols) -1 ) :\n",
        "\t    pairs[ symbols [ i ] , symbols [ i + 1 ] ] += freq\n",
        "\treturn pairs\n",
        "\n",
        "def mergevocab ( pair , vin ) :\n",
        "\tvout = {}\n",
        "\tbigram = re . escape ( ' '.join ( pair ) )\n",
        "\tp = re.compile ( r'(?<!\\S)' + bigram + r'(?!\\S)' )\n",
        "\tfor word in vin :\n",
        "\t  wout = p.sub ( ''. join ( pair ) , word )\n",
        "\t  vout [ wout ] = vin [ word ]\n",
        "\treturn vout\n",
        "  \n",
        "unigram_list = []\n",
        "generate_n_grams(1, unigram_list)\n",
        "file.seek(0,0)\n",
        "\n",
        "vocab = {}\n",
        "frequency_ngram(unigram_list,vocab )\n",
        "\n",
        "for i in range ( number_of_iterations ) :\n",
        "  pairs = getstats ( vocab )\n",
        "  best = max ( pairs , key= pairs . get )\n",
        "  vocab = mergevocab ( best , vocab )\n",
        "\n",
        "frequency_sub_tokens = {}\n",
        "\n",
        "for key,value in vocab.items():\n",
        "\tsub_tokens = key.split()\n",
        "\tfor sub_token in sub_tokens:\n",
        "\t\tif sub_token in frequency_sub_tokens:\n",
        "\t\t\tfrequency_sub_tokens[sub_token] += value\n",
        "\t\telse:\n",
        "\t\t\tfrequency_sub_tokens[sub_token] = value\n",
        "\n",
        "rank_sub_tokens = sorted(frequency_sub_tokens.items(), key = \n",
        "\t\t\t\t\t\t\t\t\t\tlambda kv:(kv[1], kv[0]) , reverse = True)\n",
        "\n",
        "print(\"50 most frequent sub-tokens: \")\n",
        "for i in range(50):\n",
        "\tif( rank_sub_tokens[-1*i][0]  == \"~\" ):\n",
        "\t\tcontinue\n",
        "\tprint( rank_sub_tokens[i][0]  + \" with frequency: \" + str(rank_sub_tokens[i][1]) )\n",
        "\n",
        "print(\"\\n\")\n",
        "print(\"50 least frequent sub-tokens: \")\n",
        "for i in range(50):\n",
        "\tif( rank_sub_tokens[-1*i][0]  == \"~\" ):\n",
        "\t\tcontinue\n",
        "\tprint( rank_sub_tokens[-1*i][0]  + \" with frequency: \" + str(rank_sub_tokens[-1*i][1]) )\n",
        " \n",
        "for line,freq in vocab.items():\n",
        "\twriter_file.write(line + \"_\" + str(freq))\n",
        "\twriter_file.write(\"\\n\")\n",
        "\n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGDoVACK3aHo"
      },
      "source": [
        "# !pip install polyglot        \n",
        "# !pip install pyicu           \n",
        "# !pip install Morfessor       \n",
        "# !pip install pycld2        \n",
        "import polyglot\n",
        "from polyglot.text import Text, Word\n",
        "# !polyglot download morph2.wo\n",
        "import random\n",
        "\n",
        "\n",
        "val = input(\"Enter input file : \")\n",
        "path_to_file = \"/content/drive/My Drive/results/\" + val\n",
        "file = write_word_file_object = open(path_to_file, \"r\")\n",
        "\n",
        "def generate_n_grams(n , list_ngrams):\n",
        "  ngram = \"\"\n",
        "  ctr = 1\n",
        "  for line in file:\n",
        "    line = line.rstrip()\n",
        "    if line == \",\" or line == \":\" or line == \"?\" or line == \".\" or line == \"'\" or line == \"\\\"\" or line ==\"(\" or line == \")\":\n",
        "      continue\n",
        "    ngram += line\n",
        "    ngram += \" \"\n",
        "    if ctr < n:\n",
        "      ctr += 1\n",
        "    else:\n",
        "      list_ngrams.append(ngram.strip())\n",
        "      ngram = ngram.split(' ',1)[1]\n",
        "\n",
        "def frequency_distribution ( list_ngrams ):\n",
        "  frequency = {} \n",
        "  rank = {}\n",
        "  for ngram in list_ngrams:  \n",
        "    if ngram in frequency:\n",
        "      frequency[ngram] += 1\n",
        "    else:\n",
        "      frequency[ngram] = 1\n",
        "    \n",
        "  rank_ngrams = sorted(frequency.items(), key = \n",
        "                      lambda kv:(kv[1], kv[0]) , reverse = True)\n",
        "  print(\"We first attempt Morphological analysis of 5 random words from 100 most frequent words: \")\n",
        "  # for i in range(5):\n",
        "  #   random_index = random.randint(1,1000)\n",
        "    \n",
        "  #   word = rank_ngrams[random_index][0]\n",
        "  #   freq = rank_ngrams[random_index][1]\n",
        "\n",
        "  #   print(\"Random Chosen Word: \" + word +\" with a frequency of: \" + str(freq))\n",
        "    \n",
        "  #   morph = Text(word).words[0]\n",
        "  #   # for morpheme in morph.morphemes:\n",
        "  #   #   print(morpheme + \" \")\n",
        "  #   print(morph.morphemes)\n",
        "\n",
        "  print(\"We now perform Morphological analysis of 5 random words from 100 least frequent words: \")\n",
        "  i = 0\n",
        "  while i < 5:\n",
        "    print(i)\n",
        "    random_index = random.randint(1,100000)\n",
        "    \n",
        "    word = rank_ngrams[-1 * random_index][0]\n",
        "    flag = 1\n",
        "    \n",
        "    print(word)\n",
        "    for j in word:\n",
        "      if not (j.isalpha()) and j!= '\\'' and j!='-':\n",
        "        flag = 0\n",
        "        break\n",
        "    \n",
        "    if word[0] == '\\'' or word[-1] == '\\'':\n",
        "      flag = 0\n",
        "    if flag == 0:\n",
        "      continue\n",
        "    i+=1\n",
        "    \n",
        "    freq = rank_ngrams[-1 * random_index][1]\n",
        "\n",
        "    print(\"Random Chosen Word: \" + word +\" with a frequency of: \" + str(freq))\n",
        "    morph = Text(word).words[0]\n",
        "    # for morpheme in morph.morphemes:\n",
        "    #   print(morpheme + \" \")\n",
        "    print(morph.morphemes)\n",
        "\n",
        "unigram_list = [] \n",
        "\n",
        "generate_n_grams(1, unigram_list)\n",
        "\n",
        "frequency_ngram_list = {}\n",
        "rank_ngram_list = {}\n",
        "\n",
        "frequency_distribution(unigram_list)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JuEQz2uhHH4n",
        "outputId": "28f159ed-b9c4-4d8b-b288-7c03c116cce9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "val = input(\"Enter input file : \")\n",
        "path_to_file = \"/content/drive/My Drive/results/\" + val\n",
        "file = open(path_to_file, \"r\")\n",
        "\n",
        "vocab = {}\n",
        "for line in file:\n",
        "  line = line.split(\"_\")\n",
        "  # print(line)\n",
        "  if len(line)!=2:\n",
        "    continue\n",
        "  line[0] = line[0].rstrip()\n",
        "  line[1] = line[1].rstrip()\n",
        "  word = line[0]\n",
        "  freq = int(line[1])\n",
        "  vocab[word] = freq\n",
        "\n",
        "\n",
        "\n",
        "frequency_sub_tokens = {}\n",
        "\n",
        "for key,value in vocab.items():\n",
        "\tsub_tokens = key.split()\n",
        "\tfor sub_token in sub_tokens:\n",
        "\t\tif sub_token in frequency_sub_tokens:\n",
        "\t\t\tfrequency_sub_tokens[sub_token] += value\n",
        "\t\telse:\n",
        "\t\t\tfrequency_sub_tokens[sub_token] = value\n",
        "\n",
        "rank_sub_tokens = sorted(frequency_sub_tokens.items(), key = \n",
        "\t\t\t\t\t\t\t\t\t\tlambda kv:(kv[1], kv[0]) , reverse = True)\n",
        "\n",
        "\n",
        "while 1:\n",
        "  index = 0\n",
        "  val_with_sub_tokens = \"\"\n",
        "  val = input(\"Enter word (press e to exit): \")\n",
        "  for char in val:\n",
        "    val_with_sub_tokens += char\n",
        "    val_with_sub_tokens += \" \"\n",
        "  val_with_sub_tokens += \"~\"\n",
        "  if val==\"e\":\n",
        "    break\n",
        "  while index < len(rank_sub_tokens):\n",
        "    temp = val_with_sub_tokens.split()\n",
        "    l = 0\n",
        "    is_sub_token_found=0\n",
        "    merged_sub_tokens = \"\"\n",
        "    temp_copy = []\n",
        "    for value in temp:\n",
        "      if l == 0 :\n",
        "        l+=1\n",
        "        merged_sub_tokens += value\n",
        "        continue\n",
        "      merged_sub_tokens += value\n",
        "      \n",
        "      if is_sub_token_found==0 and merged_sub_tokens == rank_sub_tokens[index][0]:\n",
        "        temp_copy.append(merged_sub_tokens)\n",
        "        is_sub_token_found = 1\n",
        "        merged_sub_tokens = \"\"\n",
        "        l=0\n",
        "      else:\n",
        "        merged_sub_tokens = merged_sub_tokens[:-1*len(value)]\n",
        "        temp_copy.append(merged_sub_tokens)\n",
        "        merged_sub_tokens = value\n",
        "    temp_copy.append(merged_sub_tokens)\n",
        "    if (is_sub_token_found==1):\n",
        "      is_sub_token_found=0\n",
        "      index=0\n",
        "    else:\n",
        "      index+=1\n",
        "    val_with_sub_tokens = \" \".join(temp_copy)\n",
        "\n",
        "  print(val_with_sub_tokens)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter input file : english_bpe.txt\n",
            "Enter word (press e to exit): below\n",
            "below~\n",
            "Enter word (press e to exit): population\n",
            "population~\n",
            "Enter word (press e to exit): species\n",
            "species~\n",
            "Enter word (press e to exit): where\n",
            "where~\n",
            "Enter word (press e to exit): road\n",
            "road~\n",
            "Enter word (press e to exit): Dictyopetra\n",
            "Dic ty op e tra~\n",
            "Enter word (press e to exit): Hackbridge\n",
            "H ack bridge~\n",
            "Enter word (press e to exit): Derrymore\n",
            "D err y more~\n",
            "Enter word (press e to exit): Desembarco\n",
            "Des embar co~\n",
            "Enter word (press e to exit): Lawrason\n",
            "Law ra son~\n",
            "Enter word (press e to exit): e\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}