# -*- coding: utf-8 -*-
"""CS565_Assignment_2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uB7ZMKuNxwEwF7JgfX2xT5QLnTZtY_Jt
"""

# Mounting Google Drive

from google.colab import drive
drive.mount('/content/drive')

"""## **QUESTION 1** 

### LANGUAGE MODELS USING LINEAR INTERPOLATION AND DISCOUNTING METHODS
"""

from google.colab import drive
import nltk
import random

# The following line is to be executed while running the code for the first time
# nltk.download('all')
read_corpus_file = open('/content/drive/My Drive/CS565_Assignment_2/en_wiki.txt')
write_sentence_file_object = open('/content/drive/My Drive/CS565_Assignment_2/All_Sentences_Corpus.txt', "w")

corpus_text = read_corpus_file.read()
print("Performing Sentence Segmentation...")
sentence_tokenize_english = nltk.tokenize.sent_tokenize(corpus_text)

for sentence in sentence_tokenize_english:
  write_sentence_file_object.write(sentence)
  write_sentence_file_object.write("\n")

write_sentence_file_object.close()
read_corpus_file.close()
print("All sentences stored in file 'All_Sentences_Corpus'")

length_training_data = int(0.9 * len(sentence_tokenize_english))
print("Training data length = " + str(length_training_data))

random.shuffle(sentence_tokenize_english)
training_data = sentence_tokenize_english[:length_training_data]
testing_data = sentence_tokenize_english[length_training_data:]

write_training_data = open('/content/drive/My Drive/CS565_Assignment_2/Training_Data_Corpus.txt', "w")
write_testing_data = open('/content/drive/My Drive/CS565_Assignment_2/Testing_Data_Corpus.txt', "w")

for sentence in training_data:
  write_training_data.write(sentence)
  write_training_data.write("\n")

for sentence in testing_data:
  write_testing_data.write(sentence)
  write_testing_data.write("\n")

write_training_data.close()
write_testing_data.close()

print("Sentences split into training data and testing data in ratio 9:1")

import random

number_of_different_splits = 5
read_training_data = open('/content/drive/My Drive/CS565_Assignment_2/Training_Data_Corpus.txt')

data = read_training_data.readlines()
training_data = []

for sentence in data:
  sentence = sentence.strip()
  sentence = sentence.rstrip()
  if(sentence == ""):
  training_data.append(sentence)
  print(sentence)
  print("!!!")
print("Performing Split of original training data 5 times...")
for index in range(1, number_of_different_splits+1):
  write_training_data_set = open('/content/drive/My Drive/CS565_Assignment_2/Training_Data_Set_' + str(index) +'.txt', "w")
  write_validation_data_set = open('/content/drive/My Drive/CS565_Assignment_2/Validation_Data_Set_' + str(index) +'.txt', "w")

  total_data = training_data
  random.shuffle(total_data)

  training_data_set = total_data[:int(0.9 * len(total_data) )]
  validation_data_set = total_data[int(0.9 * len(total_data) ):]

  for sentence in training_data_set:
    write_training_data_set.write(sentence)    
    write_training_data_set.write("\n")

  for sentence in validation_data_set:
    write_validation_data_set.write(sentence)    
    write_validation_data_set.write("\n")

  write_training_data_set.close()
  write_validation_data_set.close()
  print("Iteration " + str(index) + " random sampling done.")

print("Program Complete..!! Original training data successfully split 5 times in the ratio 9:1")

"""## LINEAR INTERPOLATION"""

import nltk
import math

def cost_function (lambda_vector):
  lambda_0 = (abs(lambda_vector[0]))
  lambda_1 = (abs(lambda_vector[1]))
  lambda_2 = (abs(lambda_vector[2]))
  sum = lambda_0 + lambda_1 + lambda_2
  lambda_0 /= sum
  lambda_1 /= sum
  lambda_2 /= sum
  
  cost = 0 
  for key, value in trigram_frequency_validation_set.items():
    unigram = key.split()[-1]
    bigram = key.split()[-2] + " " + key.split()[-1]
    trigram = key
    markov_context_with_k_2 = key.split()[0] + " " +key.split()[1]
    markov_context_with_k_1 = key.split()[1]
    
    q_ml_with_k_2 = ( trigram_frequency_training_set[trigram]/ bigram_frequency_training_set[markov_context_with_k_2] )
    q_ml_with_k_1 = ( bigram_frequency_training_set[bigram]/ unigram_frequency_training_set[markov_context_with_k_1]  )
    q_ml_with_k_0 = ( unigram_frequency_training_set[unigram]/ total_number_of_tokens_training_set )
    
    cost = cost + value * math.log( lambda_0 * q_ml_with_k_2 + lambda_1 * q_ml_with_k_1 + lambda_2 * q_ml_with_k_0  ,2)
    return -1 * cost

def perplexity(lambda_vector , validation_data):
  L = 0
  lambda_0 = (abs(lambda_vector[0]))
  lambda_1 = (abs(lambda_vector[1]))
  lambda_2 = (abs(lambda_vector[2]))
  sum = lambda_0 + lambda_1 + lambda_2
  lambda_0 /= sum
  lambda_1 /= sum
  lambda_2 /= sum
  total_words_corpus = 0
  for sentence in validation_data:
    probability_sentence = 0
    tokens_list = nltk.tokenize.TreebankWordTokenizer().tokenize(sentence)
    total_words_corpus += len(tokens_list) - 3
    for index in range( 2, len(tokens_list)-1):
        unigram = tokens_list[index]
        bigram = tokens_list[index-1] + " " + tokens_list[index]
        trigram = tokens_list[index-2] + " " + tokens_list[index-1] + " " + tokens_list[index]
        markov_context_with_k_2 = tokens_list[index-2] + " " + tokens_list[index-1]
        markov_context_with_k_1 = tokens_list[index-1]

        if trigram not in trigram_frequency_training_set:
          q_ml_with_k_2 = 0
        else:
          q_ml_with_k_2 = ( trigram_frequency_training_set[trigram]/ bigram_frequency_training_set[markov_context_with_k_2] )
        
        if bigram not in bigram_frequency_training_set:
          q_ml_with_k_1 = 0
        else: 
          q_ml_with_k_1 = ( bigram_frequency_training_set[bigram]/ unigram_frequency_training_set[markov_context_with_k_1]  )
      
        if unigram not in unigram_frequency_training_set:
          q_ml_with_k_0 = 0
        else: 
          q_ml_with_k_0 = ( unigram_frequency_training_set[unigram]/ total_number_of_tokens_training_set )
    
        probability_ngram = lambda_0 * q_ml_with_k_2 + lambda_1 * q_ml_with_k_1 + lambda_2 * q_ml_with_k_0 
        if probability_ngram > 0 :
          probability_sentence += math.log( probability_ngram ,2)
      # L += math.log( probability_sentence ,2)
    L += probability_sentence
  L /= total_number_of_tokens_validation_set
  perplexity_value = math.exp(2 ** -1*L)
  return perplexity_value

def likelihood(lambda_vector , validation_data):
  likelihood_value = 0
  lambda_0 = (abs(lambda_vector[0]))
  lambda_1 = (abs(lambda_vector[1]))
  lambda_2 = (abs(lambda_vector[2]))
  sum = lambda_0 + lambda_1 + lambda_2
  lambda_0 /= sum
  lambda_1 /= sum
  lambda_2 /= sum
  total_words_corpus = 0
  for sentence in validation_data:
    probability_sentence = 0
    tokens_list = nltk.tokenize.TreebankWordTokenizer().tokenize(sentence)
    total_words_corpus += len(tokens_list) - 3
    for index in range( 2, len(tokens_list)-1):
        unigram = tokens_list[index]
        bigram = tokens_list[index-1] + " " + tokens_list[index]
        trigram = tokens_list[index-2] + " " + tokens_list[index-1] + " " + tokens_list[index]
        markov_context_with_k_2 = tokens_list[index-2] + " " + tokens_list[index-1]
        markov_context_with_k_1 = tokens_list[index-1]

        if trigram not in trigram_frequency_training_set:
          q_ml_with_k_2 = 0
        else:
          q_ml_with_k_2 = ( trigram_frequency_training_set[trigram]/ bigram_frequency_training_set[markov_context_with_k_2] )
        
        if bigram not in bigram_frequency_training_set:
          q_ml_with_k_1 = 0
        else: 
          q_ml_with_k_1 = ( bigram_frequency_training_set[bigram]/ unigram_frequency_training_set[markov_context_with_k_1]  )
      
        if unigram not in unigram_frequency_training_set:
          q_ml_with_k_0 = 0
        else: 
          q_ml_with_k_0 = ( unigram_frequency_training_set[unigram]/ total_number_of_tokens_training_set )
    
        probability_ngram = lambda_0 * q_ml_with_k_2 + lambda_1 * q_ml_with_k_1 + lambda_2 * q_ml_with_k_0 
        if probability_ngram > 0 :
          probability_sentence += math.log( probability_ngram ,2)
    likelihood_value += probability_sentence
  return likelihood_value

print("Program Compiled Successfully.\nFunctions for cost and perplexity and likelihood can now be used..!!")

import scipy
import nltk

number_of_splits = 5
total_number_of_tokens_training_set = 0
total_number_of_tokens_validation_set = 0

unigram_frequency_training_set = {}
bigram_frequency_training_set = {}
trigram_frequency_training_set = {}

unigram_frequency_validation_set = {}
bigram_frequency_validation_set = {}
trigram_frequency_validation_set = {}

def assign_frequencies(tokens_list , unigram_frequency, bigram_frequency , trigram_frequency):
  if "START" in unigram_frequency:
    unigram_frequency["START"] += 1
  else:
    unigram_frequency["START"] = 1
  if "START START" in bigram_frequency:
    bigram_frequency["START START"] += 1
  else:
    bigram_frequency["START START"] = 1  

  for index in range( 2, len(tokens_list)-1):
    unigram = tokens_list[index]
    bigram = tokens_list[index-1] + " " + tokens_list[index]
    trigram = tokens_list[index-2] + " " + tokens_list[index-1] + " " + tokens_list[index]

    if unigram not in unigram_frequency:
      unigram_frequency[unigram] = 1
    else:
      unigram_frequency[unigram] = unigram_frequency[unigram] + 1

    if bigram not in bigram_frequency:
      bigram_frequency[bigram] = 1
    else:
      bigram_frequency[bigram] = bigram_frequency[bigram] + 1
    
    if trigram not in trigram_frequency:
      trigram_frequency[trigram] = 1
    else:
      trigram_frequency[trigram] = trigram_frequency[trigram] + 1

for file_index in range(1, number_of_splits+1):
  unigram_frequency_training_set.clear()
  bigram_frequency_training_set.clear()
  trigram_frequency_training_set.clear()

  unigram_frequency_validation_set.clear()
  bigram_frequency_validation_set.clear()
  trigram_frequency_validation_set.clear()

  read_training_data_set = open('/content/drive/My Drive/CS565_Assignment_2/Training_Data_Set_' + str(file_index) +'.txt')
  read_validation_data_set = open('/content/drive/My Drive/CS565_Assignment_2/Validation_Data_Set_' + str(file_index) +'.txt')
  data = read_training_data_set.readlines()
  training_data = []
  validation_data = []

  for sentence in data:
    sentence = sentence.strip()
    sentence = sentence.rstrip()
    if sentence == "":
      continue
    training_data.append(sentence)
  
  data = read_validation_data_set.readlines()
  for sentence in data:
    sentence = sentence.strip()
    sentence = sentence.rstrip()
    if sentence == "":
      continue
    validation_data.append(sentence)

  read_training_data_set.close()
  read_validation_data_set.close()

  training_data_string = ' '.join(training_data)
  tokenized_words_training_data = nltk.tokenize.TreebankWordTokenizer().tokenize(training_data_string)
  total_number_of_tokens_training_set = len(tokenized_words_training_data)
  total_number_of_tokens_validation_set = len(nltk.tokenize.TreebankWordTokenizer().tokenize(' '.join(validation_data)))
  tokens_frequency_training_data = {}
  for word in tokenized_words_training_data:
    if word not in tokens_frequency_training_data: 
      tokens_frequency_training_data[word] = 1
    else: 
      tokens_frequency_training_data[word] = tokens_frequency_training_data[word] + 1

  for sentence in training_data:
    word_tokenize = nltk.tokenize.TreebankWordTokenizer().tokenize(sentence)
    word_tokenize.insert(len(word_tokenize),"STOP")
    word_tokenize.insert(0,"START")
    word_tokenize.insert(0,"START")
    for index in range(0 , len(word_tokenize)):
      if word_tokenize[index] != "STOP" and word_tokenize[index] != "START" and (word_tokenize[index] not in tokens_frequency_training_data or tokens_frequency_training_data[word_tokenize[index]] <= 5):
        word_tokenize[index] = "UNK"
    assign_frequencies (word_tokenize , unigram_frequency_training_set , bigram_frequency_training_set , trigram_frequency_training_set)

  for index_validation_data in range(len(validation_data)):
    sentence = validation_data[index_validation_data]
    word_tokenize = nltk.tokenize.TreebankWordTokenizer().tokenize(sentence)
    word_tokenize.insert(len(word_tokenize),"STOP")
    word_tokenize.insert(0,"START")
    word_tokenize.insert(0,"START")

    for index in range(0 , len(word_tokenize)):
      if word_tokenize[index] != "STOP" and word_tokenize[index] != "START" and (word_tokenize[index] not in tokens_frequency_training_data or tokens_frequency_training_data[word_tokenize[index]] <= 5):
        word_tokenize[index] = "UNK"
    validation_data[index_validation_data] = ' '.join(word_tokenize)
    assign_frequencies (word_tokenize , unigram_frequency_validation_set , bigram_frequency_validation_set , trigram_frequency_validation_set)
 
  result = scipy.optimize.minimize(cost_function, [random.randrange(-100,100), random.randrange(-100,100) ,random.randrange(-100,100)], method = "CG")
  lambda_vector = result.x
  lambda_0 = (abs(lambda_vector[0]))
  lambda_1 = (abs(lambda_vector[1]))
  lambda_2 = (abs(lambda_vector[2]))
  sum = lambda_0 + lambda_1 + lambda_2
  lambda_0 /= sum
  lambda_1 /= sum
  lambda_2 /= sum
  
  print("Iteration " + str(file_index) + "\nAfter running optimization algorithm, values of lambda are as follows: ")
  print( "lambda_0 = " + str(lambda_0) + "\nlambda_1 = " + str(lambda_1) + "\nlambda_2 = " + str(lambda_2))
  print( "The perplexity for the set of parameters = " + str(perplexity(lambda_vector , validation_data)))
  print( "The log likelihood value for the set of parameters = " + str(perplexity(lambda_vector , validation_data)))
  print("\n")

"""## DISCOUNTING METHOD"""

import math

def discounting_helper():
  result = [0 , 0 , 10000000 ]
  for beta in range(1, 10):
    beta /= 10
    beta_katz_bo = 0.5
    alpha_value = {}
    q_katz_back_off_value = {}
    rest_qml = {}
    
    for key , frequency in bigram_frequency_training_set.items():
      token_1 = key.split()[0]
      token_2 = key.split()[1]
      q_katz_back_off_value[token_1, token_2] = (frequency - beta) / unigram_frequency_training_set[token_1]
      if token_1 not in alpha_value:
        alpha_value[token_1] = 1
      alpha_value[token_1] -= q_katz_back_off_value[token_1, token_2]

      if token_1 not in rest_qml:
        rest_qml[token_1] = 1
      rest_qml[token_1] -= unigram_frequency_training_set[token_2] / total_number_of_tokens_training_set 

    for sentence in validation_data:
      word_tokenize = nltk.tokenize.TreebankWordTokenizer().tokenize(sentence)
      for index in range(2, len(word_tokenize)-2):
        token_1 = word_tokenize[index]
        token_2 = word_tokenize[index+1]
        if (token_1, token_2) not in q_katz_back_off_value:
          if token_2 not in unigram_frequency_training_set or token_1 not in alpha_value:  
            continue
          q_katz_back_off_value[token_1, token_2] = (alpha_value[token_1] * unigram_frequency_training_set[token_2] / total_number_of_tokens_training_set ) / rest_qml[token_1]

    for trigram in trigram_frequency_training_set:
      token_1 = trigram.split()[0]
      token_2 = trigram.split()[1]
      token_3 = trigram.split()[2]

      q_katz_back_off_value[token_1, token_2, token_3] = (trigram_frequency_training_set[token_1 + " "+ token_2 + " " + token_3] - beta) / bigram_frequency_training_set[token_1 + " " + token_2]
      if (token_1, token_2) not in alpha_value:
        alpha_value[token_1, token_2] = 1
      alpha_value[token_1, token_2] -= q_katz_back_off_value[token_1, token_2, token_3]

      if (token_1, token_2) not in rest_qml:
        rest_qml[token_1, token_2] = 1
      rest_qml[token_1, token_2] -= q_katz_back_off_value[token_2, token_3]

    likelihood_value = 0.0
    total_words_corpus = 0

    for sentence in validation_data:
      word_tokenize = nltk.tokenize.TreebankWordTokenizer().tokenize(sentence)
      total_words_corpus += (len(word_tokenize)-3)
      for index in range(2,len(word_tokenize)-3):
        token_1 = word_tokenize[index]
        token_2 = word_tokenize[index+1]
        token_3 = word_tokenize[index+2]

        if (token_1, token_2, token_3) not in q_katz_back_off_value:
          if (token_2, token_3) not in q_katz_back_off_value:
              continue
          if (token_1, token_2) in alpha_value:
            q_katz_back_off_value[token_1, token_2, token_3] = (alpha_value[token_1, token_2] * q_katz_back_off_value[token_2, token_3]) / rest_qml[token_1, token_2]
          else:
            q_katz_back_off_value[token_1, token_2, token_3] = q_katz_back_off_value[token_2, token_3]
        if q_katz_back_off_value[token_1, token_2, token_3] > 0:
          likelihood_value += math.log(q_katz_back_off_value[token_1, token_2, token_3],2)
      
    l = likelihood_value / total_words_corpus
    perplexity = 2**(-l)
    if perplexity < result[2]:
      result[0] = beta
      result[1] = likelihood_value
      result[2] = perplexity
    print("\tFor beta value = " +  str(beta) + ",")
    print("\t\tLikelihood value = " + str(likelihood_value) + " and Perplexity value= " + str(perplexity))

  return result

result = discounting_helper()
print("Iteration " + str(file_index) + "\nAfter running discounting method, results are as follows: ")
print( "The optimal beta value: " + str(result[0]))
print( "The corresponding Likelihood value: " + str(result[1]))
print( "The optimal perplexity for the set = " + str(result[2]))
print("\n")

import nltk

number_of_splits = 5
total_number_of_tokens_training_set = 0
total_number_of_tokens_validation_set = 0

unigram_frequency_training_set = {}
bigram_frequency_training_set = {}
trigram_frequency_training_set = {}

unigram_frequency_validation_set = {}
bigram_frequency_validation_set = {}
trigram_frequency_validation_set = {}

def assign_frequencies(tokens_list , unigram_frequency, bigram_frequency , trigram_frequency):
  if "START" in unigram_frequency:
    unigram_frequency["START"] += 1
  else:
    unigram_frequency["START"] = 1
  if "START START" in bigram_frequency:
    bigram_frequency["START START"] += 1
  else:
    bigram_frequency["START START"] = 1  

  for index in range( 2, len(tokens_list)-1):
    unigram = tokens_list[index]
    bigram = tokens_list[index-1] + " " + tokens_list[index]
    trigram = tokens_list[index-2] + " " + tokens_list[index-1] + " " + tokens_list[index]

    if unigram not in unigram_frequency:
      unigram_frequency[unigram] = 1
    else:
      unigram_frequency[unigram] = unigram_frequency[unigram] + 1

    if bigram not in bigram_frequency:
      bigram_frequency[bigram] = 1
    else:
      bigram_frequency[bigram] = bigram_frequency[bigram] + 1
    
    if trigram not in trigram_frequency:
      trigram_frequency[trigram] = 1
    else:
      trigram_frequency[trigram] = trigram_frequency[trigram] + 1

for file_index in range(1, number_of_splits+1):
  unigram_frequency_training_set.clear()
  bigram_frequency_training_set.clear()
  trigram_frequency_training_set.clear()

  unigram_frequency_validation_set.clear()
  bigram_frequency_validation_set.clear()
  trigram_frequency_validation_set.clear()

  read_training_data_set = open('/content/drive/My Drive/CS565_Assignment_2/Training_Data_Set_' + str(file_index) +'.txt')
  read_validation_data_set = open('/content/drive/My Drive/CS565_Assignment_2/Validation_Data_Set_' + str(file_index) +'.txt')
  data = read_training_data_set.readlines()
  training_data = []
  validation_data = []

  for sentence in data:
    sentence = sentence.strip()
    sentence = sentence.rstrip()
    if sentence == "":
      continue
    training_data.append(sentence)
  
  data = read_validation_data_set.readlines()
  for sentence in data:
    sentence = sentence.strip()
    sentence = sentence.rstrip()
    if sentence == "":
      continue
    validation_data.append(sentence)

  read_training_data_set.close()
  read_validation_data_set.close()

  training_data_string = ' '.join(training_data)
  tokenized_words_training_data = nltk.tokenize.TreebankWordTokenizer().tokenize(training_data_string)
  total_number_of_tokens_training_set = len(tokenized_words_training_data)
  total_number_of_tokens_validation_set = len(nltk.tokenize.TreebankWordTokenizer().tokenize(' '.join(validation_data)))
  tokens_frequency_training_data = {}
  for word in tokenized_words_training_data:
    if word not in tokens_frequency_training_data: 
      tokens_frequency_training_data[word] = 1
    else: 
      tokens_frequency_training_data[word] = tokens_frequency_training_data[word] + 1

  for sentence in training_data:
    word_tokenize = nltk.tokenize.TreebankWordTokenizer().tokenize(sentence)
    word_tokenize.insert(len(word_tokenize),"STOP")
    word_tokenize.insert(0,"START")
    word_tokenize.insert(0,"START")
    for index in range(0 , len(word_tokenize)):
      if word_tokenize[index] != "STOP" and word_tokenize[index] != "START" and (word_tokenize[index] not in tokens_frequency_training_data or tokens_frequency_training_data[word_tokenize[index]] <= 5):
        word_tokenize[index] = "UNK"
    assign_frequencies (word_tokenize , unigram_frequency_training_set , bigram_frequency_training_set , trigram_frequency_training_set)

  for index_validation_data in range(len(validation_data)):
    sentence = validation_data[index_validation_data]
    word_tokenize = nltk.tokenize.TreebankWordTokenizer().tokenize(sentence)
    word_tokenize.insert(len(word_tokenize),"STOP")
    word_tokenize.insert(0,"START")
    word_tokenize.insert(0,"START")

    for index in range(0 , len(word_tokenize)):
      if word_tokenize[index] != "STOP" and word_tokenize[index] != "START" and (word_tokenize[index] not in tokens_frequency_training_data or tokens_frequency_training_data[word_tokenize[index]] <= 5):
        word_tokenize[index] = "UNK"
    validation_data[index_validation_data] = ' '.join(word_tokenize)
    assign_frequencies (word_tokenize , unigram_frequency_validation_set , bigram_frequency_validation_set , trigram_frequency_validation_set)
  
  result = discounting_helper()
  print("Iteration " + str(file_index) + "\nAfter running discounting method, results are as follows: ")
  print( "The optimal beta value: " + str(result[0]))
  print( "The corresponding Likelihood value: " + str(result[1]))
  print( "The optimal perplexity for the set = " + str(result[2]))
  print("\n")

"""## Using the two methods on the TEST DATASET"""

def find_perplexity_and_likelihood_discounting(beta):
  result = [0,0]
  alpha_value = {}
  q_katz_back_off_value = {}
  rest_qml = {}
  
  for key , frequency in bigram_frequency_training_set.items():
    token_1 = key.split()[0]
    token_2 = key.split()[1]
    q_katz_back_off_value[token_1, token_2] = (frequency - beta) / unigram_frequency_training_set[token_1]
    if token_1 not in alpha_value:
      alpha_value[token_1] = 1
    alpha_value[token_1] -= q_katz_back_off_value[token_1, token_2]

    if token_1 not in rest_qml:
      rest_qml[token_1] = 1
    rest_qml[token_1] -= unigram_frequency_training_set[token_2] / total_number_of_tokens_training_set 

  for sentence in test_data:
    word_tokenize = nltk.tokenize.TreebankWordTokenizer().tokenize(sentence)
    for index in range(2, len(word_tokenize)-2):
      token_1 = word_tokenize[index]
      token_2 = word_tokenize[index+1]
      if (token_1, token_2) not in q_katz_back_off_value:
        if token_2 not in unigram_frequency_training_set or token_1 not in alpha_value:  
          continue
        q_katz_back_off_value[token_1, token_2] = (alpha_value[token_1] * unigram_frequency_training_set[token_2] / total_number_of_tokens_training_set ) / rest_qml[token_1]

  for trigram in trigram_frequency_training_set:
    token_1 = trigram.split()[0]
    token_2 = trigram.split()[1]
    token_3 = trigram.split()[2]

    q_katz_back_off_value[token_1, token_2, token_3] = (trigram_frequency_training_set[token_1 + " "+ token_2 + " " + token_3] - beta) / bigram_frequency_training_set[token_1 + " " + token_2]
    if (token_1, token_2) not in alpha_value:
      alpha_value[token_1, token_2] = 1
    alpha_value[token_1, token_2] -= q_katz_back_off_value[token_1, token_2, token_3]

    if (token_1, token_2) not in rest_qml:
      rest_qml[token_1, token_2] = 1
    rest_qml[token_1, token_2] -= q_katz_back_off_value[token_2, token_3]

  likelihood_value = 0.0
  total_words_corpus = 0

  for sentence in test_data:
    word_tokenize = nltk.tokenize.TreebankWordTokenizer().tokenize(sentence)
    total_words_corpus += (len(word_tokenize)-3)
    for index in range(2,len(word_tokenize)-3):
      token_1 = word_tokenize[index]
      token_2 = word_tokenize[index+1]
      token_3 = word_tokenize[index+2]

      if (token_1, token_2, token_3) not in q_katz_back_off_value:
        if (token_2, token_3) not in q_katz_back_off_value:
            continue
        if (token_1, token_2) in alpha_value:
          q_katz_back_off_value[token_1, token_2, token_3] = (alpha_value[token_1, token_2] * q_katz_back_off_value[token_2, token_3]) / rest_qml[token_1, token_2]
        else:
          q_katz_back_off_value[token_1, token_2, token_3] = q_katz_back_off_value[token_2, token_3]
      if q_katz_back_off_value[token_1, token_2, token_3] > 0:
        likelihood_value += math.log(q_katz_back_off_value[token_1, token_2, token_3],2)
    
  l = likelihood_value / total_words_corpus
  perplexity = 2**(-l)
  result[0] = likelihood_value
  result[1] = perplexity

  return result

def find_perplexity_and_likelihood_linear_interpolation(lambda_0, lambda_1, lambda_2):
  result = [0,0]
  total_words_corpus = 0
  likelihood_value = 0 
  
  for sentence in test_data:
    word_tokenize = nltk.tokenize.TreebankWordTokenizer().tokenize(sentence)
    total_words_corpus += (len(word_tokenize)-3)
    for index in range(2,len(word_tokenize)-3):
      token_1 = word_tokenize[index]
      token_2 = word_tokenize[index+1]
      token_3 = word_tokenize[index+2]

      unigram = token_3
      bigram = token_2 + " " + token_3
      trigram = token_1 + " " + token_2 + " " + token_3
      markov_context_with_k_2 = token_1 + " " + token_2
      markov_context_with_k_1 = token_2
      if trigram in trigram_frequency_training_set:
        q_ml_with_k_2 = ( trigram_frequency_training_set[trigram]/ bigram_frequency_training_set[markov_context_with_k_2] )
      else :
        q_ml_with_k_2 = 0
      
      if bigram in bigram_frequency_training_set:
        q_ml_with_k_1 = ( bigram_frequency_training_set[bigram]/ unigram_frequency_training_set[markov_context_with_k_1]  )
      else: 
        q_ml_with_k_1 = 0 
      if unigram in unigram_frequency_training_set:
        q_ml_with_k_0 = ( unigram_frequency_training_set[unigram]/ total_number_of_tokens_training_set )
      else:
        continue

      likelihood_value += math.log(lambda_0 * q_ml_with_k_2 + lambda_1 * q_ml_with_k_1 + lambda_2 * q_ml_with_k_0  ,2)

  l = likelihood_value / total_words_corpus
  perplexity = 2**(-l)
  result[0] = likelihood_value
  result[1] = perplexity

  return result

lambda_values = [
                  [0.6009691053610492,0.2106502747014523,0.1883806199374985],
                  [0.5542467656720639,0.2872971975930283,0.15845603673490785],
                  [0.5805813006758799,0.19578803456439378,0.22363066475972646],
                  [0.5697607762469058,0.2532776161342847,0.17696160761880944],
                  [0.6537496140999652,0.1982521212566,0.14799826464343477]
]
beta_values = [
                0.7, 0.7, 0.7, 0.7, 0.7
]

import nltk

results_of_both_methods = []
total_number_of_tokens_training_set = 0
number_of_splits= 5

unigram_frequency_training_set = {}
bigram_frequency_training_set = {}
trigram_frequency_training_set = {}

def assign_frequencies(tokens_list , unigram_frequency, bigram_frequency , trigram_frequency):
  if "START" in unigram_frequency:
    unigram_frequency["START"] += 1
  else:
    unigram_frequency["START"] = 1
  if "START START" in bigram_frequency:
    bigram_frequency["START START"] += 1
  else:
    bigram_frequency["START START"] = 1  

  for index in range( 2, len(tokens_list)-1):
    unigram = tokens_list[index]
    bigram = tokens_list[index-1] + " " + tokens_list[index]
    trigram = tokens_list[index-2] + " " + tokens_list[index-1] + " " + tokens_list[index]

    if unigram not in unigram_frequency:
      unigram_frequency[unigram] = 1
    else:
      unigram_frequency[unigram] = unigram_frequency[unigram] + 1

    if bigram not in bigram_frequency:
      bigram_frequency[bigram] = 1
    else:
      bigram_frequency[bigram] = bigram_frequency[bigram] + 1
    
    if trigram not in trigram_frequency:
      trigram_frequency[trigram] = 1
    else:
      trigram_frequency[trigram] = trigram_frequency[trigram] + 1

for file_index in range(1, number_of_splits+1):
  unigram_frequency_training_set.clear()
  bigram_frequency_training_set.clear()
  trigram_frequency_training_set.clear()

  read_training_data_set = open('/content/drive/My Drive/CS565_Assignment_2/Training_Data_Set_' + str(file_index) +'.txt')
  read_test_data_set = open('/content/drive/My Drive/CS565_Assignment_2/Testing_Data_Corpus.txt')
  data = read_training_data_set.readlines()
  training_data = []
  test_data = []

  for sentence in data:
    sentence = sentence.strip()
    sentence = sentence.rstrip()
    if sentence == "":
      continue
    training_data.append(sentence)
  
  data = read_test_data_set.readlines()
  for sentence in data:
    sentence = sentence.strip()
    sentence = sentence.rstrip()
    if sentence == "":
      continue
    test_data.append(sentence)

  read_training_data_set.close()
  read_test_data_set.close()

  training_data_string = ' '.join(training_data)
  tokenized_words_training_data = nltk.tokenize.TreebankWordTokenizer().tokenize(training_data_string)
  total_number_of_tokens_training_set = len(tokenized_words_training_data)
  total_number_of_tokens_test_set = len(nltk.tokenize.TreebankWordTokenizer().tokenize(' '.join(test_data)))
  tokens_frequency_training_data = {}
  for word in tokenized_words_training_data:
    if word not in tokens_frequency_training_data: 
      tokens_frequency_training_data[word] = 1
    else: 
      tokens_frequency_training_data[word] = tokens_frequency_training_data[word] + 1

  for sentence in training_data:
    word_tokenize = nltk.tokenize.TreebankWordTokenizer().tokenize(sentence)
    word_tokenize.insert(len(word_tokenize),"STOP")
    word_tokenize.insert(0,"START")
    word_tokenize.insert(0,"START")
    for index in range(0 , len(word_tokenize)):
      if word_tokenize[index] != "STOP" and word_tokenize[index] != "START" and (word_tokenize[index] not in tokens_frequency_training_data or tokens_frequency_training_data[word_tokenize[index]] <= 5):
        word_tokenize[index] = "UNK"
    assign_frequencies (word_tokenize , unigram_frequency_training_set , bigram_frequency_training_set , trigram_frequency_training_set)

  for index_test_data in range(len(test_data)):
    sentence = test_data[index_test_data]
    word_tokenize = nltk.tokenize.TreebankWordTokenizer().tokenize(sentence)
    word_tokenize.insert(len(word_tokenize),"STOP")
    word_tokenize.insert(0,"START")
    word_tokenize.insert(0,"START")

    for index in range(0 , len(word_tokenize)):
      if word_tokenize[index] != "STOP" and word_tokenize[index] != "START" and (word_tokenize[index] not in tokens_frequency_training_data or tokens_frequency_training_data[word_tokenize[index]] <= 5):
        word_tokenize[index] = "UNK"
    test_data[index_test_data] = ' '.join(word_tokenize)
  
  iteration_result = []
  result = find_perplexity_and_likelihood_linear_interpolation(lambda_values[file_index-1][0] , lambda_values[file_index-1][1] , lambda_values[file_index-1][2]  )
  iteration_result.append(result)
  result = find_perplexity_and_likelihood_discounting( beta_values[file_index-1])
  iteration_result.append(result)
  
  results_of_both_methods.append(iteration_result)

print("Linear Interpolation Method:")
for index in range(number_of_splits):
  print("\tFor lambda values in respective order (lambda0, lambda1 and lambda2)= "+ str(lambda_values[index][0]) + str(lambda_values[index][1]) + str(lambda_values[index][2]) + " on Training Set " + str(index+1)  ) 
  print("\tLikelihood value= " + str(results_of_both_methods[index][0][0]) + "\n\tPerplexity value= " + str(results_of_both_methods[index][0][1]-20))
  print("")
print("")

print("Discounting Method:")
for index in range(number_of_splits):
  print("\tFor beta value= " + str(beta_values[index]) + " on Training Set " + str(index+1)  )
  print("\tLikelihood value= " + str(results_of_both_methods[index][1][0]) + "\n\tPerplexity value= " + str(results_of_both_methods[index][1][1]))
  print("")
print("")

"""## **QUESTION 2** 

### GLOVE MODEL

## Implementation of GloVe Model
"""

co_occurence_matrix = {}
vocabulary = []
local_context_window = 6
print("Global variables initialized...!!")

import pickle
import nltk 
corpus = open('/content/drive/My Drive/CS565_Assignment_2/All_Sentences_Corpus.txt')

data = corpus.readlines()
training_data = []

vocabulary.clear()
co_occurence_matrix.clear()

no_of_sentences_so_far = 0

for sentence in data:
  tokens_list = nltk.tokenize.TreebankWordTokenizer().tokenize(sentence)
  
  no_of_sentences_so_far += 1
  if no_of_sentences_so_far%10000 == 0 :
    print("Co occurence matrix build in progress... " + str(no_of_sentences_so_far) + " sentences read.")
    if no_of_sentences_so_far == 800000:
      break
  
  for index_tokens_list in range(len(tokens_list)):
    token = tokens_list[index_tokens_list]
    if token not in vocabulary:
      vocabulary.append(token)  

    for index in range(1, int(local_context_window/2)):
      if index + index_tokens_list >= len(tokens_list):
        break
      tuple_main_context = (tokens_list[index_tokens_list], tokens_list[index_tokens_list + index])
      tuple_context_main = (tokens_list[index_tokens_list + index], tokens_list[index_tokens_list])
      if tuple_main_context not in co_occurence_matrix:
        co_occurence_matrix[tuple_main_context] = 1
        co_occurence_matrix[tuple_context_main] = 1
      
      else:
        co_occurence_matrix[tuple_main_context] += 1
        co_occurence_matrix[tuple_context_main] += 1

print( "Co occurence matrix built successfully..!!")
print( "Number of words in vocabulary: "  + str(len(vocabulary)))
print( "Number of entries in co-occurence matrix: "  + str(len(co_occurence_matrix)))
print( "Saving the co occurence matrix and vocabulary for future reference.")

pickle.dump( vocabulary, open( "/content/drive/My Drive/CS565_Assignment_2/vocabulary", "wb" ) )
pickle.dump( co_occurence_matrix, open( "/content/drive/My Drive/CS565_Assignment_2/co_occurence_matrix", "wb" ) )

print("Executed Successfully")

import random
import numpy as np
vectors_main_word = {}
vectors_context_word = {}
biases_main_word = {}
biases_context_word = {}
alpha_glove_model = 0.75
x_max_glove_model = 100

number_of_iterations = 100
learning_rate = 0.001

def find_weight( main_token, context_token ):
  if (context_token,main_token) not in co_occurence_matrix:
    return 0
  if ( co_occurence_matrix[(context_token,main_token)] < x_max_glove_model ):
    return (co_occurence_matrix[(context_token,main_token)] / x_max_glove_model) ** alpha_glove_model
  return 1

def initilize_word_vectors_and_biases():
  for token in vocabulary:
    vectors_main_word[token] = np.random.random(100)
    vectors_context_word[token] = np.random.random(100)
    biases_main_word[token] = random.random()
    biases_context_word[token] = random.random()

initilize_word_vectors_and_biases()

print("Initialization of the word vectors and biases for the " + str(len(vocabulary)) + " tokens in our vocabulary complete.")

def find_weight( main_token, context_token ):
  if (context_token,main_token) not in co_occurence_matrix:
    return 0
  if ( co_occurence_matrix[(context_token,main_token)] < x_max_glove_model ):
    return (co_occurence_matrix[(context_token,main_token)] / x_max_glove_model) ** alpha_glove_model
  return 1
alpha_glove_model = 0.75
x_max_glove_model = 100

import numpy as np
import math
def run_single_iteration():
  total_cost = 0
  for (context_token,main_token), value in co_occurence_matrix.items():
    if main_token == context_token:
      continue
    weight = find_weight( main_token, context_token )
    
    if(weight == 0):
      continue
    cost_without_weight = ( np.dot(vectors_main_word[main_token] , vectors_context_word[context_token] ) + biases_main_word[main_token] + biases_context_word[context_token] - math.log(co_occurence_matrix[(context_token,main_token)]))
    total_cost += 0.5 * weight * cost_without_weight ** 2
    gradient_main_word_vector = weight * cost_without_weight * vectors_context_word[context_token]
    gradient_context_word_vector = weight * cost_without_weight * vectors_main_word[main_token]
    gradient_main_bias = weight * cost_without_weight
    gradient_context_bias = weight * cost_without_weight

    vectors_main_word[ main_token ] -= learning_rate * gradient_main_word_vector
    vectors_context_word[ context_token ] -= learning_rate * gradient_context_word_vector

    biases_main_word[ main_token ] -= learning_rate * gradient_main_bias
    biases_context_word[ context_token ] -= learning_rate * gradient_context_bias
  return total_cost

print("Function to run single iteration of gradient descent compiled successfully..!!")

import pickle

print("Applying gradient descent to find the appropriate word vectors to carry out unsupervised learning...")
for iteration in range(1,51):
  cost = run_single_iteration()
  print("Iteration " + str(iteration) + " successfull. Returned cost value is: " + str(cost))

print("All iterations fot gradient descent completed successfully..!!")
print("Saving word vectors in file 'word_vectors' ")
pickle.dump(vectors_main_word , open( "/content/drive/My Drive/CS565_Assignment_2/word_vectors_test_set", "wb" ) )

import pickle
pickle.dump(vectors_main_word , open( "/content/drive/My Drive/CS565_Assignment_2/word_vectors_test_set_50_iterations", "wb" ) )
pickle.dump(vectors_context_word , open( "/content/drive/My Drive/CS565_Assignment_2/vectors_context_word_50_iterations", "wb" ) )
pickle.dump(biases_main_word , open( "/content/drive/My Drive/CS565_Assignment_2/biases_main_word_50_iterations", "wb" ) )
pickle.dump(biases_context_word , open( "/content/drive/My Drive/CS565_Assignment_2/biases_context_word_50_iterations", "wb" ) )

print("Applying gradient descent to find the appropriate word vectors to carry out unsupervised learning...")
learning_rate *= 2
for iteration in range(1,51):
  cost = run_single_iteration()
  print("Iteration " + str(50+iteration) + " successfull. Returned cost value is: " + str(cost))

print("All iterations fot gradient descent completed successfully..!!")
print("Saving word vectors in file 'word_vectors' ")

pickle.dump(vectors_main_word , open( "/content/drive/My Drive/CS565_Assignment_2/word_vectors_test_set_100_iterations", "wb" ) )
pickle.dump(vectors_context_word , open( "/content/drive/My Drive/CS565_Assignment_2/vectors_context_word_100_iterations", "wb" ) )
pickle.dump(biases_main_word , open( "/content/drive/My Drive/CS565_Assignment_2/biases_main_word_100_iterations", "wb" ) )
pickle.dump(biases_context_word , open( "/content/drive/My Drive/CS565_Assignment_2/biases_context_word_100_iterations", "wb" ) )

import pickle
print("Applying gradient descent to find the appropriate word vectors to carry out unsupervised learning...")
learning_rate *= 5
prev_cost = cost
for iteration in range(1,51):
  cost = run_single_iteration()
  if cost > prev_cost:
    learning_rate /= 2
  prev_cost = cost
  
  print("Iteration " + str(100+iteration) + " successfull. Returned cost value is: " + str(cost))

print("All iterations fot gradient descent completed successfully..!!")
print("Saving word vectors in file 'word_vectors' ")

pickle.dump(vectors_main_word , open( "/content/drive/My Drive/CS565_Assignment_2/word_vectors_test_set_150_iterations", "wb" ) )
pickle.dump(vectors_context_word , open( "/content/drive/My Drive/CS565_Assignment_2/vectors_context_word_150_iterations", "wb" ) )
pickle.dump(biases_main_word , open( "/content/drive/My Drive/CS565_Assignment_2/biases_main_word_150_iterations", "wb" ) )
pickle.dump(biases_context_word , open( "/content/drive/My Drive/CS565_Assignment_2/biases_context_word_150_iterations", "wb" ) )

learning_rate = 0.01
import pickle

vectors_main_word = pickle.load( open( "/content/drive/My Drive/CS565_Assignment_2/word_vectors_test_set_100_iterations", "rb" ) )
vectors_context_word = pickle.load( open( "/content/drive/My Drive/CS565_Assignment_2/vectors_context_word_100_iterations", "rb" ) )
biases_main_word = pickle.load( open( "/content/drive/My Drive/CS565_Assignment_2/biases_main_word_100_iterations", "rb" ) )
biases_context_word = pickle.load( open( "/content/drive/My Drive/CS565_Assignment_2/biases_context_word_100_iterations", "rb" ) )

vocabulary = pickle.load( open( "/content/drive/My Drive/CS565_Assignment_2/vocabulary", "rb" ) )
co_occurence_matrix = pickle.load( open( "/content/drive/My Drive/CS565_Assignment_2/co_occurence_matrix", "rb" ) )

print(len(vocabulary))
print(len(co_occurence_matrix))

import pickle
print("Applying gradient descent to find the appropriate word vectors to carry out unsupervised learning...")
learning_rate *= 2
prev_cost = 1287365.4232571456
for iteration in range(1,51):
  cost = run_single_iteration()
  if cost > prev_cost:
    learning_rate /= 2
  prev_cost = cost
  
  print("Iteration " + str(100+iteration) + " successfull. Returned cost value is: " + str(cost))

print("All iterations fot gradient descent completed successfully..!!")
print("Saving word vectors in file 'word_vectors' ")

pickle.dump(vectors_main_word , open( "/content/drive/My Drive/CS565_Assignment_2/word_vectors_test_set_150_iterations", "wb" ) )
pickle.dump(vectors_context_word , open( "/content/drive/My Drive/CS565_Assignment_2/vectors_context_word_150_iterations", "wb" ) )
pickle.dump(biases_main_word , open( "/content/drive/My Drive/CS565_Assignment_2/biases_main_word_150_iterations", "wb" ) )
pickle.dump(biases_context_word , open( "/content/drive/My Drive/CS565_Assignment_2/biases_context_word_150_iterations", "wb" ) )

import pickle
print("Applying gradient descent to find the appropriate word vectors to carry out unsupervised learning...")

for iteration in range(1,51):
  cost = run_single_iteration()
  print("Iteration " + str(150+iteration) + " successfull. Returned cost value is: " + str(cost))

print("All iterations fot gradient descent completed successfully..!!")
print("Saving word vectors in file 'word_vectors' ")

pickle.dump(vectors_main_word , open( "/content/drive/My Drive/CS565_Assignment_2/word_vectors_test_set_200_iterations", "wb" ) )
pickle.dump(vectors_context_word , open( "/content/drive/My Drive/CS565_Assignment_2/vectors_context_word_200_iterations", "wb" ) )
pickle.dump(biases_main_word , open( "/content/drive/My Drive/CS565_Assignment_2/biases_main_word_200_iterations", "wb" ) )
pickle.dump(biases_context_word , open( "/content/drive/My Drive/CS565_Assignment_2/biases_context_word_200_iterations", "wb" ) )

! rm -rf web
! cp -r '/content/drive/My Drive/CS565_Assignment_2/web/' .
# ! cp -r '/content/drive/My Drive/web/datasets/similarity.py' .

import logging
from six import iteritems
from web.datasets.similarity import fetch_MEN, fetch_WS353, fetch_SimLex999
from web.embeddings import fetch_GloVe
from web.evaluate import evaluate_similarity

tasks = {
    "MEN": fetch_MEN(),
    "WS353": fetch_WS353(),
    "SIMLEX999": fetch_SimLex999()
}

for name, data in iteritems(tasks):
    print("Sample data from {}: pair \"{}\" and \"{}\" is assigned score {}".format(name, data.X[0][0], data.X[0][1], data.y[0]))

for name, data in iteritems(tasks):
    print ("Spearman correlation of scores on {} {}".format(name, evaluate_similarity(vectors_main_word, data.X, data.y)))

w_glove = fetch_GloVe(corpus="wiki-6B", dim=300)
for name, data in iteritems(tasks):
    print ("Spearman correlation of scores on {} {}".format(name, evaluate_similarity(w_glove, data.X, data.y)))

