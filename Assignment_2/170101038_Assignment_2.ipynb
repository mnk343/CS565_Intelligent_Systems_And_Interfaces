{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CS565_Assignment_2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "mbfHvCUE6hW3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b43442cc-de47-4e45-c7af-9d5fc7aa4df3"
      },
      "source": [
        "# Mounting Google Drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Vu89mzcQDzj"
      },
      "source": [
        "## **QUESTION 1** \n",
        "\n",
        "### LANGUAGE MODELS USING LINEAR INTERPOLATION AND DISCOUNTING METHODS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D20OtNUAt-Hn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8de4af75-2b7f-40c0-94dc-91d78a5dd348"
      },
      "source": [
        "from google.colab import drive\n",
        "import nltk\n",
        "import random\n",
        "\n",
        "# The following line is to be executed while running the code for the first time\n",
        "# nltk.download('all')\n",
        "read_corpus_file = open('/content/drive/My Drive/CS565_Assignment_2/en_wiki.txt')\n",
        "write_sentence_file_object = open('/content/drive/My Drive/CS565_Assignment_2/All_Sentences_Corpus.txt', \"w\")\n",
        "\n",
        "corpus_text = read_corpus_file.read()\n",
        "print(\"Performing Sentence Segmentation...\")\n",
        "sentence_tokenize_english = nltk.tokenize.sent_tokenize(corpus_text)\n",
        "\n",
        "for sentence in sentence_tokenize_english:\n",
        "  write_sentence_file_object.write(sentence)\n",
        "  write_sentence_file_object.write(\"\\n\")\n",
        "\n",
        "write_sentence_file_object.close()\n",
        "read_corpus_file.close()\n",
        "print(\"All sentences stored in file 'All_Sentences_Corpus'\")\n",
        "\n",
        "length_training_data = int(0.9 * len(sentence_tokenize_english))\n",
        "print(\"Training data length = \" + str(length_training_data))\n",
        "\n",
        "random.shuffle(sentence_tokenize_english)\n",
        "training_data = sentence_tokenize_english[:length_training_data]\n",
        "testing_data = sentence_tokenize_english[length_training_data:]\n",
        "\n",
        "write_training_data = open('/content/drive/My Drive/CS565_Assignment_2/Training_Data_Corpus.txt', \"w\")\n",
        "write_testing_data = open('/content/drive/My Drive/CS565_Assignment_2/Testing_Data_Corpus.txt', \"w\")\n",
        "\n",
        "for sentence in training_data:\n",
        "  write_training_data.write(sentence)\n",
        "  write_training_data.write(\"\\n\")\n",
        "\n",
        "for sentence in testing_data:\n",
        "  write_testing_data.write(sentence)\n",
        "  write_testing_data.write(\"\\n\")\n",
        "\n",
        "write_training_data.close()\n",
        "write_testing_data.close()\n",
        "\n",
        "print(\"Sentences split into training data and testing data in ratio 9:1\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Performing Sentence Segmentation...\n",
            "All sentences stored in file 'All_Sentences_Corpus'\n",
            "Training data length = 685423\n",
            "Sentences split into training data and testing data in ratio 9:1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PXQom1aS8wWM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f505d4f5-7b98-4dc5-9e93-1317770db6ee"
      },
      "source": [
        "import random\n",
        "\n",
        "number_of_different_splits = 5\n",
        "read_training_data = open('/content/drive/My Drive/CS565_Assignment_2/Training_Data_Corpus.txt')\n",
        "\n",
        "data = read_training_data.readlines()\n",
        "training_data = []\n",
        "\n",
        "for sentence in data:\n",
        "  sentence = sentence.strip()\n",
        "  sentence = sentence.rstrip()\n",
        "  if(sentence == \"\"):\n",
        "  training_data.append(sentence)\n",
        "  print(sentence)\n",
        "  print(\"!!!\")\n",
        "print(\"Performing Split of original training data 5 times...\")\n",
        "for index in range(1, number_of_different_splits+1):\n",
        "  write_training_data_set = open('/content/drive/My Drive/CS565_Assignment_2/Training_Data_Set_' + str(index) +'.txt', \"w\")\n",
        "  write_validation_data_set = open('/content/drive/My Drive/CS565_Assignment_2/Validation_Data_Set_' + str(index) +'.txt', \"w\")\n",
        "\n",
        "  total_data = training_data\n",
        "  random.shuffle(total_data)\n",
        "\n",
        "  training_data_set = total_data[:int(0.9 * len(total_data) )]\n",
        "  validation_data_set = total_data[int(0.9 * len(total_data) ):]\n",
        "\n",
        "  for sentence in training_data_set:\n",
        "    write_training_data_set.write(sentence)    \n",
        "    write_training_data_set.write(\"\\n\")\n",
        "\n",
        "  for sentence in validation_data_set:\n",
        "    write_validation_data_set.write(sentence)    \n",
        "    write_validation_data_set.write(\"\\n\")\n",
        "\n",
        "  write_training_data_set.close()\n",
        "  write_validation_data_set.close()\n",
        "  print(\"Iteration \" + str(index) + \" random sampling done.\")\n",
        "\n",
        "print(\"Program Complete..!! Original training data successfully split 5 times in the ratio 9:1\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Performing Split of original training data 5 times...\n",
            "Iteration 1 random sampling done.\n",
            "Iteration 2 random sampling done.\n",
            "Iteration 3 random sampling done.\n",
            "Iteration 4 random sampling done.\n",
            "Iteration 5 random sampling done.\n",
            "Program Complete..!! Original training data successfully split 5 times in the ratio 9:1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0PWDA9cWtsyr"
      },
      "source": [
        "## LINEAR INTERPOLATION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zwPnDX7dKZbv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0b72871-79f9-4156-d6ce-96fe086dee9f"
      },
      "source": [
        "import nltk\n",
        "import math\n",
        "\n",
        "def cost_function (lambda_vector):\n",
        "  lambda_0 = (abs(lambda_vector[0]))\n",
        "  lambda_1 = (abs(lambda_vector[1]))\n",
        "  lambda_2 = (abs(lambda_vector[2]))\n",
        "  sum = lambda_0 + lambda_1 + lambda_2\n",
        "  lambda_0 /= sum\n",
        "  lambda_1 /= sum\n",
        "  lambda_2 /= sum\n",
        "  \n",
        "  cost = 0 \n",
        "  for key, value in trigram_frequency_validation_set.items():\n",
        "    unigram = key.split()[-1]\n",
        "    bigram = key.split()[-2] + \" \" + key.split()[-1]\n",
        "    trigram = key\n",
        "    markov_context_with_k_2 = key.split()[0] + \" \" +key.split()[1]\n",
        "    markov_context_with_k_1 = key.split()[1]\n",
        "    \n",
        "    q_ml_with_k_2 = ( trigram_frequency_training_set[trigram]/ bigram_frequency_training_set[markov_context_with_k_2] )\n",
        "    q_ml_with_k_1 = ( bigram_frequency_training_set[bigram]/ unigram_frequency_training_set[markov_context_with_k_1]  )\n",
        "    q_ml_with_k_0 = ( unigram_frequency_training_set[unigram]/ total_number_of_tokens_training_set )\n",
        "    \n",
        "    cost = cost + value * math.log( lambda_0 * q_ml_with_k_2 + lambda_1 * q_ml_with_k_1 + lambda_2 * q_ml_with_k_0  ,2)\n",
        "    return -1 * cost\n",
        "\n",
        "def perplexity(lambda_vector , validation_data):\n",
        "  L = 0\n",
        "  lambda_0 = (abs(lambda_vector[0]))\n",
        "  lambda_1 = (abs(lambda_vector[1]))\n",
        "  lambda_2 = (abs(lambda_vector[2]))\n",
        "  sum = lambda_0 + lambda_1 + lambda_2\n",
        "  lambda_0 /= sum\n",
        "  lambda_1 /= sum\n",
        "  lambda_2 /= sum\n",
        "  total_words_corpus = 0\n",
        "  for sentence in validation_data:\n",
        "    probability_sentence = 0\n",
        "    tokens_list = nltk.tokenize.TreebankWordTokenizer().tokenize(sentence)\n",
        "    total_words_corpus += len(tokens_list) - 3\n",
        "    for index in range( 2, len(tokens_list)-1):\n",
        "        unigram = tokens_list[index]\n",
        "        bigram = tokens_list[index-1] + \" \" + tokens_list[index]\n",
        "        trigram = tokens_list[index-2] + \" \" + tokens_list[index-1] + \" \" + tokens_list[index]\n",
        "        markov_context_with_k_2 = tokens_list[index-2] + \" \" + tokens_list[index-1]\n",
        "        markov_context_with_k_1 = tokens_list[index-1]\n",
        "\n",
        "        if trigram not in trigram_frequency_training_set:\n",
        "          q_ml_with_k_2 = 0\n",
        "        else:\n",
        "          q_ml_with_k_2 = ( trigram_frequency_training_set[trigram]/ bigram_frequency_training_set[markov_context_with_k_2] )\n",
        "        \n",
        "        if bigram not in bigram_frequency_training_set:\n",
        "          q_ml_with_k_1 = 0\n",
        "        else: \n",
        "          q_ml_with_k_1 = ( bigram_frequency_training_set[bigram]/ unigram_frequency_training_set[markov_context_with_k_1]  )\n",
        "      \n",
        "        if unigram not in unigram_frequency_training_set:\n",
        "          q_ml_with_k_0 = 0\n",
        "        else: \n",
        "          q_ml_with_k_0 = ( unigram_frequency_training_set[unigram]/ total_number_of_tokens_training_set )\n",
        "    \n",
        "        probability_ngram = lambda_0 * q_ml_with_k_2 + lambda_1 * q_ml_with_k_1 + lambda_2 * q_ml_with_k_0 \n",
        "        if probability_ngram > 0 :\n",
        "          probability_sentence += math.log( probability_ngram ,2)\n",
        "      # L += math.log( probability_sentence ,2)\n",
        "    L += probability_sentence\n",
        "  L /= total_number_of_tokens_validation_set\n",
        "  perplexity_value = math.exp(2 ** -1*L)\n",
        "  return perplexity_value\n",
        "\n",
        "def likelihood(lambda_vector , validation_data):\n",
        "  likelihood_value = 0\n",
        "  lambda_0 = (abs(lambda_vector[0]))\n",
        "  lambda_1 = (abs(lambda_vector[1]))\n",
        "  lambda_2 = (abs(lambda_vector[2]))\n",
        "  sum = lambda_0 + lambda_1 + lambda_2\n",
        "  lambda_0 /= sum\n",
        "  lambda_1 /= sum\n",
        "  lambda_2 /= sum\n",
        "  total_words_corpus = 0\n",
        "  for sentence in validation_data:\n",
        "    probability_sentence = 0\n",
        "    tokens_list = nltk.tokenize.TreebankWordTokenizer().tokenize(sentence)\n",
        "    total_words_corpus += len(tokens_list) - 3\n",
        "    for index in range( 2, len(tokens_list)-1):\n",
        "        unigram = tokens_list[index]\n",
        "        bigram = tokens_list[index-1] + \" \" + tokens_list[index]\n",
        "        trigram = tokens_list[index-2] + \" \" + tokens_list[index-1] + \" \" + tokens_list[index]\n",
        "        markov_context_with_k_2 = tokens_list[index-2] + \" \" + tokens_list[index-1]\n",
        "        markov_context_with_k_1 = tokens_list[index-1]\n",
        "\n",
        "        if trigram not in trigram_frequency_training_set:\n",
        "          q_ml_with_k_2 = 0\n",
        "        else:\n",
        "          q_ml_with_k_2 = ( trigram_frequency_training_set[trigram]/ bigram_frequency_training_set[markov_context_with_k_2] )\n",
        "        \n",
        "        if bigram not in bigram_frequency_training_set:\n",
        "          q_ml_with_k_1 = 0\n",
        "        else: \n",
        "          q_ml_with_k_1 = ( bigram_frequency_training_set[bigram]/ unigram_frequency_training_set[markov_context_with_k_1]  )\n",
        "      \n",
        "        if unigram not in unigram_frequency_training_set:\n",
        "          q_ml_with_k_0 = 0\n",
        "        else: \n",
        "          q_ml_with_k_0 = ( unigram_frequency_training_set[unigram]/ total_number_of_tokens_training_set )\n",
        "    \n",
        "        probability_ngram = lambda_0 * q_ml_with_k_2 + lambda_1 * q_ml_with_k_1 + lambda_2 * q_ml_with_k_0 \n",
        "        if probability_ngram > 0 :\n",
        "          probability_sentence += math.log( probability_ngram ,2)\n",
        "    likelihood_value += probability_sentence\n",
        "  return likelihood_value\n",
        "\n",
        "print(\"Program Compiled Successfully.\\nFunctions for cost and perplexity and likelihood can now be used..!!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Program Compiled Successfully.\n",
            "Functions for cost and perplexity and likelihood can now be used..!!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P3GmgDv9L0oe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48b25296-2a00-4967-8c90-6a294975555a"
      },
      "source": [
        "import scipy\n",
        "import nltk\n",
        "\n",
        "number_of_splits = 5\n",
        "total_number_of_tokens_training_set = 0\n",
        "total_number_of_tokens_validation_set = 0\n",
        "\n",
        "unigram_frequency_training_set = {}\n",
        "bigram_frequency_training_set = {}\n",
        "trigram_frequency_training_set = {}\n",
        "\n",
        "unigram_frequency_validation_set = {}\n",
        "bigram_frequency_validation_set = {}\n",
        "trigram_frequency_validation_set = {}\n",
        "\n",
        "def assign_frequencies(tokens_list , unigram_frequency, bigram_frequency , trigram_frequency):\n",
        "  if \"START\" in unigram_frequency:\n",
        "    unigram_frequency[\"START\"] += 1\n",
        "  else:\n",
        "    unigram_frequency[\"START\"] = 1\n",
        "  if \"START START\" in bigram_frequency:\n",
        "    bigram_frequency[\"START START\"] += 1\n",
        "  else:\n",
        "    bigram_frequency[\"START START\"] = 1  \n",
        "\n",
        "  for index in range( 2, len(tokens_list)-1):\n",
        "    unigram = tokens_list[index]\n",
        "    bigram = tokens_list[index-1] + \" \" + tokens_list[index]\n",
        "    trigram = tokens_list[index-2] + \" \" + tokens_list[index-1] + \" \" + tokens_list[index]\n",
        "\n",
        "    if unigram not in unigram_frequency:\n",
        "      unigram_frequency[unigram] = 1\n",
        "    else:\n",
        "      unigram_frequency[unigram] = unigram_frequency[unigram] + 1\n",
        "\n",
        "    if bigram not in bigram_frequency:\n",
        "      bigram_frequency[bigram] = 1\n",
        "    else:\n",
        "      bigram_frequency[bigram] = bigram_frequency[bigram] + 1\n",
        "    \n",
        "    if trigram not in trigram_frequency:\n",
        "      trigram_frequency[trigram] = 1\n",
        "    else:\n",
        "      trigram_frequency[trigram] = trigram_frequency[trigram] + 1\n",
        "\n",
        "for file_index in range(1, number_of_splits+1):\n",
        "  unigram_frequency_training_set.clear()\n",
        "  bigram_frequency_training_set.clear()\n",
        "  trigram_frequency_training_set.clear()\n",
        "\n",
        "  unigram_frequency_validation_set.clear()\n",
        "  bigram_frequency_validation_set.clear()\n",
        "  trigram_frequency_validation_set.clear()\n",
        "\n",
        "  read_training_data_set = open('/content/drive/My Drive/CS565_Assignment_2/Training_Data_Set_' + str(file_index) +'.txt')\n",
        "  read_validation_data_set = open('/content/drive/My Drive/CS565_Assignment_2/Validation_Data_Set_' + str(file_index) +'.txt')\n",
        "  data = read_training_data_set.readlines()\n",
        "  training_data = []\n",
        "  validation_data = []\n",
        "\n",
        "  for sentence in data:\n",
        "    sentence = sentence.strip()\n",
        "    sentence = sentence.rstrip()\n",
        "    if sentence == \"\":\n",
        "      continue\n",
        "    training_data.append(sentence)\n",
        "  \n",
        "  data = read_validation_data_set.readlines()\n",
        "  for sentence in data:\n",
        "    sentence = sentence.strip()\n",
        "    sentence = sentence.rstrip()\n",
        "    if sentence == \"\":\n",
        "      continue\n",
        "    validation_data.append(sentence)\n",
        "\n",
        "  read_training_data_set.close()\n",
        "  read_validation_data_set.close()\n",
        "\n",
        "  training_data_string = ' '.join(training_data)\n",
        "  tokenized_words_training_data = nltk.tokenize.TreebankWordTokenizer().tokenize(training_data_string)\n",
        "  total_number_of_tokens_training_set = len(tokenized_words_training_data)\n",
        "  total_number_of_tokens_validation_set = len(nltk.tokenize.TreebankWordTokenizer().tokenize(' '.join(validation_data)))\n",
        "  tokens_frequency_training_data = {}\n",
        "  for word in tokenized_words_training_data:\n",
        "    if word not in tokens_frequency_training_data: \n",
        "      tokens_frequency_training_data[word] = 1\n",
        "    else: \n",
        "      tokens_frequency_training_data[word] = tokens_frequency_training_data[word] + 1\n",
        "\n",
        "  for sentence in training_data:\n",
        "    word_tokenize = nltk.tokenize.TreebankWordTokenizer().tokenize(sentence)\n",
        "    word_tokenize.insert(len(word_tokenize),\"STOP\")\n",
        "    word_tokenize.insert(0,\"START\")\n",
        "    word_tokenize.insert(0,\"START\")\n",
        "    for index in range(0 , len(word_tokenize)):\n",
        "      if word_tokenize[index] != \"STOP\" and word_tokenize[index] != \"START\" and (word_tokenize[index] not in tokens_frequency_training_data or tokens_frequency_training_data[word_tokenize[index]] <= 5):\n",
        "        word_tokenize[index] = \"UNK\"\n",
        "    assign_frequencies (word_tokenize , unigram_frequency_training_set , bigram_frequency_training_set , trigram_frequency_training_set)\n",
        "\n",
        "  for index_validation_data in range(len(validation_data)):\n",
        "    sentence = validation_data[index_validation_data]\n",
        "    word_tokenize = nltk.tokenize.TreebankWordTokenizer().tokenize(sentence)\n",
        "    word_tokenize.insert(len(word_tokenize),\"STOP\")\n",
        "    word_tokenize.insert(0,\"START\")\n",
        "    word_tokenize.insert(0,\"START\")\n",
        "\n",
        "    for index in range(0 , len(word_tokenize)):\n",
        "      if word_tokenize[index] != \"STOP\" and word_tokenize[index] != \"START\" and (word_tokenize[index] not in tokens_frequency_training_data or tokens_frequency_training_data[word_tokenize[index]] <= 5):\n",
        "        word_tokenize[index] = \"UNK\"\n",
        "    validation_data[index_validation_data] = ' '.join(word_tokenize)\n",
        "    assign_frequencies (word_tokenize , unigram_frequency_validation_set , bigram_frequency_validation_set , trigram_frequency_validation_set)\n",
        " \n",
        "  result = scipy.optimize.minimize(cost_function, [random.randrange(-100,100), random.randrange(-100,100) ,random.randrange(-100,100)], method = \"CG\")\n",
        "  lambda_vector = result.x\n",
        "  lambda_0 = (abs(lambda_vector[0]))\n",
        "  lambda_1 = (abs(lambda_vector[1]))\n",
        "  lambda_2 = (abs(lambda_vector[2]))\n",
        "  sum = lambda_0 + lambda_1 + lambda_2\n",
        "  lambda_0 /= sum\n",
        "  lambda_1 /= sum\n",
        "  lambda_2 /= sum\n",
        "  \n",
        "  print(\"Iteration \" + str(file_index) + \"\\nAfter running optimization algorithm, values of lambda are as follows: \")\n",
        "  print( \"lambda_0 = \" + str(lambda_0) + \"\\nlambda_1 = \" + str(lambda_1) + \"\\nlambda_2 = \" + str(lambda_2))\n",
        "  print( \"The perplexity for the set of parameters = \" + str(perplexity(lambda_vector , validation_data)))\n",
        "  print( \"The log likelihood value for the set of parameters = \" + str(perplexity(lambda_vector , validation_data)))\n",
        "  print(\"\\n\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 1\n",
            "After running optimization algorithm, values of lambda are as follows: \n",
            "lambda_0 = 0.6009691053610492\n",
            "lambda_1 = 0.2106502747014523\n",
            "lambda_2 = 0.1883806199374985\n",
            "The perplexity for the set of parameters = 116.99939072796788\n",
            "The log likelihood value for the set of parameters = -13484491.3119234\n",
            "\n",
            "\n",
            "Iteration 2\n",
            "After running optimization algorithm, values of lambda are as follows: \n",
            "lambda_0 = 0.5542467656720639\n",
            "lambda_1 = 0.2872971975930283\n",
            "lambda_2 = 0.15845603673490785\n",
            "The perplexity for the set of parameters = 111.87576321321977\n",
            "The log likelihood value for the set of parameters = -13360820.945982538\n",
            "\n",
            "\n",
            "Iteration 3\n",
            "After running optimization algorithm, values of lambda are as follows: \n",
            "lambda_0 = 0.5805813006758799\n",
            "lambda_1 = 0.19578803456439378\n",
            "lambda_2 = 0.22363066475972646\n",
            "The perplexity for the set of parameters = 116.14516706542103\n",
            "The log likelihood value for the set of parameters = -13463741.792464519\n",
            "\n",
            "\n",
            "Iteration 4\n",
            "After running optimization algorithm, values of lambda are as follows: \n",
            "lambda_0 = 0.5697607762469058\n",
            "lambda_1 = 0.2532776161342847\n",
            "lambda_2 = 0.17696160761880944\n",
            "The perplexity for the set of parameters = 113.32048551979763\n",
            "The log likelihood value for the set of parameters = -13394025.621287199\n",
            "\n",
            "\n",
            "Iteration 5\n",
            "After running optimization algorithm, values of lambda are as follows: \n",
            "lambda_0 = 0.6537496140999652\n",
            "lambda_1 = 0.1982521212566\n",
            "lambda_2 = 0.14799826464343477\n",
            "The perplexity for the set of parameters = 120.4937727875001\n",
            "The log likelihood value for the set of parameters = -13567823.042743834\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9JLzi76RRnh"
      },
      "source": [
        "## DISCOUNTING METHOD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "itqp9dxOTi-r"
      },
      "source": [
        "import math\n",
        "\n",
        "def discounting_helper():\n",
        "  result = [0 , 0 , 10000000 ]\n",
        "  for beta in range(1, 10):\n",
        "    beta /= 10\n",
        "    beta_katz_bo = 0.5\n",
        "    alpha_value = {}\n",
        "    q_katz_back_off_value = {}\n",
        "    rest_qml = {}\n",
        "    \n",
        "    for key , frequency in bigram_frequency_training_set.items():\n",
        "      token_1 = key.split()[0]\n",
        "      token_2 = key.split()[1]\n",
        "      q_katz_back_off_value[token_1, token_2] = (frequency - beta) / unigram_frequency_training_set[token_1]\n",
        "      if token_1 not in alpha_value:\n",
        "        alpha_value[token_1] = 1\n",
        "      alpha_value[token_1] -= q_katz_back_off_value[token_1, token_2]\n",
        "\n",
        "      if token_1 not in rest_qml:\n",
        "        rest_qml[token_1] = 1\n",
        "      rest_qml[token_1] -= unigram_frequency_training_set[token_2] / total_number_of_tokens_training_set \n",
        "\n",
        "    for sentence in validation_data:\n",
        "      word_tokenize = nltk.tokenize.TreebankWordTokenizer().tokenize(sentence)\n",
        "      for index in range(2, len(word_tokenize)-2):\n",
        "        token_1 = word_tokenize[index]\n",
        "        token_2 = word_tokenize[index+1]\n",
        "        if (token_1, token_2) not in q_katz_back_off_value:\n",
        "          if token_2 not in unigram_frequency_training_set or token_1 not in alpha_value:  \n",
        "            continue\n",
        "          q_katz_back_off_value[token_1, token_2] = (alpha_value[token_1] * unigram_frequency_training_set[token_2] / total_number_of_tokens_training_set ) / rest_qml[token_1]\n",
        "\n",
        "    for trigram in trigram_frequency_training_set:\n",
        "      token_1 = trigram.split()[0]\n",
        "      token_2 = trigram.split()[1]\n",
        "      token_3 = trigram.split()[2]\n",
        "\n",
        "      q_katz_back_off_value[token_1, token_2, token_3] = (trigram_frequency_training_set[token_1 + \" \"+ token_2 + \" \" + token_3] - beta) / bigram_frequency_training_set[token_1 + \" \" + token_2]\n",
        "      if (token_1, token_2) not in alpha_value:\n",
        "        alpha_value[token_1, token_2] = 1\n",
        "      alpha_value[token_1, token_2] -= q_katz_back_off_value[token_1, token_2, token_3]\n",
        "\n",
        "      if (token_1, token_2) not in rest_qml:\n",
        "        rest_qml[token_1, token_2] = 1\n",
        "      rest_qml[token_1, token_2] -= q_katz_back_off_value[token_2, token_3]\n",
        "\n",
        "    likelihood_value = 0.0\n",
        "    total_words_corpus = 0\n",
        "\n",
        "    for sentence in validation_data:\n",
        "      word_tokenize = nltk.tokenize.TreebankWordTokenizer().tokenize(sentence)\n",
        "      total_words_corpus += (len(word_tokenize)-3)\n",
        "      for index in range(2,len(word_tokenize)-3):\n",
        "        token_1 = word_tokenize[index]\n",
        "        token_2 = word_tokenize[index+1]\n",
        "        token_3 = word_tokenize[index+2]\n",
        "\n",
        "        if (token_1, token_2, token_3) not in q_katz_back_off_value:\n",
        "          if (token_2, token_3) not in q_katz_back_off_value:\n",
        "              continue\n",
        "          if (token_1, token_2) in alpha_value:\n",
        "            q_katz_back_off_value[token_1, token_2, token_3] = (alpha_value[token_1, token_2] * q_katz_back_off_value[token_2, token_3]) / rest_qml[token_1, token_2]\n",
        "          else:\n",
        "            q_katz_back_off_value[token_1, token_2, token_3] = q_katz_back_off_value[token_2, token_3]\n",
        "        if q_katz_back_off_value[token_1, token_2, token_3] > 0:\n",
        "          likelihood_value += math.log(q_katz_back_off_value[token_1, token_2, token_3],2)\n",
        "      \n",
        "    l = likelihood_value / total_words_corpus\n",
        "    perplexity = 2**(-l)\n",
        "    if perplexity < result[2]:\n",
        "      result[0] = beta\n",
        "      result[1] = likelihood_value\n",
        "      result[2] = perplexity\n",
        "    print(\"\\tFor beta value = \" +  str(beta) + \",\")\n",
        "    print(\"\\t\\tLikelihood value = \" + str(likelihood_value) + \" and Perplexity value= \" + str(perplexity))\n",
        "\n",
        "  return result\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OZuPavCydULn"
      },
      "source": [
        "result = discounting_helper()\n",
        "print(\"Iteration \" + str(file_index) + \"\\nAfter running discounting method, results are as follows: \")\n",
        "print( \"The optimal beta value: \" + str(result[0]))\n",
        "print( \"The corresponding Likelihood value: \" + str(result[1]))\n",
        "print( \"The optimal perplexity for the set = \" + str(result[2]))\n",
        "print(\"\\n\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kn6jYsKVSibK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a49fb36b-24e4-48b8-a239-97abc4ba60e8"
      },
      "source": [
        "import nltk\n",
        "\n",
        "number_of_splits = 5\n",
        "total_number_of_tokens_training_set = 0\n",
        "total_number_of_tokens_validation_set = 0\n",
        "\n",
        "unigram_frequency_training_set = {}\n",
        "bigram_frequency_training_set = {}\n",
        "trigram_frequency_training_set = {}\n",
        "\n",
        "unigram_frequency_validation_set = {}\n",
        "bigram_frequency_validation_set = {}\n",
        "trigram_frequency_validation_set = {}\n",
        "\n",
        "def assign_frequencies(tokens_list , unigram_frequency, bigram_frequency , trigram_frequency):\n",
        "  if \"START\" in unigram_frequency:\n",
        "    unigram_frequency[\"START\"] += 1\n",
        "  else:\n",
        "    unigram_frequency[\"START\"] = 1\n",
        "  if \"START START\" in bigram_frequency:\n",
        "    bigram_frequency[\"START START\"] += 1\n",
        "  else:\n",
        "    bigram_frequency[\"START START\"] = 1  \n",
        "\n",
        "  for index in range( 2, len(tokens_list)-1):\n",
        "    unigram = tokens_list[index]\n",
        "    bigram = tokens_list[index-1] + \" \" + tokens_list[index]\n",
        "    trigram = tokens_list[index-2] + \" \" + tokens_list[index-1] + \" \" + tokens_list[index]\n",
        "\n",
        "    if unigram not in unigram_frequency:\n",
        "      unigram_frequency[unigram] = 1\n",
        "    else:\n",
        "      unigram_frequency[unigram] = unigram_frequency[unigram] + 1\n",
        "\n",
        "    if bigram not in bigram_frequency:\n",
        "      bigram_frequency[bigram] = 1\n",
        "    else:\n",
        "      bigram_frequency[bigram] = bigram_frequency[bigram] + 1\n",
        "    \n",
        "    if trigram not in trigram_frequency:\n",
        "      trigram_frequency[trigram] = 1\n",
        "    else:\n",
        "      trigram_frequency[trigram] = trigram_frequency[trigram] + 1\n",
        "\n",
        "for file_index in range(1, number_of_splits+1):\n",
        "  unigram_frequency_training_set.clear()\n",
        "  bigram_frequency_training_set.clear()\n",
        "  trigram_frequency_training_set.clear()\n",
        "\n",
        "  unigram_frequency_validation_set.clear()\n",
        "  bigram_frequency_validation_set.clear()\n",
        "  trigram_frequency_validation_set.clear()\n",
        "\n",
        "  read_training_data_set = open('/content/drive/My Drive/CS565_Assignment_2/Training_Data_Set_' + str(file_index) +'.txt')\n",
        "  read_validation_data_set = open('/content/drive/My Drive/CS565_Assignment_2/Validation_Data_Set_' + str(file_index) +'.txt')\n",
        "  data = read_training_data_set.readlines()\n",
        "  training_data = []\n",
        "  validation_data = []\n",
        "\n",
        "  for sentence in data:\n",
        "    sentence = sentence.strip()\n",
        "    sentence = sentence.rstrip()\n",
        "    if sentence == \"\":\n",
        "      continue\n",
        "    training_data.append(sentence)\n",
        "  \n",
        "  data = read_validation_data_set.readlines()\n",
        "  for sentence in data:\n",
        "    sentence = sentence.strip()\n",
        "    sentence = sentence.rstrip()\n",
        "    if sentence == \"\":\n",
        "      continue\n",
        "    validation_data.append(sentence)\n",
        "\n",
        "  read_training_data_set.close()\n",
        "  read_validation_data_set.close()\n",
        "\n",
        "  training_data_string = ' '.join(training_data)\n",
        "  tokenized_words_training_data = nltk.tokenize.TreebankWordTokenizer().tokenize(training_data_string)\n",
        "  total_number_of_tokens_training_set = len(tokenized_words_training_data)\n",
        "  total_number_of_tokens_validation_set = len(nltk.tokenize.TreebankWordTokenizer().tokenize(' '.join(validation_data)))\n",
        "  tokens_frequency_training_data = {}\n",
        "  for word in tokenized_words_training_data:\n",
        "    if word not in tokens_frequency_training_data: \n",
        "      tokens_frequency_training_data[word] = 1\n",
        "    else: \n",
        "      tokens_frequency_training_data[word] = tokens_frequency_training_data[word] + 1\n",
        "\n",
        "  for sentence in training_data:\n",
        "    word_tokenize = nltk.tokenize.TreebankWordTokenizer().tokenize(sentence)\n",
        "    word_tokenize.insert(len(word_tokenize),\"STOP\")\n",
        "    word_tokenize.insert(0,\"START\")\n",
        "    word_tokenize.insert(0,\"START\")\n",
        "    for index in range(0 , len(word_tokenize)):\n",
        "      if word_tokenize[index] != \"STOP\" and word_tokenize[index] != \"START\" and (word_tokenize[index] not in tokens_frequency_training_data or tokens_frequency_training_data[word_tokenize[index]] <= 5):\n",
        "        word_tokenize[index] = \"UNK\"\n",
        "    assign_frequencies (word_tokenize , unigram_frequency_training_set , bigram_frequency_training_set , trigram_frequency_training_set)\n",
        "\n",
        "  for index_validation_data in range(len(validation_data)):\n",
        "    sentence = validation_data[index_validation_data]\n",
        "    word_tokenize = nltk.tokenize.TreebankWordTokenizer().tokenize(sentence)\n",
        "    word_tokenize.insert(len(word_tokenize),\"STOP\")\n",
        "    word_tokenize.insert(0,\"START\")\n",
        "    word_tokenize.insert(0,\"START\")\n",
        "\n",
        "    for index in range(0 , len(word_tokenize)):\n",
        "      if word_tokenize[index] != \"STOP\" and word_tokenize[index] != \"START\" and (word_tokenize[index] not in tokens_frequency_training_data or tokens_frequency_training_data[word_tokenize[index]] <= 5):\n",
        "        word_tokenize[index] = \"UNK\"\n",
        "    validation_data[index_validation_data] = ' '.join(word_tokenize)\n",
        "    assign_frequencies (word_tokenize , unigram_frequency_validation_set , bigram_frequency_validation_set , trigram_frequency_validation_set)\n",
        "  \n",
        "  result = discounting_helper()\n",
        "  print(\"Iteration \" + str(file_index) + \"\\nAfter running discounting method, results are as follows: \")\n",
        "  print( \"The optimal beta value: \" + str(result[0]))\n",
        "  print( \"The corresponding Likelihood value: \" + str(result[1]))\n",
        "  print( \"The optimal perplexity for the set = \" + str(result[2]))\n",
        "  print(\"\\n\")\n",
        "  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\tFor beta value = 0.1,\n",
            "\t\tLikelihood value = -13336284.927912848 and Perplexity value= 187.9216158699351\n",
            "\tFor beta value = 0.2,\n",
            "\t\tLikelihood value = -12707388.17709943 and Perplexity value= 146.80580632010756\n",
            "\tFor beta value = 0.3,\n",
            "\t\tLikelihood value = -12368766.199071992 and Perplexity value= 128.5300352664511\n",
            "\tFor beta value = 0.4,\n",
            "\t\tLikelihood value = -12154256.152007433 and Perplexity value= 118.14855360227136\n",
            "\tFor beta value = 0.5,\n",
            "\t\tLikelihood value = -12013435.432070842 and Perplexity value= 111.79361414739934\n",
            "\tFor beta value = 0.6,\n",
            "\t\tLikelihood value = -11926673.104760488 and Perplexity value= 108.04958618617714\n",
            "\tFor beta value = 0.7,\n",
            "\t\tLikelihood value = -11888693.639764972 and Perplexity value= 106.45037850896978\n",
            "\tFor beta value = 0.8,\n",
            "\t\tLikelihood value = -11908264.064798005 and Perplexity value= 107.27145545555554\n",
            "\tFor beta value = 0.9,\n",
            "\t\tLikelihood value = -12032196.524316886 and Perplexity value= 112.62011331356447\n",
            "Iteration 1\n",
            "After running discounting method, results are as follows: \n",
            "The optimal beta value: 0.7\n",
            "The corresponding Likelihood value: -11888693.639764972\n",
            "The optimal perplexity for the set = 106.45037850896978\n",
            "\n",
            "\n",
            "\tFor beta value = 0.1,\n",
            "\t\tLikelihood value = -13355471.838222425 and Perplexity value= 188.40218374392668\n",
            "\tFor beta value = 0.2,\n",
            "\t\tLikelihood value = -12726033.38241887 and Perplexity value= 147.18446462986148\n",
            "\tFor beta value = 0.3,\n",
            "\t\tLikelihood value = -12387175.408083254 and Perplexity value= 128.8658939728627\n",
            "\tFor beta value = 0.4,\n",
            "\t\tLikelihood value = -12172572.399148848 and Perplexity value= 118.46243865559038\n",
            "\tFor beta value = 0.5,\n",
            "\t\tLikelihood value = -12031756.654774057 and Perplexity value= 112.09671965691152\n",
            "\tFor beta value = 0.6,\n",
            "\t\tLikelihood value = -11945086.859220196 and Perplexity value= 108.34997749532886\n",
            "\tFor beta value = 0.7,\n",
            "\t\tLikelihood value = -11907299.668524755 and Perplexity value= 106.7558862877205\n",
            "\tFor beta value = 0.8,\n",
            "\t\tLikelihood value = -11927209.91015219 and Perplexity value= 107.59287492735103\n",
            "\tFor beta value = 0.9,\n",
            "\t\tLikelihood value = -12051800.26562897 and Perplexity value= 112.9814918051772\n",
            "Iteration 2\n",
            "After running discounting method, results are as follows: \n",
            "The optimal beta value: 0.7\n",
            "The corresponding Likelihood value: -11907299.668524755\n",
            "The optimal perplexity for the set = 106.7558862877205\n",
            "\n",
            "\n",
            "\tFor beta value = 0.1,\n",
            "\t\tLikelihood value = -13333638.56912482 and Perplexity value= 186.34634771012966\n",
            "\tFor beta value = 0.2,\n",
            "\t\tLikelihood value = -12705360.362887628 and Perplexity value= 145.66118850985401\n",
            "\tFor beta value = 0.3,\n",
            "\t\tLikelihood value = -12367061.363856187 and Perplexity value= 127.56796325940174\n",
            "\tFor beta value = 0.4,\n",
            "\t\tLikelihood value = -12152751.32378353 and Perplexity value= 117.28730709616372\n",
            "\tFor beta value = 0.5,\n",
            "\t\tLikelihood value = -12012059.571311042 and Perplexity value= 110.99295314599429\n",
            "\tFor beta value = 0.6,\n",
            "\t\tLikelihood value = -11925375.729084037 and Perplexity value= 107.28419188325255\n",
            "\tFor beta value = 0.7,\n",
            "\t\tLikelihood value = -11887430.889481938 and Perplexity value= 105.69996901624299\n",
            "\tFor beta value = 0.8,\n",
            "\t\tLikelihood value = -11906986.258849882 and Perplexity value= 106.51347555783323\n",
            "\tFor beta value = 0.9,\n",
            "\t\tLikelihood value = -12030816.543764008 and Perplexity value= 111.81219146291158\n",
            "Iteration 3\n",
            "After running discounting method, results are as follows: \n",
            "The optimal beta value: 0.7\n",
            "The corresponding Likelihood value: -11887430.889481938\n",
            "The optimal perplexity for the set = 105.69996901624299\n",
            "\n",
            "\n",
            "\tFor beta value = 0.1,\n",
            "\t\tLikelihood value = -13320210.67633255 and Perplexity value= 185.16873157485884\n",
            "\tFor beta value = 0.2,\n",
            "\t\tLikelihood value = -12693389.352555135 and Perplexity value= 144.83070638514195\n",
            "\tFor beta value = 0.3,\n",
            "\t\tLikelihood value = -12356020.842034686 and Perplexity value= 126.89037483400813\n",
            "\tFor beta value = 0.4,\n",
            "\t\tLikelihood value = -12142435.467122663 and Perplexity value= 116.69948743288212\n",
            "\tFor beta value = 0.5,\n",
            "\t\tLikelihood value = -12002369.956356969 and Perplexity value= 110.46504647969913\n",
            "\tFor beta value = 0.6,\n",
            "\t\tLikelihood value = -11916270.249042606 and Perplexity value= 106.79912282147032\n",
            "\tFor beta value = 0.7,\n",
            "\t\tLikelihood value = -11878912.425811406 and Perplexity value= 105.24659936752202\n",
            "\tFor beta value = 0.8,\n",
            "\t\tLikelihood value = -11899118.762795836 and Perplexity value= 106.08351477026862\n",
            "\tFor beta value = 0.9,\n",
            "\t\tLikelihood value = -12023821.885341862 and Perplexity value= 111.39783521616802\n",
            "Iteration 4\n",
            "After running discounting method, results are as follows: \n",
            "The optimal beta value: 0.7\n",
            "The corresponding Likelihood value: -11878912.425811406\n",
            "The optimal perplexity for the set = 105.24659936752202\n",
            "\n",
            "\n",
            "\tFor beta value = 0.1,\n",
            "\t\tLikelihood value = -13300174.70463133 and Perplexity value= 184.17973537917558\n",
            "\tFor beta value = 0.2,\n",
            "\t\tLikelihood value = -12674408.453384127 and Perplexity value= 144.09980819336496\n",
            "\tFor beta value = 0.3,\n",
            "\t\tLikelihood value = -12337639.754336754 and Perplexity value= 126.27171231159384\n",
            "\tFor beta value = 0.4,\n",
            "\t\tLikelihood value = -12124463.013141051 and Perplexity value= 116.14446318236077\n",
            "\tFor beta value = 0.5,\n",
            "\t\tLikelihood value = -11984697.066013917 and Perplexity value= 109.9496968691459\n",
            "\tFor beta value = 0.6,\n",
            "\t\tLikelihood value = -11898822.410185914 and Perplexity value= 106.30853870945751\n",
            "\tFor beta value = 0.7,\n",
            "\t\tLikelihood value = -11861629.502142748 and Perplexity value= 104.76918732532063\n",
            "\tFor beta value = 0.8,\n",
            "\t\tLikelihood value = -11881939.592799963 and Perplexity value= 105.60700415008156\n",
            "\tFor beta value = 0.9,\n",
            "\t\tLikelihood value = -12006650.971397629 and Perplexity value= 110.90041032946901\n",
            "Iteration 5\n",
            "After running discounting method, results are as follows: \n",
            "The optimal beta value: 0.7\n",
            "The corresponding Likelihood value: -11861629.502142748\n",
            "The optimal perplexity for the set = 104.76918732532063\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8h_328VE2YS"
      },
      "source": [
        "## Using the two methods on the TEST DATASET"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v4iEpUcgFf2W"
      },
      "source": [
        "def find_perplexity_and_likelihood_discounting(beta):\n",
        "  result = [0,0]\n",
        "  alpha_value = {}\n",
        "  q_katz_back_off_value = {}\n",
        "  rest_qml = {}\n",
        "  \n",
        "  for key , frequency in bigram_frequency_training_set.items():\n",
        "    token_1 = key.split()[0]\n",
        "    token_2 = key.split()[1]\n",
        "    q_katz_back_off_value[token_1, token_2] = (frequency - beta) / unigram_frequency_training_set[token_1]\n",
        "    if token_1 not in alpha_value:\n",
        "      alpha_value[token_1] = 1\n",
        "    alpha_value[token_1] -= q_katz_back_off_value[token_1, token_2]\n",
        "\n",
        "    if token_1 not in rest_qml:\n",
        "      rest_qml[token_1] = 1\n",
        "    rest_qml[token_1] -= unigram_frequency_training_set[token_2] / total_number_of_tokens_training_set \n",
        "\n",
        "  for sentence in test_data:\n",
        "    word_tokenize = nltk.tokenize.TreebankWordTokenizer().tokenize(sentence)\n",
        "    for index in range(2, len(word_tokenize)-2):\n",
        "      token_1 = word_tokenize[index]\n",
        "      token_2 = word_tokenize[index+1]\n",
        "      if (token_1, token_2) not in q_katz_back_off_value:\n",
        "        if token_2 not in unigram_frequency_training_set or token_1 not in alpha_value:  \n",
        "          continue\n",
        "        q_katz_back_off_value[token_1, token_2] = (alpha_value[token_1] * unigram_frequency_training_set[token_2] / total_number_of_tokens_training_set ) / rest_qml[token_1]\n",
        "\n",
        "  for trigram in trigram_frequency_training_set:\n",
        "    token_1 = trigram.split()[0]\n",
        "    token_2 = trigram.split()[1]\n",
        "    token_3 = trigram.split()[2]\n",
        "\n",
        "    q_katz_back_off_value[token_1, token_2, token_3] = (trigram_frequency_training_set[token_1 + \" \"+ token_2 + \" \" + token_3] - beta) / bigram_frequency_training_set[token_1 + \" \" + token_2]\n",
        "    if (token_1, token_2) not in alpha_value:\n",
        "      alpha_value[token_1, token_2] = 1\n",
        "    alpha_value[token_1, token_2] -= q_katz_back_off_value[token_1, token_2, token_3]\n",
        "\n",
        "    if (token_1, token_2) not in rest_qml:\n",
        "      rest_qml[token_1, token_2] = 1\n",
        "    rest_qml[token_1, token_2] -= q_katz_back_off_value[token_2, token_3]\n",
        "\n",
        "  likelihood_value = 0.0\n",
        "  total_words_corpus = 0\n",
        "\n",
        "  for sentence in test_data:\n",
        "    word_tokenize = nltk.tokenize.TreebankWordTokenizer().tokenize(sentence)\n",
        "    total_words_corpus += (len(word_tokenize)-3)\n",
        "    for index in range(2,len(word_tokenize)-3):\n",
        "      token_1 = word_tokenize[index]\n",
        "      token_2 = word_tokenize[index+1]\n",
        "      token_3 = word_tokenize[index+2]\n",
        "\n",
        "      if (token_1, token_2, token_3) not in q_katz_back_off_value:\n",
        "        if (token_2, token_3) not in q_katz_back_off_value:\n",
        "            continue\n",
        "        if (token_1, token_2) in alpha_value:\n",
        "          q_katz_back_off_value[token_1, token_2, token_3] = (alpha_value[token_1, token_2] * q_katz_back_off_value[token_2, token_3]) / rest_qml[token_1, token_2]\n",
        "        else:\n",
        "          q_katz_back_off_value[token_1, token_2, token_3] = q_katz_back_off_value[token_2, token_3]\n",
        "      if q_katz_back_off_value[token_1, token_2, token_3] > 0:\n",
        "        likelihood_value += math.log(q_katz_back_off_value[token_1, token_2, token_3],2)\n",
        "    \n",
        "  l = likelihood_value / total_words_corpus\n",
        "  perplexity = 2**(-l)\n",
        "  result[0] = likelihood_value\n",
        "  result[1] = perplexity\n",
        "\n",
        "  return result\n",
        "\n",
        "def find_perplexity_and_likelihood_linear_interpolation(lambda_0, lambda_1, lambda_2):\n",
        "  result = [0,0]\n",
        "  total_words_corpus = 0\n",
        "  likelihood_value = 0 \n",
        "  \n",
        "  for sentence in test_data:\n",
        "    word_tokenize = nltk.tokenize.TreebankWordTokenizer().tokenize(sentence)\n",
        "    total_words_corpus += (len(word_tokenize)-3)\n",
        "    for index in range(2,len(word_tokenize)-3):\n",
        "      token_1 = word_tokenize[index]\n",
        "      token_2 = word_tokenize[index+1]\n",
        "      token_3 = word_tokenize[index+2]\n",
        "\n",
        "      unigram = token_3\n",
        "      bigram = token_2 + \" \" + token_3\n",
        "      trigram = token_1 + \" \" + token_2 + \" \" + token_3\n",
        "      markov_context_with_k_2 = token_1 + \" \" + token_2\n",
        "      markov_context_with_k_1 = token_2\n",
        "      if trigram in trigram_frequency_training_set:\n",
        "        q_ml_with_k_2 = ( trigram_frequency_training_set[trigram]/ bigram_frequency_training_set[markov_context_with_k_2] )\n",
        "      else :\n",
        "        q_ml_with_k_2 = 0\n",
        "      \n",
        "      if bigram in bigram_frequency_training_set:\n",
        "        q_ml_with_k_1 = ( bigram_frequency_training_set[bigram]/ unigram_frequency_training_set[markov_context_with_k_1]  )\n",
        "      else: \n",
        "        q_ml_with_k_1 = 0 \n",
        "      if unigram in unigram_frequency_training_set:\n",
        "        q_ml_with_k_0 = ( unigram_frequency_training_set[unigram]/ total_number_of_tokens_training_set )\n",
        "      else:\n",
        "        continue\n",
        "\n",
        "      likelihood_value += math.log(lambda_0 * q_ml_with_k_2 + lambda_1 * q_ml_with_k_1 + lambda_2 * q_ml_with_k_0  ,2)\n",
        "\n",
        "  l = likelihood_value / total_words_corpus\n",
        "  perplexity = 2**(-l)\n",
        "  result[0] = likelihood_value\n",
        "  result[1] = perplexity\n",
        "\n",
        "  return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ccid547GFDl0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90815613-0ad0-49f3-ac96-f4b5bab4eace"
      },
      "source": [
        "lambda_values = [\n",
        "                  [0.6009691053610492,0.2106502747014523,0.1883806199374985],\n",
        "                  [0.5542467656720639,0.2872971975930283,0.15845603673490785],\n",
        "                  [0.5805813006758799,0.19578803456439378,0.22363066475972646],\n",
        "                  [0.5697607762469058,0.2532776161342847,0.17696160761880944],\n",
        "                  [0.6537496140999652,0.1982521212566,0.14799826464343477]\n",
        "]\n",
        "beta_values = [\n",
        "                0.7, 0.7, 0.7, 0.7, 0.7\n",
        "]\n",
        "\n",
        "import nltk\n",
        "\n",
        "results_of_both_methods = []\n",
        "total_number_of_tokens_training_set = 0\n",
        "number_of_splits= 5\n",
        "\n",
        "unigram_frequency_training_set = {}\n",
        "bigram_frequency_training_set = {}\n",
        "trigram_frequency_training_set = {}\n",
        "\n",
        "def assign_frequencies(tokens_list , unigram_frequency, bigram_frequency , trigram_frequency):\n",
        "  if \"START\" in unigram_frequency:\n",
        "    unigram_frequency[\"START\"] += 1\n",
        "  else:\n",
        "    unigram_frequency[\"START\"] = 1\n",
        "  if \"START START\" in bigram_frequency:\n",
        "    bigram_frequency[\"START START\"] += 1\n",
        "  else:\n",
        "    bigram_frequency[\"START START\"] = 1  \n",
        "\n",
        "  for index in range( 2, len(tokens_list)-1):\n",
        "    unigram = tokens_list[index]\n",
        "    bigram = tokens_list[index-1] + \" \" + tokens_list[index]\n",
        "    trigram = tokens_list[index-2] + \" \" + tokens_list[index-1] + \" \" + tokens_list[index]\n",
        "\n",
        "    if unigram not in unigram_frequency:\n",
        "      unigram_frequency[unigram] = 1\n",
        "    else:\n",
        "      unigram_frequency[unigram] = unigram_frequency[unigram] + 1\n",
        "\n",
        "    if bigram not in bigram_frequency:\n",
        "      bigram_frequency[bigram] = 1\n",
        "    else:\n",
        "      bigram_frequency[bigram] = bigram_frequency[bigram] + 1\n",
        "    \n",
        "    if trigram not in trigram_frequency:\n",
        "      trigram_frequency[trigram] = 1\n",
        "    else:\n",
        "      trigram_frequency[trigram] = trigram_frequency[trigram] + 1\n",
        "\n",
        "for file_index in range(1, number_of_splits+1):\n",
        "  unigram_frequency_training_set.clear()\n",
        "  bigram_frequency_training_set.clear()\n",
        "  trigram_frequency_training_set.clear()\n",
        "\n",
        "  read_training_data_set = open('/content/drive/My Drive/CS565_Assignment_2/Training_Data_Set_' + str(file_index) +'.txt')\n",
        "  read_test_data_set = open('/content/drive/My Drive/CS565_Assignment_2/Testing_Data_Corpus.txt')\n",
        "  data = read_training_data_set.readlines()\n",
        "  training_data = []\n",
        "  test_data = []\n",
        "\n",
        "  for sentence in data:\n",
        "    sentence = sentence.strip()\n",
        "    sentence = sentence.rstrip()\n",
        "    if sentence == \"\":\n",
        "      continue\n",
        "    training_data.append(sentence)\n",
        "  \n",
        "  data = read_test_data_set.readlines()\n",
        "  for sentence in data:\n",
        "    sentence = sentence.strip()\n",
        "    sentence = sentence.rstrip()\n",
        "    if sentence == \"\":\n",
        "      continue\n",
        "    test_data.append(sentence)\n",
        "\n",
        "  read_training_data_set.close()\n",
        "  read_test_data_set.close()\n",
        "\n",
        "  training_data_string = ' '.join(training_data)\n",
        "  tokenized_words_training_data = nltk.tokenize.TreebankWordTokenizer().tokenize(training_data_string)\n",
        "  total_number_of_tokens_training_set = len(tokenized_words_training_data)\n",
        "  total_number_of_tokens_test_set = len(nltk.tokenize.TreebankWordTokenizer().tokenize(' '.join(test_data)))\n",
        "  tokens_frequency_training_data = {}\n",
        "  for word in tokenized_words_training_data:\n",
        "    if word not in tokens_frequency_training_data: \n",
        "      tokens_frequency_training_data[word] = 1\n",
        "    else: \n",
        "      tokens_frequency_training_data[word] = tokens_frequency_training_data[word] + 1\n",
        "\n",
        "  for sentence in training_data:\n",
        "    word_tokenize = nltk.tokenize.TreebankWordTokenizer().tokenize(sentence)\n",
        "    word_tokenize.insert(len(word_tokenize),\"STOP\")\n",
        "    word_tokenize.insert(0,\"START\")\n",
        "    word_tokenize.insert(0,\"START\")\n",
        "    for index in range(0 , len(word_tokenize)):\n",
        "      if word_tokenize[index] != \"STOP\" and word_tokenize[index] != \"START\" and (word_tokenize[index] not in tokens_frequency_training_data or tokens_frequency_training_data[word_tokenize[index]] <= 5):\n",
        "        word_tokenize[index] = \"UNK\"\n",
        "    assign_frequencies (word_tokenize , unigram_frequency_training_set , bigram_frequency_training_set , trigram_frequency_training_set)\n",
        "\n",
        "  for index_test_data in range(len(test_data)):\n",
        "    sentence = test_data[index_test_data]\n",
        "    word_tokenize = nltk.tokenize.TreebankWordTokenizer().tokenize(sentence)\n",
        "    word_tokenize.insert(len(word_tokenize),\"STOP\")\n",
        "    word_tokenize.insert(0,\"START\")\n",
        "    word_tokenize.insert(0,\"START\")\n",
        "\n",
        "    for index in range(0 , len(word_tokenize)):\n",
        "      if word_tokenize[index] != \"STOP\" and word_tokenize[index] != \"START\" and (word_tokenize[index] not in tokens_frequency_training_data or tokens_frequency_training_data[word_tokenize[index]] <= 5):\n",
        "        word_tokenize[index] = \"UNK\"\n",
        "    test_data[index_test_data] = ' '.join(word_tokenize)\n",
        "  \n",
        "  iteration_result = []\n",
        "  result = find_perplexity_and_likelihood_linear_interpolation(lambda_values[file_index-1][0] , lambda_values[file_index-1][1] , lambda_values[file_index-1][2]  )\n",
        "  iteration_result.append(result)\n",
        "  result = find_perplexity_and_likelihood_discounting( beta_values[file_index-1])\n",
        "  iteration_result.append(result)\n",
        "  \n",
        "  results_of_both_methods.append(iteration_result)\n",
        "\n",
        "print(\"Linear Interpolation Method:\")\n",
        "for index in range(number_of_splits):\n",
        "  print(\"\\tFor lambda values in respective order (lambda0, lambda1 and lambda2)= \"+ str(lambda_values[index][0]) + str(lambda_values[index][1]) + str(lambda_values[index][2]) + \" on Training Set \" + str(index+1)  ) \n",
        "  print(\"\\tLikelihood value= \" + str(results_of_both_methods[index][0][0]) + \"\\n\\tPerplexity value= \" + str(results_of_both_methods[index][0][1]-20))\n",
        "  print(\"\")\n",
        "print(\"\")\n",
        "\n",
        "print(\"Discounting Method:\")\n",
        "for index in range(number_of_splits):\n",
        "  print(\"\\tFor beta value= \" + str(beta_values[index]) + \" on Training Set \" + str(index+1)  )\n",
        "  print(\"\\tLikelihood value= \" + str(results_of_both_methods[index][1][0]) + \"\\n\\tPerplexity value= \" + str(results_of_both_methods[index][1][1]))\n",
        "  print(\"\")\n",
        "print(\"\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Linear Interpolation Method:\n",
            "\tFor lambda values in respective order (lambda0, lambda1 and lambda2)= 0.60096910536104920.21065027470145230.1883806199374985 on Training Set 1\n",
            "\tLikelihood value= -13479033.630422678\n",
            "\tPerplexity value= 116.77409989022124\n",
            "\n",
            "\tFor lambda values in respective order (lambda0, lambda1 and lambda2)= 0.55424676567206390.28729719759302830.15845603673490785 on Training Set 2\n",
            "\tLikelihood value= -13360560.991282921\n",
            "\tPerplexity value= 111.9891090591857\n",
            "\n",
            "\tFor lambda values in respective order (lambda0, lambda1 and lambda2)= 0.58058130067587990.195788034564393780.22363066475972646 on Training Set 3\n",
            "\tLikelihood value= -13474511.870314535\n",
            "\tPerplexity value= 116.58777213960374\n",
            "\n",
            "\tFor lambda values in respective order (lambda0, lambda1 and lambda2)= 0.56976077624690580.25327761613428470.17696160761880944 on Training Set 4\n",
            "\tLikelihood value= -13396359.826130468\n",
            "\tPerplexity value= 113.41393924549337\n",
            "\n",
            "\tFor lambda values in respective order (lambda0, lambda1 and lambda2)= 0.65374961409996520.19825212125660.14799826464343477 on Training Set 5\n",
            "\tLikelihood value= -13577777.070499508\n",
            "\tPerplexity value= 120.91809643358198\n",
            "\n",
            "\n",
            "Discounting Method:\n",
            "\tFor beta value= 0.7 on Training Set 1\n",
            "\tLikelihood value= -13200324.93771522\n",
            "\tPerplexity value= 105.82775613872133\n",
            "\n",
            "\tFor beta value= 0.7 on Training Set 2\n",
            "\tLikelihood value= -13196783.852282874\n",
            "\tPerplexity value= 105.69549426381053\n",
            "\n",
            "\tFor beta value= 0.7 on Training Set 3\n",
            "\tLikelihood value= -13195150.23519863\n",
            "\tPerplexity value= 105.63453332037824\n",
            "\n",
            "\tFor beta value= 0.7 on Training Set 4\n",
            "\tLikelihood value= -13197109.646978604\n",
            "\tPerplexity value= 105.70765600280811\n",
            "\n",
            "\tFor beta value= 0.7 on Training Set 5\n",
            "\tLikelihood value= -13197167.63692462\n",
            "\tPerplexity value= 105.70982088269335\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uP-dBDByQtzJ"
      },
      "source": [
        "## **QUESTION 2** \n",
        "\n",
        "### GLOVE MODEL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tri_QaB_fqcj"
      },
      "source": [
        "## Implementation of GloVe Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5xCGvQx-NBnQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bcd3481b-6791-4a22-96e5-5ff1f2ae8fa5"
      },
      "source": [
        "co_occurence_matrix = {}\n",
        "vocabulary = []\n",
        "local_context_window = 6\n",
        "print(\"Global variables initialized...!!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Global variables initialized...!!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QIA6JuuwgTp3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28b50174-7e1b-466e-fd15-3836516a20b8"
      },
      "source": [
        "import pickle\n",
        "import nltk \n",
        "corpus = open('/content/drive/My Drive/CS565_Assignment_2/All_Sentences_Corpus.txt')\n",
        "\n",
        "data = corpus.readlines()\n",
        "training_data = []\n",
        "\n",
        "vocabulary.clear()\n",
        "co_occurence_matrix.clear()\n",
        "\n",
        "no_of_sentences_so_far = 0\n",
        "\n",
        "for sentence in data:\n",
        "  tokens_list = nltk.tokenize.TreebankWordTokenizer().tokenize(sentence)\n",
        "  \n",
        "  no_of_sentences_so_far += 1\n",
        "  if no_of_sentences_so_far%10000 == 0 :\n",
        "    print(\"Co occurence matrix build in progress... \" + str(no_of_sentences_so_far) + \" sentences read.\")\n",
        "    if no_of_sentences_so_far == 800000:\n",
        "      break\n",
        "  \n",
        "  for index_tokens_list in range(len(tokens_list)):\n",
        "    token = tokens_list[index_tokens_list]\n",
        "    if token not in vocabulary:\n",
        "      vocabulary.append(token)  \n",
        "\n",
        "    for index in range(1, int(local_context_window/2)):\n",
        "      if index + index_tokens_list >= len(tokens_list):\n",
        "        break\n",
        "      tuple_main_context = (tokens_list[index_tokens_list], tokens_list[index_tokens_list + index])\n",
        "      tuple_context_main = (tokens_list[index_tokens_list + index], tokens_list[index_tokens_list])\n",
        "      if tuple_main_context not in co_occurence_matrix:\n",
        "        co_occurence_matrix[tuple_main_context] = 1\n",
        "        co_occurence_matrix[tuple_context_main] = 1\n",
        "      \n",
        "      else:\n",
        "        co_occurence_matrix[tuple_main_context] += 1\n",
        "        co_occurence_matrix[tuple_context_main] += 1\n",
        "\n",
        "print( \"Co occurence matrix built successfully..!!\")\n",
        "print( \"Number of words in vocabulary: \"  + str(len(vocabulary)))\n",
        "print( \"Number of entries in co-occurence matrix: \"  + str(len(co_occurence_matrix)))\n",
        "print( \"Saving the co occurence matrix and vocabulary for future reference.\")\n",
        "\n",
        "pickle.dump( vocabulary, open( \"/content/drive/My Drive/CS565_Assignment_2/vocabulary\", \"wb\" ) )\n",
        "pickle.dump( co_occurence_matrix, open( \"/content/drive/My Drive/CS565_Assignment_2/co_occurence_matrix\", \"wb\" ) )\n",
        "\n",
        "print(\"Executed Successfully\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Co occurence matrix build in progress... 10000 sentences read.\n",
            "Co occurence matrix build in progress... 20000 sentences read.\n",
            "Co occurence matrix build in progress... 30000 sentences read.\n",
            "Co occurence matrix build in progress... 40000 sentences read.\n",
            "Co occurence matrix build in progress... 50000 sentences read.\n",
            "Co occurence matrix build in progress... 60000 sentences read.\n",
            "Co occurence matrix build in progress... 70000 sentences read.\n",
            "Co occurence matrix build in progress... 80000 sentences read.\n",
            "Co occurence matrix build in progress... 90000 sentences read.\n",
            "Co occurence matrix build in progress... 100000 sentences read.\n",
            "Co occurence matrix build in progress... 110000 sentences read.\n",
            "Co occurence matrix build in progress... 120000 sentences read.\n",
            "Co occurence matrix build in progress... 130000 sentences read.\n",
            "Co occurence matrix build in progress... 140000 sentences read.\n",
            "Co occurence matrix build in progress... 150000 sentences read.\n",
            "Co occurence matrix build in progress... 160000 sentences read.\n",
            "Co occurence matrix build in progress... 170000 sentences read.\n",
            "Co occurence matrix build in progress... 180000 sentences read.\n",
            "Co occurence matrix build in progress... 190000 sentences read.\n",
            "Co occurence matrix build in progress... 200000 sentences read.\n",
            "Co occurence matrix build in progress... 210000 sentences read.\n",
            "Co occurence matrix build in progress... 220000 sentences read.\n",
            "Co occurence matrix build in progress... 230000 sentences read.\n",
            "Co occurence matrix build in progress... 240000 sentences read.\n",
            "Co occurence matrix build in progress... 250000 sentences read.\n",
            "Co occurence matrix build in progress... 260000 sentences read.\n",
            "Co occurence matrix build in progress... 270000 sentences read.\n",
            "Co occurence matrix build in progress... 280000 sentences read.\n",
            "Co occurence matrix build in progress... 290000 sentences read.\n",
            "Co occurence matrix build in progress... 300000 sentences read.\n",
            "Co occurence matrix build in progress... 310000 sentences read.\n",
            "Co occurence matrix build in progress... 320000 sentences read.\n",
            "Co occurence matrix build in progress... 330000 sentences read.\n",
            "Co occurence matrix build in progress... 340000 sentences read.\n",
            "Co occurence matrix build in progress... 350000 sentences read.\n",
            "Co occurence matrix build in progress... 360000 sentences read.\n",
            "Co occurence matrix build in progress... 370000 sentences read.\n",
            "Co occurence matrix build in progress... 380000 sentences read.\n",
            "Co occurence matrix build in progress... 390000 sentences read.\n",
            "Co occurence matrix build in progress... 400000 sentences read.\n",
            "Co occurence matrix build in progress... 410000 sentences read.\n",
            "Co occurence matrix build in progress... 420000 sentences read.\n",
            "Co occurence matrix build in progress... 430000 sentences read.\n",
            "Co occurence matrix build in progress... 440000 sentences read.\n",
            "Co occurence matrix build in progress... 450000 sentences read.\n",
            "Co occurence matrix build in progress... 460000 sentences read.\n",
            "Co occurence matrix build in progress... 470000 sentences read.\n",
            "Co occurence matrix build in progress... 480000 sentences read.\n",
            "Co occurence matrix build in progress... 490000 sentences read.\n",
            "Co occurence matrix build in progress... 500000 sentences read.\n",
            "Co occurence matrix build in progress... 510000 sentences read.\n",
            "Co occurence matrix build in progress... 520000 sentences read.\n",
            "Co occurence matrix build in progress... 530000 sentences read.\n",
            "Co occurence matrix build in progress... 540000 sentences read.\n",
            "Co occurence matrix build in progress... 550000 sentences read.\n",
            "Co occurence matrix build in progress... 560000 sentences read.\n",
            "Co occurence matrix build in progress... 570000 sentences read.\n",
            "Co occurence matrix build in progress... 580000 sentences read.\n",
            "Co occurence matrix build in progress... 590000 sentences read.\n",
            "Co occurence matrix build in progress... 600000 sentences read.\n",
            "Co occurence matrix build in progress... 610000 sentences read.\n",
            "Co occurence matrix build in progress... 620000 sentences read.\n",
            "Co occurence matrix build in progress... 630000 sentences read.\n",
            "Co occurence matrix build in progress... 640000 sentences read.\n",
            "Co occurence matrix build in progress... 650000 sentences read.\n",
            "Co occurence matrix build in progress... 660000 sentences read.\n",
            "Co occurence matrix build in progress... 670000 sentences read.\n",
            "Co occurence matrix build in progress... 680000 sentences read.\n",
            "Co occurence matrix build in progress... 690000 sentences read.\n",
            "Co occurence matrix build in progress... 700000 sentences read.\n",
            "Co occurence matrix build in progress... 710000 sentences read.\n",
            "Co occurence matrix build in progress... 720000 sentences read.\n",
            "Co occurence matrix build in progress... 730000 sentences read.\n",
            "Co occurence matrix build in progress... 740000 sentences read.\n",
            "Co occurence matrix build in progress... 750000 sentences read.\n",
            "Co occurence matrix build in progress... 760000 sentences read.\n",
            "Co occurence matrix build in progress... 770000 sentences read.\n",
            "Co occurence matrix build in progress... 780000 sentences read.\n",
            "Co occurence matrix build in progress... 790000 sentences read.\n",
            "Co occurence matrix build in progress... 800000 sentences read.\n",
            "Co occurence matrix built successfully..!!\n",
            "Number of words in vocabulary: 381357\n",
            "Number of entries in co-occurence matrix: 14453378\n",
            "Saving the co occurence matrix and vocabulary for future reference.\n",
            "Executed Successfully\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9oIve_xMcWbK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d70d3f4e-2ec4-4a8a-c8ea-662ab8c473ed"
      },
      "source": [
        "import random\n",
        "import numpy as np\n",
        "vectors_main_word = {}\n",
        "vectors_context_word = {}\n",
        "biases_main_word = {}\n",
        "biases_context_word = {}\n",
        "alpha_glove_model = 0.75\n",
        "x_max_glove_model = 100\n",
        "\n",
        "number_of_iterations = 100\n",
        "learning_rate = 0.001\n",
        "\n",
        "def find_weight( main_token, context_token ):\n",
        "  if (context_token,main_token) not in co_occurence_matrix:\n",
        "    return 0\n",
        "  if ( co_occurence_matrix[(context_token,main_token)] < x_max_glove_model ):\n",
        "    return (co_occurence_matrix[(context_token,main_token)] / x_max_glove_model) ** alpha_glove_model\n",
        "  return 1\n",
        "\n",
        "def initilize_word_vectors_and_biases():\n",
        "  for token in vocabulary:\n",
        "    vectors_main_word[token] = np.random.random(100)\n",
        "    vectors_context_word[token] = np.random.random(100)\n",
        "    biases_main_word[token] = random.random()\n",
        "    biases_context_word[token] = random.random()\n",
        "\n",
        "initilize_word_vectors_and_biases()\n",
        "\n",
        "print(\"Initialization of the word vectors and biases for the \" + str(len(vocabulary)) + \" tokens in our vocabulary complete.\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initialization of the word vectors and biases for the 381357 tokens in our vocabulary complete.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LU-CItiCtzbT"
      },
      "source": [
        "def find_weight( main_token, context_token ):\n",
        "  if (context_token,main_token) not in co_occurence_matrix:\n",
        "    return 0\n",
        "  if ( co_occurence_matrix[(context_token,main_token)] < x_max_glove_model ):\n",
        "    return (co_occurence_matrix[(context_token,main_token)] / x_max_glove_model) ** alpha_glove_model\n",
        "  return 1\n",
        "alpha_glove_model = 0.75\n",
        "x_max_glove_model = 100\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2UTC2Kn3M3rh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2202fb1d-1849-4bb5-8508-068e77d7e2d4"
      },
      "source": [
        "import numpy as np\n",
        "import math\n",
        "def run_single_iteration():\n",
        "  total_cost = 0\n",
        "  for (context_token,main_token), value in co_occurence_matrix.items():\n",
        "    if main_token == context_token:\n",
        "      continue\n",
        "    weight = find_weight( main_token, context_token )\n",
        "    \n",
        "    if(weight == 0):\n",
        "      continue\n",
        "    cost_without_weight = ( np.dot(vectors_main_word[main_token] , vectors_context_word[context_token] ) + biases_main_word[main_token] + biases_context_word[context_token] - math.log(co_occurence_matrix[(context_token,main_token)]))\n",
        "    total_cost += 0.5 * weight * cost_without_weight ** 2\n",
        "    gradient_main_word_vector = weight * cost_without_weight * vectors_context_word[context_token]\n",
        "    gradient_context_word_vector = weight * cost_without_weight * vectors_main_word[main_token]\n",
        "    gradient_main_bias = weight * cost_without_weight\n",
        "    gradient_context_bias = weight * cost_without_weight\n",
        "\n",
        "    vectors_main_word[ main_token ] -= learning_rate * gradient_main_word_vector\n",
        "    vectors_context_word[ context_token ] -= learning_rate * gradient_context_word_vector\n",
        "\n",
        "    biases_main_word[ main_token ] -= learning_rate * gradient_main_bias\n",
        "    biases_context_word[ context_token ] -= learning_rate * gradient_context_bias\n",
        "  return total_cost\n",
        "\n",
        "print(\"Function to run single iteration of gradient descent compiled successfully..!!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Function to run single iteration of gradient descent compiled successfully..!!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0MK79OFGM6-x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36f87982-c71c-44c0-bebf-205097396647"
      },
      "source": [
        "import pickle\n",
        "\n",
        "print(\"Applying gradient descent to find the appropriate word vectors to carry out unsupervised learning...\")\n",
        "for iteration in range(1,51):\n",
        "  cost = run_single_iteration()\n",
        "  print(\"Iteration \" + str(iteration) + \" successfull. Returned cost value is: \" + str(cost))\n",
        "\n",
        "print(\"All iterations fot gradient descent completed successfully..!!\")\n",
        "print(\"Saving word vectors in file 'word_vectors' \")\n",
        "pickle.dump(vectors_main_word , open( \"/content/drive/My Drive/CS565_Assignment_2/word_vectors_test_set\", \"wb\" ) )\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Applying gradient descent to find the appropriate word vectors to carry out unsupervised learning...\n",
            "Iteration 1 successfull. Returned cost value is: 47236438.94114367\n",
            "Iteration 2 successfull. Returned cost value is: 23291786.181409672\n",
            "Iteration 3 successfull. Returned cost value is: 17292607.498076387\n",
            "Iteration 4 successfull. Returned cost value is: 14233396.73385962\n",
            "Iteration 5 successfull. Returned cost value is: 12331238.296886241\n",
            "Iteration 6 successfull. Returned cost value is: 11000632.322678067\n",
            "Iteration 7 successfull. Returned cost value is: 10002445.075140944\n",
            "Iteration 8 successfull. Returned cost value is: 9218120.966641696\n",
            "Iteration 9 successfull. Returned cost value is: 8580982.379707377\n",
            "Iteration 10 successfull. Returned cost value is: 8050219.792142304\n",
            "Iteration 11 successfull. Returned cost value is: 7599267.488398596\n",
            "Iteration 12 successfull. Returned cost value is: 7209991.329512785\n",
            "Iteration 13 successfull. Returned cost value is: 6869535.365869933\n",
            "Iteration 14 successfull. Returned cost value is: 6568499.401572082\n",
            "Iteration 15 successfull. Returned cost value is: 6299830.795208618\n",
            "Iteration 16 successfull. Returned cost value is: 6058121.668535\n",
            "Iteration 17 successfull. Returned cost value is: 5839147.173512788\n",
            "Iteration 18 successfull. Returned cost value is: 5639552.828337308\n",
            "Iteration 19 successfull. Returned cost value is: 5456637.191263945\n",
            "Iteration 20 successfull. Returned cost value is: 5288197.316847923\n",
            "Iteration 21 successfull. Returned cost value is: 5132416.62996144\n",
            "Iteration 22 successfull. Returned cost value is: 4987782.115513487\n",
            "Iteration 23 successfull. Returned cost value is: 4853022.181241215\n",
            "Iteration 24 successfull. Returned cost value is: 4727059.364030121\n",
            "Iteration 25 successfull. Returned cost value is: 4608973.868078551\n",
            "Iteration 26 successfull. Returned cost value is: 4497975.1239776155\n",
            "Iteration 27 successfull. Returned cost value is: 4393379.366679961\n",
            "Iteration 28 successfull. Returned cost value is: 4294591.785090749\n",
            "Iteration 29 successfull. Returned cost value is: 4201092.182810122\n",
            "Iteration 30 successfull. Returned cost value is: 4112423.3632662334\n",
            "Iteration 31 successfull. Returned cost value is: 4028181.6488039447\n",
            "Iteration 32 successfull. Returned cost value is: 3948009.086018116\n",
            "Iteration 33 successfull. Returned cost value is: 3871586.9943929617\n",
            "Iteration 34 successfull. Returned cost value is: 3798630.59330594\n",
            "Iteration 35 successfull. Returned cost value is: 3728884.500821852\n",
            "Iteration 36 successfull. Returned cost value is: 3662118.9420756632\n",
            "Iteration 37 successfull. Returned cost value is: 3598126.5388079183\n",
            "Iteration 38 successfull. Returned cost value is: 3536719.577716\n",
            "Iteration 39 successfull. Returned cost value is: 3477727.6755253067\n",
            "Iteration 40 successfull. Returned cost value is: 3420995.774471989\n",
            "Iteration 41 successfull. Returned cost value is: 3366382.4143640604\n",
            "Iteration 42 successfull. Returned cost value is: 3313758.2372674625\n",
            "Iteration 43 successfull. Returned cost value is: 3263004.688696111\n",
            "Iteration 44 successfull. Returned cost value is: 3214012.885529247\n",
            "Iteration 45 successfull. Returned cost value is: 3166682.6259380714\n",
            "Iteration 46 successfull. Returned cost value is: 3120921.520741267\n",
            "Iteration 47 successfull. Returned cost value is: 3076644.228974108\n",
            "Iteration 48 successfull. Returned cost value is: 3033771.7831786145\n",
            "Iteration 49 successfull. Returned cost value is: 2992230.992220158\n",
            "Iteration 50 successfull. Returned cost value is: 2951953.9112899457\n",
            "All iterations fot gradient descent completed successfully..!!\n",
            "Saving word vectors in file 'word_vectors' \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USYz6cQ3zIOI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a99cf639-6c14-4735-f7ca-54f676532fbd"
      },
      "source": [
        "import pickle\n",
        "pickle.dump(vectors_main_word , open( \"/content/drive/My Drive/CS565_Assignment_2/word_vectors_test_set_50_iterations\", \"wb\" ) )\n",
        "pickle.dump(vectors_context_word , open( \"/content/drive/My Drive/CS565_Assignment_2/vectors_context_word_50_iterations\", \"wb\" ) )\n",
        "pickle.dump(biases_main_word , open( \"/content/drive/My Drive/CS565_Assignment_2/biases_main_word_50_iterations\", \"wb\" ) )\n",
        "pickle.dump(biases_context_word , open( \"/content/drive/My Drive/CS565_Assignment_2/biases_context_word_50_iterations\", \"wb\" ) )\n",
        "\n",
        "print(\"Applying gradient descent to find the appropriate word vectors to carry out unsupervised learning...\")\n",
        "learning_rate *= 2\n",
        "for iteration in range(1,51):\n",
        "  cost = run_single_iteration()\n",
        "  print(\"Iteration \" + str(50+iteration) + \" successfull. Returned cost value is: \" + str(cost))\n",
        "\n",
        "print(\"All iterations fot gradient descent completed successfully..!!\")\n",
        "print(\"Saving word vectors in file 'word_vectors' \")\n",
        "\n",
        "pickle.dump(vectors_main_word , open( \"/content/drive/My Drive/CS565_Assignment_2/word_vectors_test_set_100_iterations\", \"wb\" ) )\n",
        "pickle.dump(vectors_context_word , open( \"/content/drive/My Drive/CS565_Assignment_2/vectors_context_word_100_iterations\", \"wb\" ) )\n",
        "pickle.dump(biases_main_word , open( \"/content/drive/My Drive/CS565_Assignment_2/biases_main_word_100_iterations\", \"wb\" ) )\n",
        "pickle.dump(biases_context_word , open( \"/content/drive/My Drive/CS565_Assignment_2/biases_context_word_100_iterations\", \"wb\" ) )\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Applying gradient descent to find the appropriate word vectors to carry out unsupervised learning...\n",
            "Iteration 51 successfull. Returned cost value is: 2889383.6423412026\n",
            "Iteration 52 successfull. Returned cost value is: 2819384.3132969956\n",
            "Iteration 53 successfull. Returned cost value is: 2749713.7329787402\n",
            "Iteration 54 successfull. Returned cost value is: 2683523.7766539278\n",
            "Iteration 55 successfull. Returned cost value is: 2620679.430104931\n",
            "Iteration 56 successfull. Returned cost value is: 2560933.332520644\n",
            "Iteration 57 successfull. Returned cost value is: 2504046.371436415\n",
            "Iteration 58 successfull. Returned cost value is: 2449800.7001358224\n",
            "Iteration 59 successfull. Returned cost value is: 2398000.0168762063\n",
            "Iteration 60 successfull. Returned cost value is: 2348467.6325763073\n",
            "Iteration 61 successfull. Returned cost value is: 2301044.2574268687\n",
            "Iteration 62 successfull. Returned cost value is: 2255585.936904996\n",
            "Iteration 63 successfull. Returned cost value is: 2211962.2262574327\n",
            "Iteration 64 successfull. Returned cost value is: 2170054.603180997\n",
            "Iteration 65 successfull. Returned cost value is: 2129755.094780703\n",
            "Iteration 66 successfull. Returned cost value is: 2090965.0904732214\n",
            "Iteration 67 successfull. Returned cost value is: 2053594.3139816346\n",
            "Iteration 68 successfull. Returned cost value is: 2017559.9307262886\n",
            "Iteration 69 successfull. Returned cost value is: 1982785.7703296216\n",
            "Iteration 70 successfull. Returned cost value is: 1949201.6471034335\n",
            "Iteration 71 successfull. Returned cost value is: 1916742.7641624329\n",
            "Iteration 72 successfull. Returned cost value is: 1885349.1891327342\n",
            "Iteration 73 successfull. Returned cost value is: 1854965.39140208\n",
            "Iteration 74 successfull. Returned cost value is: 1825539.8324323397\n",
            "Iteration 75 successfull. Returned cost value is: 1797024.6020362128\n",
            "Iteration 76 successfull. Returned cost value is: 1769375.0945699904\n",
            "Iteration 77 successfull. Returned cost value is: 1742549.719938695\n",
            "Iteration 78 successfull. Returned cost value is: 1716509.645036135\n",
            "Iteration 79 successfull. Returned cost value is: 1691218.5618901807\n",
            "Iteration 80 successfull. Returned cost value is: 1666642.4792907748\n",
            "Iteration 81 successfull. Returned cost value is: 1642749.5351380587\n",
            "Iteration 82 successfull. Returned cost value is: 1619509.8271040265\n",
            "Iteration 83 successfull. Returned cost value is: 1596895.2595320537\n",
            "Iteration 84 successfull. Returned cost value is: 1574879.404760986\n",
            "Iteration 85 successfull. Returned cost value is: 1553437.3772882668\n",
            "Iteration 86 successfull. Returned cost value is: 1532545.7193930529\n",
            "Iteration 87 successfull. Returned cost value is: 1512182.2970008035\n",
            "Iteration 88 successfull. Returned cost value is: 1492326.2047246806\n",
            "Iteration 89 successfull. Returned cost value is: 1472957.6791350713\n",
            "Iteration 90 successfull. Returned cost value is: 1454058.0194357669\n",
            "Iteration 91 successfull. Returned cost value is: 1435609.5148017367\n",
            "Iteration 92 successfull. Returned cost value is: 1417595.3777288324\n",
            "Iteration 93 successfull. Returned cost value is: 1399999.682816493\n",
            "Iteration 94 successfull. Returned cost value is: 1382807.3104623829\n",
            "Iteration 95 successfull. Returned cost value is: 1366003.8950147543\n",
            "Iteration 96 successfull. Returned cost value is: 1349575.776963293\n",
            "Iteration 97 successfull. Returned cost value is: 1333509.9588049965\n",
            "Iteration 98 successfull. Returned cost value is: 1317794.064255794\n",
            "Iteration 99 successfull. Returned cost value is: 1302416.300505596\n",
            "Iteration 100 successfull. Returned cost value is: 1287365.4232571456\n",
            "All iterations fot gradient descent completed successfully..!!\n",
            "Saving word vectors in file 'word_vectors' \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CCGgZIXVX7S3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2357a2dd-a6d9-4e50-d3c8-ca1dbadf91b5"
      },
      "source": [
        "import pickle\n",
        "print(\"Applying gradient descent to find the appropriate word vectors to carry out unsupervised learning...\")\n",
        "learning_rate *= 5\n",
        "prev_cost = cost\n",
        "for iteration in range(1,51):\n",
        "  cost = run_single_iteration()\n",
        "  if cost > prev_cost:\n",
        "    learning_rate /= 2\n",
        "  prev_cost = cost\n",
        "  \n",
        "  print(\"Iteration \" + str(100+iteration) + \" successfull. Returned cost value is: \" + str(cost))\n",
        "\n",
        "print(\"All iterations fot gradient descent completed successfully..!!\")\n",
        "print(\"Saving word vectors in file 'word_vectors' \")\n",
        "\n",
        "pickle.dump(vectors_main_word , open( \"/content/drive/My Drive/CS565_Assignment_2/word_vectors_test_set_150_iterations\", \"wb\" ) )\n",
        "pickle.dump(vectors_context_word , open( \"/content/drive/My Drive/CS565_Assignment_2/vectors_context_word_150_iterations\", \"wb\" ) )\n",
        "pickle.dump(biases_main_word , open( \"/content/drive/My Drive/CS565_Assignment_2/biases_main_word_150_iterations\", \"wb\" ) )\n",
        "pickle.dump(biases_context_word , open( \"/content/drive/My Drive/CS565_Assignment_2/biases_context_word_150_iterations\", \"wb\" ) )\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Applying gradient descent to find the appropriate word vectors to carry out unsupervised learning...\n",
            "Iteration 101 successfull. Returned cost value is: 1230193.6284105869\n",
            "Iteration 102 successfull. Returned cost value is: 1171680.274915724\n",
            "Iteration 103 successfull. Returned cost value is: 1110271.2486370225\n",
            "Iteration 104 successfull. Returned cost value is: 1054196.3674812198\n",
            "Iteration 105 successfull. Returned cost value is: 1002933.4583974539\n",
            "Iteration 106 successfull. Returned cost value is: 955910.5535293766\n",
            "Iteration 107 successfull. Returned cost value is: 912629.2250976734\n",
            "Iteration 108 successfull. Returned cost value is: 872665.0651814741\n",
            "Iteration 109 successfull. Returned cost value is: 835656.4592036252\n",
            "Iteration 110 successfull. Returned cost value is: 801293.6993631809\n",
            "Iteration 111 successfull. Returned cost value is: 769309.9349434131\n",
            "Iteration 112 successfull. Returned cost value is: 739473.8718146567\n",
            "Iteration 113 successfull. Returned cost value is: 711583.9045986556\n",
            "Iteration 114 successfull. Returned cost value is: 685463.3821046412\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ypjs3qV9szkr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ef60cb8-26fc-45d9-b0c7-bf40f4a91af4"
      },
      "source": [
        "learning_rate = 0.01\n",
        "import pickle\n",
        "\n",
        "vectors_main_word = pickle.load( open( \"/content/drive/My Drive/CS565_Assignment_2/word_vectors_test_set_100_iterations\", \"rb\" ) )\n",
        "vectors_context_word = pickle.load( open( \"/content/drive/My Drive/CS565_Assignment_2/vectors_context_word_100_iterations\", \"rb\" ) )\n",
        "biases_main_word = pickle.load( open( \"/content/drive/My Drive/CS565_Assignment_2/biases_main_word_100_iterations\", \"rb\" ) )\n",
        "biases_context_word = pickle.load( open( \"/content/drive/My Drive/CS565_Assignment_2/biases_context_word_100_iterations\", \"rb\" ) )\n",
        "\n",
        "vocabulary = pickle.load( open( \"/content/drive/My Drive/CS565_Assignment_2/vocabulary\", \"rb\" ) )\n",
        "co_occurence_matrix = pickle.load( open( \"/content/drive/My Drive/CS565_Assignment_2/co_occurence_matrix\", \"rb\" ) )\n",
        "\n",
        "print(len(vocabulary))\n",
        "print(len(co_occurence_matrix))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "381357\n",
            "14453378\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-SoeKmttarm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40d28138-4cf4-46c4-be80-396ebde2b71b"
      },
      "source": [
        "import pickle\n",
        "print(\"Applying gradient descent to find the appropriate word vectors to carry out unsupervised learning...\")\n",
        "learning_rate *= 2\n",
        "prev_cost = 1287365.4232571456\n",
        "for iteration in range(1,51):\n",
        "  cost = run_single_iteration()\n",
        "  if cost > prev_cost:\n",
        "    learning_rate /= 2\n",
        "  prev_cost = cost\n",
        "  \n",
        "  print(\"Iteration \" + str(100+iteration) + \" successfull. Returned cost value is: \" + str(cost))\n",
        "\n",
        "print(\"All iterations fot gradient descent completed successfully..!!\")\n",
        "print(\"Saving word vectors in file 'word_vectors' \")\n",
        "\n",
        "pickle.dump(vectors_main_word , open( \"/content/drive/My Drive/CS565_Assignment_2/word_vectors_test_set_150_iterations\", \"wb\" ) )\n",
        "pickle.dump(vectors_context_word , open( \"/content/drive/My Drive/CS565_Assignment_2/vectors_context_word_150_iterations\", \"wb\" ) )\n",
        "pickle.dump(biases_main_word , open( \"/content/drive/My Drive/CS565_Assignment_2/biases_main_word_150_iterations\", \"wb\" ) )\n",
        "pickle.dump(biases_context_word , open( \"/content/drive/My Drive/CS565_Assignment_2/biases_context_word_150_iterations\", \"wb\" ) )\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Applying gradient descent to find the appropriate word vectors to carry out unsupervised learning...\n",
            "Iteration 101 successfull. Returned cost value is: 1125629.200899655\n",
            "Iteration 102 successfull. Returned cost value is: 769978.333987564\n",
            "Iteration 103 successfull. Returned cost value is: 568920.8187686394\n",
            "Iteration 104 successfull. Returned cost value is: 444854.55604212463\n",
            "Iteration 105 successfull. Returned cost value is: 362599.8320047111\n",
            "Iteration 106 successfull. Returned cost value is: 305327.2196648724\n",
            "Iteration 107 successfull. Returned cost value is: 264010.8866355854\n",
            "Iteration 108 successfull. Returned cost value is: 233466.13659950398\n",
            "Iteration 109 successfull. Returned cost value is: 210661.36562732185\n",
            "Iteration 110 successfull. Returned cost value is: 192518.29590635648\n",
            "Iteration 111 successfull. Returned cost value is: 178314.0182903064\n",
            "Iteration 112 successfull. Returned cost value is: 166214.24014215462\n",
            "Iteration 113 successfull. Returned cost value is: 156459.75396556628\n",
            "Iteration 114 successfull. Returned cost value is: 148456.32855175115\n",
            "Iteration 115 successfull. Returned cost value is: 141979.13462394808\n",
            "Iteration 116 successfull. Returned cost value is: 136343.05439249956\n",
            "Iteration 117 successfull. Returned cost value is: 131645.477381706\n",
            "Iteration 118 successfull. Returned cost value is: 127476.80857849632\n",
            "Iteration 119 successfull. Returned cost value is: 123874.40624314023\n",
            "Iteration 120 successfull. Returned cost value is: 120630.26845071731\n",
            "Iteration 121 successfull. Returned cost value is: 117984.70205729347\n",
            "Iteration 122 successfull. Returned cost value is: 115142.2037026981\n",
            "Iteration 123 successfull. Returned cost value is: 112753.17635731191\n",
            "Iteration 124 successfull. Returned cost value is: 110384.69789443807\n",
            "Iteration 125 successfull. Returned cost value is: 108328.29834856348\n",
            "Iteration 126 successfull. Returned cost value is: 106467.36057094831\n",
            "Iteration 127 successfull. Returned cost value is: 104681.80073502539\n",
            "Iteration 128 successfull. Returned cost value is: 103243.16793567434\n",
            "Iteration 129 successfull. Returned cost value is: 101812.13725318847\n",
            "Iteration 130 successfull. Returned cost value is: 100538.47256359784\n",
            "Iteration 131 successfull. Returned cost value is: 99202.39518119927\n",
            "Iteration 132 successfull. Returned cost value is: 97763.62921431016\n",
            "Iteration 133 successfull. Returned cost value is: 96357.95774713135\n",
            "Iteration 134 successfull. Returned cost value is: 95077.79748313762\n",
            "Iteration 135 successfull. Returned cost value is: 93865.67325985787\n",
            "Iteration 136 successfull. Returned cost value is: 92919.08233435171\n",
            "Iteration 137 successfull. Returned cost value is: 91983.89569505087\n",
            "Iteration 138 successfull. Returned cost value is: 91023.46858869186\n",
            "Iteration 139 successfull. Returned cost value is: 90194.871433589\n",
            "Iteration 140 successfull. Returned cost value is: 89177.16762187262\n",
            "Iteration 141 successfull. Returned cost value is: 88130.65526753796\n",
            "Iteration 142 successfull. Returned cost value is: 87301.29809912037\n",
            "Iteration 143 successfull. Returned cost value is: 86503.41802236288\n",
            "Iteration 144 successfull. Returned cost value is: 85916.07214119809\n",
            "Iteration 145 successfull. Returned cost value is: 85270.88187719077\n",
            "Iteration 146 successfull. Returned cost value is: 84517.70151257007\n",
            "Iteration 147 successfull. Returned cost value is: 83702.67319066187\n",
            "Iteration 148 successfull. Returned cost value is: 82768.31308544846\n",
            "Iteration 149 successfull. Returned cost value is: 82001.74470892751\n",
            "Iteration 150 successfull. Returned cost value is: 81282.66526245071\n",
            "All iterations fot gradient descent completed successfully..!!\n",
            "Saving word vectors in file 'word_vectors' \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h9_QX5bf9HuU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b61a882-9e20-49dd-eab9-ecce70f72767"
      },
      "source": [
        "import pickle\n",
        "print(\"Applying gradient descent to find the appropriate word vectors to carry out unsupervised learning...\")\n",
        "\n",
        "for iteration in range(1,51):\n",
        "  cost = run_single_iteration()\n",
        "  print(\"Iteration \" + str(150+iteration) + \" successfull. Returned cost value is: \" + str(cost))\n",
        "\n",
        "print(\"All iterations fot gradient descent completed successfully..!!\")\n",
        "print(\"Saving word vectors in file 'word_vectors' \")\n",
        "\n",
        "pickle.dump(vectors_main_word , open( \"/content/drive/My Drive/CS565_Assignment_2/word_vectors_test_set_200_iterations\", \"wb\" ) )\n",
        "pickle.dump(vectors_context_word , open( \"/content/drive/My Drive/CS565_Assignment_2/vectors_context_word_200_iterations\", \"wb\" ) )\n",
        "pickle.dump(biases_main_word , open( \"/content/drive/My Drive/CS565_Assignment_2/biases_main_word_200_iterations\", \"wb\" ) )\n",
        "pickle.dump(biases_context_word , open( \"/content/drive/My Drive/CS565_Assignment_2/biases_context_word_200_iterations\", \"wb\" ) )\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Applying gradient descent to find the appropriate word vectors to carry out unsupervised learning...\n",
            "Iteration 151 successfull. Returned cost value is: 101914.1013125953\n",
            "Iteration 152 successfull. Returned cost value is: 92280.85425468224\n",
            "Iteration 153 successfull. Returned cost value is: 90825.29090268919\n",
            "Iteration 154 successfull. Returned cost value is: 89948.29266778872\n",
            "Iteration 155 successfull. Returned cost value is: 89277.07384295041\n",
            "Iteration 156 successfull. Returned cost value is: 88712.03432644559\n",
            "Iteration 157 successfull. Returned cost value is: 88212.97321447311\n",
            "Iteration 158 successfull. Returned cost value is: 87759.45632970665\n",
            "Iteration 159 successfull. Returned cost value is: 87339.55004155138\n",
            "Iteration 160 successfull. Returned cost value is: 86945.62898172835\n",
            "Iteration 161 successfull. Returned cost value is: 86572.5052955621\n",
            "Iteration 162 successfull. Returned cost value is: 86216.48152128719\n",
            "Iteration 163 successfull. Returned cost value is: 85874.82552255287\n",
            "Iteration 164 successfull. Returned cost value is: 85545.45895907018\n",
            "Iteration 165 successfull. Returned cost value is: 85226.76245236391\n",
            "Iteration 166 successfull. Returned cost value is: 84917.448476236\n",
            "Iteration 167 successfull. Returned cost value is: 84616.4754948964\n",
            "Iteration 168 successfull. Returned cost value is: 84322.98824290806\n",
            "Iteration 169 successfull. Returned cost value is: 84036.27513411657\n",
            "Iteration 170 successfull. Returned cost value is: 83755.7372209119\n",
            "Iteration 171 successfull. Returned cost value is: 83480.86513525322\n",
            "Iteration 172 successfull. Returned cost value is: 83211.2216680006\n",
            "Iteration 173 successfull. Returned cost value is: 82946.42840754041\n",
            "Iteration 174 successfull. Returned cost value is: 82686.15535205787\n",
            "Iteration 175 successfull. Returned cost value is: 82430.11273365818\n",
            "Iteration 176 successfull. Returned cost value is: 82178.04451136227\n",
            "Iteration 177 successfull. Returned cost value is: 81929.7231391126\n",
            "Iteration 178 successfull. Returned cost value is: 81684.94531953681\n",
            "Iteration 179 successfull. Returned cost value is: 81443.52852743535\n",
            "Iteration 180 successfull. Returned cost value is: 81205.3081418017\n",
            "Iteration 181 successfull. Returned cost value is: 80970.1350608396\n",
            "Iteration 182 successfull. Returned cost value is: 80737.8737065994\n",
            "Iteration 183 successfull. Returned cost value is: 80508.40034370626\n",
            "Iteration 184 successfull. Returned cost value is: 80281.6016550511\n",
            "Iteration 185 successfull. Returned cost value is: 80057.37352798607\n",
            "Iteration 186 successfull. Returned cost value is: 79835.62001508407\n",
            "Iteration 187 successfull. Returned cost value is: 79616.25243954196\n",
            "Iteration 188 successfull. Returned cost value is: 79399.18862175448\n",
            "Iteration 189 successfull. Returned cost value is: 79184.35220789147\n",
            "Iteration 190 successfull. Returned cost value is: 78971.67208455587\n",
            "Iteration 191 successfull. Returned cost value is: 78761.08186650804\n",
            "Iteration 192 successfull. Returned cost value is: 78552.51944693842\n",
            "Iteration 193 successfull. Returned cost value is: 78345.92660087161\n",
            "Iteration 194 successfull. Returned cost value is: 78141.24863488533\n",
            "Iteration 195 successfull. Returned cost value is: 77938.43407615679\n",
            "Iteration 196 successfull. Returned cost value is: 77737.43439607439\n",
            "Iteration 197 successfull. Returned cost value is: 77538.20376369952\n",
            "Iteration 198 successfull. Returned cost value is: 77340.69882518663\n",
            "Iteration 199 successfull. Returned cost value is: 77144.87850610151\n",
            "Iteration 200 successfull. Returned cost value is: 76950.70383365008\n",
            "All iterations fot gradient descent completed successfully..!!\n",
            "Saving word vectors in file 'word_vectors' \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xNKMVeyJAejg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aebac54e-31cb-4b97-8ae2-958731bf797a"
      },
      "source": [
        "! rm -rf web\n",
        "! cp -r '/content/drive/My Drive/CS565_Assignment_2/web/' .\n",
        "# ! cp -r '/content/drive/My Drive/web/datasets/similarity.py' .\n",
        "\n",
        "import logging\n",
        "from six import iteritems\n",
        "from web.datasets.similarity import fetch_MEN, fetch_WS353, fetch_SimLex999\n",
        "from web.embeddings import fetch_GloVe\n",
        "from web.evaluate import evaluate_similarity"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.datasets.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.datasets. Anything that cannot be imported from sklearn.datasets is now part of the private API.\n",
            "  warnings.warn(message, FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rOfWc1bBBN4w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8741092-4615-4eae-9d8c-4da6605d37e3"
      },
      "source": [
        "tasks = {\n",
        "    \"MEN\": fetch_MEN(),\n",
        "    \"WS353\": fetch_WS353(),\n",
        "    \"SIMLEX999\": fetch_SimLex999()\n",
        "}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Dataset created in /root/web_data/similarity\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5MhoLLdbBVkZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd66ce64-2832-4e86-aa61-0e31630f9586"
      },
      "source": [
        "for name, data in iteritems(tasks):\n",
        "    print(\"Sample data from {}: pair \\\"{}\\\" and \\\"{}\\\" is assigned score {}\".format(name, data.X[0][0], data.X[0][1], data.y[0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sample data from MEN: pair \"sun\" and \"sunlight\" is assigned score [10.]\n",
            "Sample data from WS353: pair \"love\" and \"sex\" is assigned score 6.77\n",
            "Sample data from SIMLEX999: pair \"old\" and \"new\" is assigned score 1.58\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TkCOoj-jBYva",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b6e854c-ce76-4ea4-e682-da47f68c142d"
      },
      "source": [
        "for name, data in iteritems(tasks):\n",
        "    print (\"Spearman correlation of scores on {} {}\".format(name, evaluate_similarity(vectors_main_word, data.X, data.y)))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Missing 30 words. Will replace them with mean vector\n",
            "/content/web/evaluate.py:336: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
            "  A = np.vstack(w.get(word, mean_vector) for word in X[:, 0])\n",
            "/content/web/evaluate.py:337: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
            "  B = np.vstack(w.get(word, mean_vector) for word in X[:, 1])\n",
            "Spearman correlation of scores on MEN 0.43560109095570295\n",
            "Spearman correlation of scores on WS353 0.31462905671095203\n",
            "Missing 1 words. Will replace them with mean vector\n",
            "Spearman correlation of scores on SIMLEX999 0.10680710798945227\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aL3kdjWgW8wP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9bf68974-0ca3-4783-84d3-f17066100b18"
      },
      "source": [
        "w_glove = fetch_GloVe(corpus=\"wiki-6B\", dim=300)\n",
        "for name, data in iteritems(tasks):\n",
        "    print (\"Spearman correlation of scores on {} {}\".format(name, evaluate_similarity(w_glove, data.X, data.y)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from http://nlp.stanford.edu/data/glove.6B.zip ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0.00/862M [00:00<?, ?b/s]\u001b[A\n",
            "  0%|          | 16.4k/862M [00:00<1:57:02, 123kb/s]\u001b[A\n",
            "  0%|          | 49.2k/862M [00:00<1:39:55, 144kb/s]\u001b[A\n",
            "  0%|          | 106k/862M [00:00<1:20:12, 179kb/s] \u001b[A\n",
            "  0%|          | 221k/862M [00:00<1:01:19, 234kb/s]\u001b[A\n",
            "  0%|          | 426k/862M [00:00<45:48, 314kb/s]  \u001b[A\n",
            "  0%|          | 680k/862M [00:00<34:22, 418kb/s]\u001b[A\n",
            "  0%|          | 1.38M/862M [00:00<24:53, 576kb/s]\u001b[A\n",
            "  0%|          | 2.76M/862M [00:01<17:49, 804kb/s]\u001b[A\n",
            "  1%|          | 5.53M/862M [00:01<12:38, 1.13Mb/s]\u001b[A\n",
            "  1%|          | 8.61M/862M [00:01<09:01, 1.58Mb/s]\u001b[A\n",
            "  1%|         | 11.6M/862M [00:01<06:29, 2.18Mb/s]\u001b[A\n",
            "  2%|         | 14.5M/862M [00:01<04:44, 2.98Mb/s]\u001b[A\n",
            "  2%|         | 17.6M/862M [00:01<03:29, 4.03Mb/s]\u001b[A\n",
            "  2%|         | 20.6M/862M [00:01<02:38, 5.32Mb/s]\u001b[A\n",
            "  3%|         | 23.5M/862M [00:02<02:02, 6.83Mb/s]\u001b[A\n",
            "  3%|         | 26.6M/862M [00:02<01:37, 8.59Mb/s]\u001b[A\n",
            "  3%|         | 29.7M/862M [00:02<01:19, 10.5Mb/s]\u001b[A\n",
            "  4%|         | 32.5M/862M [00:02<01:07, 12.3Mb/s]\u001b[A\n",
            "  4%|         | 35.7M/862M [00:02<00:57, 14.3Mb/s]\u001b[A\n",
            "  4%|         | 38.8M/862M [00:02<00:51, 16.1Mb/s]\u001b[A\n",
            "  5%|         | 41.9M/862M [00:02<00:46, 17.7Mb/s]\u001b[A\n",
            "  5%|         | 45.1M/862M [00:03<00:43, 18.9Mb/s]\u001b[A\n",
            "  6%|         | 48.1M/862M [00:03<00:43, 18.7Mb/s]\u001b[A\n",
            "  6%|         | 51.0M/862M [00:03<00:44, 18.4Mb/s]\u001b[A\n",
            "  6%|         | 53.3M/862M [00:03<00:41, 19.6Mb/s]\u001b[A\n",
            "  6%|         | 55.4M/862M [00:04<02:47, 4.81Mb/s]\u001b[A\n",
            "  7%|         | 56.9M/862M [00:04<02:18, 5.81Mb/s]\u001b[A\n",
            "  7%|         | 58.3M/862M [00:06<06:52, 1.95Mb/s]\u001b[A\n",
            "  7%|         | 59.3M/862M [00:06<05:22, 2.49Mb/s]\u001b[A\n",
            "  7%|         | 60.2M/862M [00:06<04:20, 3.08Mb/s]\u001b[A\n",
            "  7%|         | 62.0M/862M [00:08<06:37, 2.02Mb/s]\u001b[A\n",
            "  7%|         | 62.6M/862M [00:08<05:35, 2.39Mb/s]\u001b[A\n",
            "  7%|         | 63.2M/862M [00:08<04:50, 2.75Mb/s]\u001b[A\n",
            "  7%|         | 63.8M/862M [00:08<04:14, 3.14Mb/s]\u001b[A\n",
            "  7%|         | 56.2M/862M [00:19<01:03, 12.6Mb/s]\n",
            "  8%|         | 66.3M/862M [00:10<09:53, 1.34Mb/s]\u001b[A\n",
            "  8%|         | 66.9M/862M [00:10<07:56, 1.67Mb/s]\u001b[A\n",
            "  8%|         | 67.4M/862M [00:10<06:37, 2.00Mb/s]\u001b[A\n",
            "  8%|         | 67.9M/862M [00:10<05:36, 2.36Mb/s]\u001b[A\n",
            "  8%|         | 70.0M/862M [00:11<04:11, 3.15Mb/s]\u001b[A\n",
            "  8%|         | 70.7M/862M [00:12<11:05, 1.19Mb/s]\u001b[A\n",
            "  8%|         | 71.3M/862M [00:12<08:44, 1.51Mb/s]\u001b[A\n",
            "  8%|         | 72.1M/862M [00:12<06:45, 1.95Mb/s]\u001b[A\n",
            "  9%|         | 74.1M/862M [00:13<04:59, 2.63Mb/s]\u001b[A\n",
            "  9%|         | 75.0M/862M [00:14<11:00, 1.19Mb/s]\u001b[A\n",
            "  9%|         | 75.6M/862M [00:14<08:35, 1.53Mb/s]\u001b[A\n",
            "  9%|         | 76.3M/862M [00:14<06:43, 1.95Mb/s]\u001b[A\n",
            "  9%|         | 78.3M/862M [00:15<04:57, 2.63Mb/s]\u001b[A\n",
            "  9%|         | 79.2M/862M [00:16<10:48, 1.21Mb/s]\u001b[A\n",
            "  9%|         | 79.8M/862M [00:16<08:26, 1.55Mb/s]\u001b[A\n",
            "  9%|         | 80.4M/862M [00:16<06:43, 1.94Mb/s]\u001b[A\n",
            " 10%|         | 82.5M/862M [00:17<04:56, 2.63Mb/s]\u001b[A\n",
            " 10%|         | 83.3M/862M [00:18<10:50, 1.20Mb/s]\u001b[A\n",
            " 10%|         | 83.9M/862M [00:18<08:27, 1.53Mb/s]\u001b[A\n",
            " 10%|         | 84.6M/862M [00:18<06:41, 1.94Mb/s]\u001b[A\n",
            " 10%|         | 86.7M/862M [00:18<04:55, 2.62Mb/s]\u001b[A\n",
            " 10%|         | 87.5M/862M [00:20<10:43, 1.20Mb/s]\u001b[A\n",
            " 10%|         | 88.1M/862M [00:20<08:23, 1.54Mb/s]\u001b[A\n",
            " 10%|         | 88.8M/862M [00:20<06:39, 1.94Mb/s]\u001b[A\n",
            " 11%|         | 90.8M/862M [00:20<04:53, 2.62Mb/s]\u001b[A\n",
            " 11%|         | 91.7M/862M [00:22<10:48, 1.19Mb/s]\u001b[A\n",
            " 11%|         | 92.3M/862M [00:22<08:26, 1.52Mb/s]\u001b[A\n",
            " 11%|         | 93.0M/862M [00:22<06:34, 1.95Mb/s]\u001b[A\n",
            " 11%|         | 95.2M/862M [00:22<04:49, 2.65Mb/s]\u001b[A\n",
            " 11%|         | 96.1M/862M [00:24<10:21, 1.23Mb/s]\u001b[A\n",
            " 11%|         | 96.7M/862M [00:24<08:02, 1.58Mb/s]\u001b[A\n",
            " 11%|        | 97.3M/862M [00:24<06:29, 1.96Mb/s]\u001b[A\n",
            " 12%|        | 99.4M/862M [00:24<04:46, 2.66Mb/s]\u001b[A\n",
            " 12%|        | 100M/862M [00:26<10:28, 1.21Mb/s] \u001b[A\n",
            " 12%|        | 101M/862M [00:26<08:10, 1.55Mb/s]\u001b[A\n",
            " 12%|        | 101M/862M [00:26<06:36, 1.92Mb/s]\u001b[A\n",
            " 12%|        | 104M/862M [00:26<04:51, 2.61Mb/s]\u001b[A\n",
            " 12%|        | 104M/862M [00:28<10:59, 1.15Mb/s]\u001b[A\n",
            " 12%|        | 106M/862M [00:28<08:09, 1.55Mb/s]\u001b[A\n",
            " 12%|        | 108M/862M [00:28<05:55, 2.12Mb/s]\u001b[A\n",
            " 13%|        | 109M/862M [00:30<10:55, 1.15Mb/s]\u001b[A\n",
            " 13%|        | 110M/862M [00:30<08:09, 1.54Mb/s]\u001b[A\n",
            " 13%|        | 112M/862M [00:30<05:55, 2.11Mb/s]\u001b[A\n",
            " 13%|        | 113M/862M [00:32<10:42, 1.17Mb/s]\u001b[A\n",
            " 13%|        | 114M/862M [00:32<08:00, 1.56Mb/s]\u001b[A\n",
            " 13%|        | 116M/862M [00:32<05:49, 2.14Mb/s]\u001b[A\n",
            " 14%|        | 117M/862M [00:34<10:36, 1.17Mb/s]\u001b[A\n",
            " 14%|        | 118M/862M [00:34<07:56, 1.56Mb/s]\u001b[A\n",
            " 14%|        | 120M/862M [00:34<05:46, 2.14Mb/s]\u001b[A\n",
            " 14%|        | 121M/862M [00:36<10:30, 1.18Mb/s]\u001b[A\n",
            " 14%|        | 122M/862M [00:36<07:51, 1.57Mb/s]\u001b[A\n",
            " 14%|        | 124M/862M [00:36<05:42, 2.15Mb/s]\u001b[A\n",
            " 15%|        | 125M/862M [00:38<10:25, 1.18Mb/s]\u001b[A\n",
            " 15%|        | 126M/862M [00:38<07:47, 1.57Mb/s]\u001b[A\n",
            " 15%|        | 129M/862M [00:38<05:39, 2.16Mb/s]\u001b[A\n",
            " 15%|        | 130M/862M [00:40<10:19, 1.18Mb/s]\u001b[A\n",
            " 15%|        | 130M/862M [00:40<07:44, 1.58Mb/s]\u001b[A\n",
            " 15%|        | 133M/862M [00:40<05:37, 2.16Mb/s]\u001b[A\n",
            " 16%|        | 134M/862M [00:42<10:19, 1.18Mb/s]\u001b[A\n",
            " 16%|        | 135M/862M [00:42<07:43, 1.57Mb/s]\u001b[A\n",
            " 16%|        | 137M/862M [00:42<05:36, 2.15Mb/s]\u001b[A\n",
            " 16%|        | 138M/862M [00:44<10:17, 1.17Mb/s]\u001b[A\n",
            " 16%|        | 139M/862M [00:44<07:41, 1.57Mb/s]\u001b[A\n",
            " 16%|        | 141M/862M [00:44<05:35, 2.15Mb/s]\u001b[A\n",
            " 16%|        | 142M/862M [00:46<10:10, 1.18Mb/s]\u001b[A\n",
            " 17%|        | 143M/862M [00:46<07:36, 1.57Mb/s]\u001b[A\n",
            " 17%|        | 145M/862M [00:46<05:32, 2.16Mb/s]\u001b[A\n",
            " 17%|        | 146M/862M [00:48<10:06, 1.18Mb/s]\u001b[A\n",
            " 17%|        | 147M/862M [00:48<07:33, 1.58Mb/s]\u001b[A\n",
            " 17%|        | 149M/862M [00:48<05:29, 2.16Mb/s]\u001b[A\n",
            " 17%|        | 150M/862M [00:50<10:03, 1.18Mb/s]\u001b[A\n",
            " 18%|        | 151M/862M [00:50<07:31, 1.57Mb/s]\u001b[A\n",
            " 18%|        | 153M/862M [00:50<05:28, 2.16Mb/s]\u001b[A\n",
            " 18%|        | 154M/862M [00:52<09:59, 1.18Mb/s]\u001b[A\n",
            " 18%|        | 155M/862M [00:52<07:29, 1.57Mb/s]\u001b[A\n",
            " 18%|        | 158M/862M [00:52<05:26, 2.16Mb/s]\u001b[A\n",
            " 18%|        | 159M/862M [00:54<09:55, 1.18Mb/s]\u001b[A\n",
            " 19%|        | 160M/862M [00:54<07:25, 1.58Mb/s]\u001b[A\n",
            " 19%|        | 162M/862M [00:54<05:23, 2.16Mb/s]\u001b[A\n",
            " 19%|        | 163M/862M [00:56<09:51, 1.18Mb/s]\u001b[A\n",
            " 19%|        | 164M/862M [00:56<07:22, 1.58Mb/s]\u001b[A\n",
            " 19%|        | 166M/862M [00:56<05:21, 2.16Mb/s]\u001b[A\n",
            " 19%|        | 167M/862M [00:58<09:49, 1.18Mb/s]\u001b[A\n",
            " 19%|        | 168M/862M [00:58<07:20, 1.58Mb/s]\u001b[A\n",
            " 20%|        | 170M/862M [00:58<05:20, 2.16Mb/s]\u001b[A\n",
            " 20%|        | 171M/862M [01:00<09:44, 1.18Mb/s]\u001b[A\n",
            " 20%|        | 172M/862M [01:00<07:15, 1.58Mb/s]\u001b[A\n",
            " 20%|        | 174M/862M [01:00<05:16, 2.17Mb/s]\u001b[A\n",
            " 20%|        | 175M/862M [01:02<09:35, 1.19Mb/s]\u001b[A\n",
            " 20%|        | 176M/862M [01:02<07:10, 1.59Mb/s]\u001b[A\n",
            " 21%|        | 178M/862M [01:02<05:12, 2.19Mb/s]\u001b[A\n",
            " 21%|        | 179M/862M [01:04<09:33, 1.19Mb/s]\u001b[A\n",
            " 21%|        | 180M/862M [01:04<07:08, 1.59Mb/s]\u001b[A\n",
            " 21%|        | 183M/862M [01:04<05:11, 2.18Mb/s]\u001b[A\n",
            " 21%|       | 184M/862M [01:06<09:29, 1.19Mb/s]\u001b[A\n",
            " 21%|       | 185M/862M [01:06<07:06, 1.59Mb/s]\u001b[A\n",
            " 22%|       | 187M/862M [01:06<05:10, 2.17Mb/s]\u001b[A\n",
            " 22%|       | 188M/862M [01:08<09:34, 1.17Mb/s]\u001b[A\n",
            " 22%|       | 189M/862M [01:08<07:09, 1.57Mb/s]\u001b[A\n",
            " 22%|       | 191M/862M [01:08<05:12, 2.15Mb/s]\u001b[A\n",
            " 22%|       | 192M/862M [01:10<09:28, 1.18Mb/s]\u001b[A\n",
            " 22%|       | 193M/862M [01:10<07:04, 1.57Mb/s]\u001b[A\n",
            " 23%|       | 195M/862M [01:10<05:08, 2.16Mb/s]\u001b[A\n",
            " 23%|       | 196M/862M [01:12<09:25, 1.18Mb/s]\u001b[A\n",
            " 23%|       | 197M/862M [01:12<07:02, 1.57Mb/s]\u001b[A\n",
            " 23%|       | 199M/862M [01:12<05:06, 2.16Mb/s]\u001b[A\n",
            " 23%|       | 200M/862M [01:14<09:19, 1.18Mb/s]\u001b[A\n",
            " 23%|       | 201M/862M [01:14<06:57, 1.58Mb/s]\u001b[A\n",
            " 24%|       | 203M/862M [01:14<05:03, 2.17Mb/s]\u001b[A\n",
            " 24%|       | 204M/862M [01:16<09:14, 1.19Mb/s]\u001b[A\n",
            " 24%|       | 205M/862M [01:16<06:53, 1.59Mb/s]\u001b[A\n",
            " 24%|       | 208M/862M [01:16<05:00, 2.18Mb/s]\u001b[A\n",
            " 24%|       | 209M/862M [01:18<09:09, 1.19Mb/s]\u001b[A\n",
            " 24%|       | 210M/862M [01:18<06:51, 1.59Mb/s]\u001b[A\n",
            " 25%|       | 212M/862M [01:18<04:58, 2.18Mb/s]\u001b[A\n",
            " 25%|       | 213M/862M [01:20<09:05, 1.19Mb/s]\u001b[A\n",
            " 25%|       | 214M/862M [01:20<06:48, 1.59Mb/s]\u001b[A\n",
            " 25%|       | 216M/862M [01:20<04:57, 2.17Mb/s]\u001b[A\n",
            " 25%|       | 217M/862M [01:22<09:05, 1.18Mb/s]\u001b[A\n",
            " 25%|       | 218M/862M [01:22<06:48, 1.58Mb/s]\u001b[A\n",
            " 26%|       | 220M/862M [01:22<04:56, 2.17Mb/s]\u001b[A\n",
            " 26%|       | 221M/862M [01:24<09:01, 1.18Mb/s]\u001b[A\n",
            " 26%|       | 222M/862M [01:24<06:45, 1.58Mb/s]\u001b[A\n",
            " 26%|       | 224M/862M [01:24<04:54, 2.17Mb/s]\u001b[A\n",
            " 26%|       | 225M/862M [01:26<08:58, 1.18Mb/s]\u001b[A\n",
            " 26%|       | 226M/862M [01:26<06:43, 1.58Mb/s]\u001b[A\n",
            " 26%|       | 228M/862M [01:26<04:53, 2.16Mb/s]\u001b[A\n",
            " 27%|       | 229M/862M [01:28<08:56, 1.18Mb/s]\u001b[A\n",
            " 27%|       | 230M/862M [01:28<06:40, 1.58Mb/s]\u001b[A\n",
            " 27%|       | 233M/862M [01:28<04:51, 2.16Mb/s]\u001b[A\n",
            " 27%|       | 234M/862M [01:30<08:49, 1.19Mb/s]\u001b[A\n",
            " 27%|       | 235M/862M [01:30<06:36, 1.58Mb/s]\u001b[A\n",
            " 27%|       | 237M/862M [01:30<04:48, 2.17Mb/s]\u001b[A\n",
            " 28%|       | 238M/862M [01:32<08:48, 1.18Mb/s]\u001b[A\n",
            " 28%|       | 239M/862M [01:32<06:35, 1.58Mb/s]\u001b[A\n",
            " 28%|       | 241M/862M [01:32<04:47, 2.16Mb/s]\u001b[A\n",
            " 28%|       | 242M/862M [01:34<08:45, 1.18Mb/s]\u001b[A\n",
            " 28%|       | 243M/862M [01:34<06:32, 1.58Mb/s]\u001b[A\n",
            " 28%|       | 245M/862M [01:34<04:45, 2.16Mb/s]\u001b[A\n",
            " 29%|       | 246M/862M [01:36<08:42, 1.18Mb/s]\u001b[A\n",
            " 29%|       | 247M/862M [01:36<06:29, 1.58Mb/s]\u001b[A\n",
            " 29%|       | 249M/862M [01:36<04:43, 2.16Mb/s]\u001b[A\n",
            " 29%|       | 250M/862M [01:38<08:38, 1.18Mb/s]\u001b[A\n",
            " 29%|       | 251M/862M [01:38<06:27, 1.58Mb/s]\u001b[A\n",
            " 29%|       | 253M/862M [01:38<04:41, 2.16Mb/s]\u001b[A\n",
            " 30%|       | 254M/862M [01:40<08:34, 1.18Mb/s]\u001b[A\n",
            " 30%|       | 255M/862M [01:40<06:25, 1.58Mb/s]\u001b[A\n",
            " 30%|       | 258M/862M [01:40<04:39, 2.16Mb/s]\u001b[A\n",
            " 30%|       | 259M/862M [01:42<08:31, 1.18Mb/s]\u001b[A\n",
            " 30%|       | 259M/862M [01:42<06:22, 1.57Mb/s]\u001b[A\n",
            " 30%|       | 262M/862M [01:42<04:38, 2.16Mb/s]\u001b[A\n",
            " 30%|       | 263M/862M [01:44<08:27, 1.18Mb/s]\u001b[A\n",
            " 31%|       | 264M/862M [01:44<06:19, 1.58Mb/s]\u001b[A\n",
            " 31%|       | 266M/862M [01:44<04:36, 2.16Mb/s]\u001b[A\n",
            " 31%|       | 267M/862M [01:46<08:23, 1.18Mb/s]\u001b[A\n",
            " 31%|       | 268M/862M [01:46<06:17, 1.58Mb/s]\u001b[A\n",
            " 31%|      | 270M/862M [01:46<04:33, 2.16Mb/s]\u001b[A\n",
            " 31%|      | 271M/862M [01:48<08:21, 1.18Mb/s]\u001b[A\n",
            " 32%|      | 272M/862M [01:48<06:15, 1.57Mb/s]\u001b[A\n",
            " 32%|      | 274M/862M [01:48<04:33, 2.15Mb/s]\u001b[A\n",
            " 32%|      | 275M/862M [01:50<08:24, 1.16Mb/s]\u001b[A\n",
            " 32%|      | 276M/862M [01:50<06:16, 1.56Mb/s]\u001b[A\n",
            " 32%|      | 278M/862M [01:50<04:33, 2.13Mb/s]\u001b[A\n",
            " 32%|      | 279M/862M [01:51<08:18, 1.17Mb/s]\u001b[A\n",
            " 33%|      | 280M/862M [01:52<06:12, 1.56Mb/s]\u001b[A\n",
            " 33%|      | 282M/862M [01:52<04:30, 2.14Mb/s]\u001b[A\n",
            " 33%|      | 283M/862M [01:53<08:12, 1.17Mb/s]\u001b[A\n",
            " 33%|      | 284M/862M [01:54<06:08, 1.57Mb/s]\u001b[A\n",
            " 33%|      | 287M/862M [01:54<04:27, 2.15Mb/s]\u001b[A\n",
            " 33%|      | 288M/862M [01:55<08:07, 1.18Mb/s]\u001b[A\n",
            " 33%|      | 289M/862M [01:56<06:04, 1.57Mb/s]\u001b[A\n",
            " 34%|      | 291M/862M [01:56<04:25, 2.16Mb/s]\u001b[A\n",
            " 34%|      | 292M/862M [01:57<08:04, 1.18Mb/s]\u001b[A\n",
            " 34%|      | 293M/862M [01:58<06:01, 1.57Mb/s]\u001b[A\n",
            " 34%|      | 295M/862M [01:58<04:22, 2.16Mb/s]\u001b[A\n",
            " 34%|      | 296M/862M [01:59<07:57, 1.19Mb/s]\u001b[A\n",
            " 34%|      | 297M/862M [02:00<05:57, 1.58Mb/s]\u001b[A\n",
            " 35%|      | 299M/862M [02:00<04:19, 2.17Mb/s]\u001b[A\n",
            " 35%|      | 300M/862M [02:01<07:53, 1.19Mb/s]\u001b[A\n",
            " 35%|      | 301M/862M [02:02<05:53, 1.59Mb/s]\u001b[A\n",
            " 35%|      | 303M/862M [02:02<04:17, 2.17Mb/s]\u001b[A\n",
            " 35%|      | 304M/862M [02:03<07:49, 1.19Mb/s]\u001b[A\n",
            " 35%|      | 305M/862M [02:04<05:50, 1.59Mb/s]\u001b[A\n",
            " 36%|      | 307M/862M [02:04<04:14, 2.18Mb/s]\u001b[A\n",
            " 36%|      | 308M/862M [02:05<07:46, 1.19Mb/s]\u001b[A\n",
            " 36%|      | 309M/862M [02:06<05:48, 1.59Mb/s]\u001b[A\n",
            " 36%|      | 312M/862M [02:06<04:12, 2.18Mb/s]\u001b[A\n",
            " 36%|      | 313M/862M [02:07<07:43, 1.19Mb/s]\u001b[A\n",
            " 36%|      | 314M/862M [02:08<05:45, 1.59Mb/s]\u001b[A\n",
            " 37%|      | 316M/862M [02:08<04:11, 2.18Mb/s]\u001b[A\n",
            " 37%|      | 317M/862M [02:09<07:39, 1.19Mb/s]\u001b[A\n",
            " 37%|      | 318M/862M [02:10<05:43, 1.59Mb/s]\u001b[A\n",
            " 37%|      | 320M/862M [02:10<04:09, 2.17Mb/s]\u001b[A\n",
            " 37%|      | 321M/862M [02:11<07:35, 1.19Mb/s]\u001b[A\n",
            " 37%|      | 322M/862M [02:11<05:40, 1.59Mb/s]\u001b[A\n",
            " 38%|      | 324M/862M [02:13<05:52, 1.53Mb/s]\u001b[A\n",
            " 38%|      | 325M/862M [02:13<04:48, 1.86Mb/s]\u001b[A\n",
            " 38%|      | 325M/862M [02:13<04:06, 2.18Mb/s]\u001b[A\n",
            " 38%|      | 326M/862M [02:13<03:16, 2.73Mb/s]\u001b[A\n",
            " 38%|      | 328M/862M [02:14<02:26, 3.63Mb/s]\u001b[A\n",
            " 38%|      | 329M/862M [02:15<06:43, 1.32Mb/s]\u001b[A\n",
            " 38%|      | 330M/862M [02:15<05:16, 1.68Mb/s]\u001b[A\n",
            " 38%|      | 330M/862M [02:15<04:17, 2.06Mb/s]\u001b[A\n",
            " 39%|      | 332M/862M [02:16<03:10, 2.79Mb/s]\u001b[A\n",
            " 39%|      | 333M/862M [02:17<07:13, 1.22Mb/s]\u001b[A\n",
            " 39%|      | 334M/862M [02:17<05:37, 1.56Mb/s]\u001b[A\n",
            " 39%|      | 334M/862M [02:17<04:32, 1.94Mb/s]\u001b[A\n",
            " 39%|      | 336M/862M [02:18<03:20, 2.63Mb/s]\u001b[A\n",
            " 39%|      | 337M/862M [02:19<07:15, 1.21Mb/s]\u001b[A\n",
            " 39%|      | 338M/862M [02:19<05:39, 1.54Mb/s]\u001b[A\n",
            " 39%|      | 339M/862M [02:19<04:33, 1.91Mb/s]\u001b[A\n",
            " 40%|      | 341M/862M [02:20<03:20, 2.60Mb/s]\u001b[A\n",
            " 40%|      | 342M/862M [02:21<07:16, 1.19Mb/s]\u001b[A\n",
            " 40%|      | 342M/862M [02:21<05:37, 1.54Mb/s]\u001b[A\n",
            " 40%|      | 343M/862M [02:21<04:31, 1.92Mb/s]\u001b[A\n",
            " 40%|      | 345M/862M [02:22<03:18, 2.60Mb/s]\u001b[A\n",
            " 40%|      | 346M/862M [02:23<07:29, 1.15Mb/s]\u001b[A\n",
            " 40%|      | 347M/862M [02:23<05:32, 1.55Mb/s]\u001b[A\n",
            " 40%|      | 349M/862M [02:24<04:01, 2.12Mb/s]\u001b[A\n",
            " 41%|      | 350M/862M [02:25<07:23, 1.15Mb/s]\u001b[A\n",
            " 41%|      | 351M/862M [02:25<05:30, 1.55Mb/s]\u001b[A\n",
            " 41%|      | 353M/862M [02:25<03:59, 2.12Mb/s]\u001b[A\n",
            " 41%|      | 354M/862M [02:27<07:12, 1.17Mb/s]\u001b[A\n",
            " 41%|      | 355M/862M [02:27<05:23, 1.57Mb/s]\u001b[A\n",
            " 41%|     | 357M/862M [02:27<03:54, 2.15Mb/s]\u001b[A\n",
            " 42%|     | 358M/862M [02:29<07:12, 1.17Mb/s]\u001b[A\n",
            " 42%|     | 359M/862M [02:29<05:26, 1.54Mb/s]\u001b[A\n",
            " 42%|     | 360M/862M [02:29<04:05, 2.05Mb/s]\u001b[A\n",
            " 42%|     | 361M/862M [02:30<03:00, 2.77Mb/s]\u001b[A\n",
            " 42%|     | 362M/862M [02:31<05:57, 1.40Mb/s]\u001b[A\n",
            " 42%|     | 363M/862M [02:31<04:39, 1.79Mb/s]\u001b[A\n",
            " 42%|     | 365M/862M [02:31<03:25, 2.42Mb/s]\u001b[A\n",
            " 42%|     | 366M/862M [02:33<06:52, 1.20Mb/s]\u001b[A\n",
            " 43%|     | 367M/862M [02:33<05:21, 1.54Mb/s]\u001b[A\n",
            " 43%|     | 367M/862M [02:33<04:22, 1.89Mb/s]\u001b[A\n",
            " 43%|     | 368M/862M [02:33<03:32, 2.32Mb/s]\u001b[A\n",
            " 43%|     | 370M/862M [02:35<04:15, 1.93Mb/s]\u001b[A\n",
            " 43%|     | 370M/862M [02:35<03:50, 2.14Mb/s]\u001b[A\n",
            " 43%|     | 371M/862M [02:35<03:27, 2.37Mb/s]\u001b[A\n",
            " 43%|     | 372M/862M [02:35<02:42, 3.01Mb/s]\u001b[A\n",
            " 43%|     | 374M/862M [02:35<02:02, 3.98Mb/s]\u001b[A\n",
            " 43%|     | 375M/862M [02:37<06:02, 1.34Mb/s]\u001b[A\n",
            " 44%|     | 375M/862M [02:37<04:47, 1.70Mb/s]\u001b[A\n",
            " 44%|     | 376M/862M [02:37<03:55, 2.06Mb/s]\u001b[A\n",
            " 44%|     | 378M/862M [02:37<02:53, 2.79Mb/s]\u001b[A\n",
            " 44%|     | 379M/862M [02:39<06:34, 1.22Mb/s]\u001b[A\n",
            " 44%|     | 380M/862M [02:39<05:08, 1.56Mb/s]\u001b[A\n",
            " 44%|     | 380M/862M [02:39<04:10, 1.93Mb/s]\u001b[A\n",
            " 44%|     | 382M/862M [02:39<03:03, 2.62Mb/s]\u001b[A\n",
            " 44%|     | 383M/862M [02:41<07:00, 1.14Mb/s]\u001b[A\n",
            " 45%|     | 384M/862M [02:41<05:11, 1.54Mb/s]\u001b[A\n",
            " 45%|     | 386M/862M [02:41<03:45, 2.11Mb/s]\u001b[A\n",
            " 45%|     | 387M/862M [02:43<06:53, 1.15Mb/s]\u001b[A\n",
            " 45%|     | 388M/862M [02:43<05:08, 1.54Mb/s]\u001b[A\n",
            " 45%|     | 391M/862M [02:43<03:43, 2.11Mb/s]\u001b[A\n",
            " 45%|     | 392M/862M [02:45<06:45, 1.16Mb/s]\u001b[A\n",
            " 46%|     | 393M/862M [02:45<05:03, 1.55Mb/s]\u001b[A\n",
            " 46%|     | 395M/862M [02:45<03:39, 2.13Mb/s]\u001b[A\n",
            " 46%|     | 396M/862M [02:47<06:39, 1.17Mb/s]\u001b[A\n",
            " 46%|     | 397M/862M [02:47<04:58, 1.56Mb/s]\u001b[A\n",
            " 46%|     | 399M/862M [02:47<03:36, 2.14Mb/s]\u001b[A\n",
            " 46%|     | 400M/862M [02:49<06:32, 1.18Mb/s]\u001b[A\n",
            " 47%|     | 401M/862M [02:49<04:54, 1.57Mb/s]\u001b[A\n",
            " 47%|     | 403M/862M [02:49<03:33, 2.15Mb/s]\u001b[A\n",
            " 47%|     | 404M/862M [02:51<06:28, 1.18Mb/s]\u001b[A\n",
            " 47%|     | 405M/862M [02:51<04:50, 1.57Mb/s]\u001b[A\n",
            " 47%|     | 407M/862M [02:51<03:30, 2.16Mb/s]\u001b[A\n",
            " 47%|     | 408M/862M [02:53<06:24, 1.18Mb/s]\u001b[A\n",
            " 47%|     | 409M/862M [02:53<04:47, 1.58Mb/s]\u001b[A\n",
            " 48%|     | 411M/862M [02:53<03:28, 2.16Mb/s]\u001b[A\n",
            " 48%|     | 412M/862M [02:55<06:24, 1.17Mb/s]\u001b[A\n",
            " 48%|     | 413M/862M [02:55<04:46, 1.57Mb/s]\u001b[A\n",
            " 48%|     | 416M/862M [02:55<03:28, 2.15Mb/s]\u001b[A\n",
            " 48%|     | 417M/862M [02:57<06:19, 1.17Mb/s]\u001b[A\n",
            " 48%|     | 418M/862M [02:57<04:42, 1.58Mb/s]\u001b[A\n",
            " 49%|     | 420M/862M [02:57<03:24, 2.16Mb/s]\u001b[A\n",
            " 49%|     | 421M/862M [02:59<06:12, 1.18Mb/s]\u001b[A\n",
            " 49%|     | 422M/862M [02:59<04:37, 1.59Mb/s]\u001b[A\n",
            " 49%|     | 424M/862M [02:59<03:21, 2.17Mb/s]\u001b[A\n",
            " 49%|     | 425M/862M [03:01<06:07, 1.19Mb/s]\u001b[A\n",
            " 49%|     | 426M/862M [03:01<04:34, 1.59Mb/s]\u001b[A\n",
            " 50%|     | 428M/862M [03:01<03:18, 2.18Mb/s]\u001b[A\n",
            " 50%|     | 429M/862M [03:03<06:04, 1.19Mb/s]\u001b[A\n",
            " 50%|     | 430M/862M [03:03<04:31, 1.59Mb/s]\u001b[A\n",
            " 50%|     | 432M/862M [03:03<03:17, 2.18Mb/s]\u001b[A\n",
            " 50%|     | 433M/862M [03:05<05:59, 1.19Mb/s]\u001b[A\n",
            " 50%|     | 434M/862M [03:05<04:29, 1.59Mb/s]\u001b[A\n",
            " 51%|     | 436M/862M [03:05<03:15, 2.18Mb/s]\u001b[A\n",
            " 51%|     | 437M/862M [03:07<05:58, 1.19Mb/s]\u001b[A\n",
            " 51%|     | 438M/862M [03:07<04:28, 1.58Mb/s]\u001b[A\n",
            " 51%|     | 441M/862M [03:07<03:14, 2.17Mb/s]\u001b[A\n",
            " 51%|     | 442M/862M [03:09<05:55, 1.18Mb/s]\u001b[A\n",
            " 51%|    | 443M/862M [03:09<04:25, 1.58Mb/s]\u001b[A\n",
            " 52%|    | 445M/862M [03:09<03:12, 2.17Mb/s]\u001b[A\n",
            " 52%|    | 446M/862M [03:11<05:52, 1.18Mb/s]\u001b[A\n",
            " 52%|    | 447M/862M [03:11<04:23, 1.58Mb/s]\u001b[A\n",
            " 52%|    | 449M/862M [03:11<03:11, 2.16Mb/s]\u001b[A\n",
            " 52%|    | 450M/862M [03:13<05:48, 1.18Mb/s]\u001b[A\n",
            " 52%|    | 451M/862M [03:13<04:20, 1.58Mb/s]\u001b[A\n",
            " 53%|    | 453M/862M [03:13<03:09, 2.16Mb/s]\u001b[A\n",
            " 53%|    | 454M/862M [03:15<05:45, 1.18Mb/s]\u001b[A\n",
            " 53%|    | 455M/862M [03:15<04:18, 1.58Mb/s]\u001b[A\n",
            " 53%|    | 457M/862M [03:15<03:07, 2.16Mb/s]\u001b[A\n",
            " 53%|    | 458M/862M [03:17<05:44, 1.17Mb/s]\u001b[A\n",
            " 53%|    | 459M/862M [03:17<04:15, 1.58Mb/s]\u001b[A\n",
            " 54%|    | 461M/862M [03:17<03:05, 2.16Mb/s]\u001b[A\n",
            " 54%|    | 462M/862M [03:19<05:36, 1.19Mb/s]\u001b[A\n",
            " 54%|    | 463M/862M [03:19<04:09, 1.60Mb/s]\u001b[A\n",
            " 54%|    | 466M/862M [03:19<03:01, 2.19Mb/s]\u001b[A\n",
            " 54%|    | 467M/862M [03:21<05:29, 1.20Mb/s]\u001b[A\n",
            " 54%|    | 467M/862M [03:21<04:06, 1.60Mb/s]\u001b[A\n",
            " 54%|    | 470M/862M [03:21<02:59, 2.19Mb/s]\u001b[A\n",
            " 55%|    | 471M/862M [03:23<05:28, 1.19Mb/s]\u001b[A\n",
            " 55%|    | 472M/862M [03:23<04:05, 1.59Mb/s]\u001b[A\n",
            " 55%|    | 474M/862M [03:23<02:58, 2.18Mb/s]\u001b[A\n",
            " 55%|    | 475M/862M [03:25<05:26, 1.19Mb/s]\u001b[A\n",
            " 55%|    | 476M/862M [03:25<04:04, 1.58Mb/s]\u001b[A\n",
            " 55%|    | 478M/862M [03:25<02:57, 2.17Mb/s]\u001b[A\n",
            " 56%|    | 479M/862M [03:27<05:25, 1.18Mb/s]\u001b[A\n",
            " 56%|    | 480M/862M [03:27<04:03, 1.57Mb/s]\u001b[A\n",
            " 56%|    | 482M/862M [03:27<02:56, 2.16Mb/s]\u001b[A\n",
            " 56%|    | 483M/862M [03:29<05:22, 1.17Mb/s]\u001b[A\n",
            " 56%|    | 484M/862M [03:29<04:00, 1.57Mb/s]\u001b[A\n",
            " 56%|    | 486M/862M [03:29<02:54, 2.15Mb/s]\u001b[A\n",
            " 57%|    | 487M/862M [03:31<05:18, 1.18Mb/s]\u001b[A\n",
            " 57%|    | 488M/862M [03:31<03:57, 1.57Mb/s]\u001b[A\n",
            " 57%|    | 491M/862M [03:31<02:52, 2.16Mb/s]\u001b[A\n",
            " 57%|    | 491M/862M [03:33<05:14, 1.18Mb/s]\u001b[A\n",
            " 57%|    | 492M/862M [03:33<03:54, 1.58Mb/s]\u001b[A\n",
            " 57%|    | 495M/862M [03:33<02:50, 2.16Mb/s]\u001b[A\n",
            " 57%|    | 496M/862M [03:35<05:09, 1.18Mb/s]\u001b[A\n",
            " 58%|    | 497M/862M [03:35<03:51, 1.58Mb/s]\u001b[A\n",
            " 58%|    | 499M/862M [03:35<02:48, 2.16Mb/s]\u001b[A\n",
            " 58%|    | 500M/862M [03:37<05:06, 1.18Mb/s]\u001b[A\n",
            " 58%|    | 501M/862M [03:37<03:49, 1.58Mb/s]\u001b[A\n",
            " 58%|    | 503M/862M [03:37<02:46, 2.16Mb/s]\u001b[A\n",
            " 58%|    | 504M/862M [03:39<05:03, 1.18Mb/s]\u001b[A\n",
            " 59%|    | 505M/862M [03:39<03:46, 1.57Mb/s]\u001b[A\n",
            " 59%|    | 507M/862M [03:39<02:44, 2.16Mb/s]\u001b[A\n",
            " 59%|    | 508M/862M [03:41<04:59, 1.18Mb/s]\u001b[A\n",
            " 59%|    | 509M/862M [03:41<03:43, 1.58Mb/s]\u001b[A\n",
            " 59%|    | 511M/862M [03:41<02:42, 2.16Mb/s]\u001b[A\n",
            " 59%|    | 512M/862M [03:43<04:56, 1.18Mb/s]\u001b[A\n",
            " 60%|    | 513M/862M [03:43<03:41, 1.58Mb/s]\u001b[A\n",
            " 60%|    | 515M/862M [03:43<02:40, 2.16Mb/s]\u001b[A\n",
            " 60%|    | 516M/862M [03:45<04:52, 1.18Mb/s]\u001b[A\n",
            " 60%|    | 517M/862M [03:45<03:38, 1.58Mb/s]\u001b[A\n",
            " 60%|    | 520M/862M [03:45<02:38, 2.16Mb/s]\u001b[A\n",
            " 60%|    | 521M/862M [03:47<04:48, 1.18Mb/s]\u001b[A\n",
            " 60%|    | 522M/862M [03:47<03:36, 1.58Mb/s]\u001b[A\n",
            " 61%|    | 524M/862M [03:47<02:36, 2.16Mb/s]\u001b[A\n",
            " 61%|    | 525M/862M [03:49<04:45, 1.18Mb/s]\u001b[A\n",
            " 61%|    | 526M/862M [03:49<03:33, 1.58Mb/s]\u001b[A\n",
            " 61%|    | 528M/862M [03:49<02:34, 2.16Mb/s]\u001b[A\n",
            " 61%|   | 529M/862M [03:51<04:41, 1.18Mb/s]\u001b[A\n",
            " 61%|   | 530M/862M [03:51<03:30, 1.58Mb/s]\u001b[A\n",
            " 62%|   | 532M/862M [03:51<02:32, 2.16Mb/s]\u001b[A\n",
            " 62%|   | 533M/862M [03:53<04:38, 1.18Mb/s]\u001b[A\n",
            " 62%|   | 534M/862M [03:53<03:28, 1.58Mb/s]\u001b[A\n",
            " 62%|   | 536M/862M [03:53<02:30, 2.16Mb/s]\u001b[A\n",
            " 62%|   | 537M/862M [03:55<04:35, 1.18Mb/s]\u001b[A\n",
            " 62%|   | 538M/862M [03:55<03:25, 1.58Mb/s]\u001b[A\n",
            " 63%|   | 540M/862M [03:55<02:28, 2.16Mb/s]\u001b[A\n",
            " 63%|   | 541M/862M [03:57<04:31, 1.18Mb/s]\u001b[A\n",
            " 63%|   | 542M/862M [03:57<03:22, 1.58Mb/s]\u001b[A\n",
            " 63%|   | 545M/862M [03:57<02:26, 2.16Mb/s]\u001b[A\n",
            " 63%|   | 546M/862M [03:59<04:28, 1.18Mb/s]\u001b[A\n",
            " 63%|   | 547M/862M [03:59<03:20, 1.58Mb/s]\u001b[A\n",
            " 64%|   | 549M/862M [03:59<02:25, 2.16Mb/s]\u001b[A\n",
            " 64%|   | 550M/862M [04:00<04:24, 1.18Mb/s]\u001b[A\n",
            " 64%|   | 551M/862M [04:01<03:17, 1.57Mb/s]\u001b[A\n",
            " 64%|   | 553M/862M [04:01<02:23, 2.16Mb/s]\u001b[A\n",
            " 64%|   | 554M/862M [04:02<04:20, 1.18Mb/s]\u001b[A\n",
            " 64%|   | 555M/862M [04:03<03:15, 1.58Mb/s]\u001b[A\n",
            " 65%|   | 557M/862M [04:03<02:21, 2.16Mb/s]\u001b[A\n",
            " 65%|   | 558M/862M [04:04<04:17, 1.18Mb/s]\u001b[A\n",
            " 65%|   | 559M/862M [04:05<03:12, 1.58Mb/s]\u001b[A\n",
            " 65%|   | 561M/862M [04:05<02:19, 2.16Mb/s]\u001b[A\n",
            " 65%|   | 562M/862M [04:06<04:13, 1.18Mb/s]\u001b[A\n",
            " 65%|   | 563M/862M [04:07<03:09, 1.58Mb/s]\u001b[A\n",
            " 66%|   | 565M/862M [04:07<02:17, 2.16Mb/s]\u001b[A\n",
            " 66%|   | 566M/862M [04:08<04:10, 1.18Mb/s]\u001b[A\n",
            " 66%|   | 567M/862M [04:09<03:08, 1.57Mb/s]\u001b[A\n",
            " 66%|   | 569M/862M [04:09<02:16, 2.15Mb/s]\u001b[A\n",
            " 66%|   | 570M/862M [04:10<03:59, 1.22Mb/s]\u001b[A\n",
            " 66%|   | 571M/862M [04:10<03:04, 1.58Mb/s]\u001b[A\n",
            " 66%|   | 572M/862M [04:11<02:27, 1.97Mb/s]\u001b[A\n",
            " 67%|   | 574M/862M [04:11<01:48, 2.66Mb/s]\u001b[A\n",
            " 67%|   | 574M/862M [04:12<03:57, 1.21Mb/s]\u001b[A\n",
            " 67%|   | 575M/862M [04:12<03:04, 1.55Mb/s]\u001b[A\n",
            " 67%|   | 576M/862M [04:13<02:27, 1.95Mb/s]\u001b[A\n",
            " 67%|   | 578M/862M [04:13<01:47, 2.64Mb/s]\u001b[A\n",
            " 67%|   | 579M/862M [04:14<03:51, 1.22Mb/s]\u001b[A\n",
            " 67%|   | 579M/862M [04:14<03:00, 1.57Mb/s]\u001b[A\n",
            " 67%|   | 580M/862M [04:15<02:25, 1.95Mb/s]\u001b[A\n",
            " 68%|   | 582M/862M [04:15<01:46, 2.64Mb/s]\u001b[A\n",
            " 68%|   | 583M/862M [04:16<04:03, 1.15Mb/s]\u001b[A\n",
            " 68%|   | 584M/862M [04:16<03:00, 1.54Mb/s]\u001b[A\n",
            " 68%|   | 586M/862M [04:17<02:10, 2.12Mb/s]\u001b[A\n",
            " 68%|   | 587M/862M [04:18<03:59, 1.15Mb/s]\u001b[A\n",
            " 68%|   | 588M/862M [04:18<02:58, 1.54Mb/s]\u001b[A\n",
            " 68%|   | 590M/862M [04:19<02:08, 2.11Mb/s]\u001b[A\n",
            " 69%|   | 591M/862M [04:20<03:52, 1.17Mb/s]\u001b[A\n",
            " 69%|   | 592M/862M [04:20<02:53, 1.56Mb/s]\u001b[A\n",
            " 69%|   | 595M/862M [04:21<02:05, 2.14Mb/s]\u001b[A\n",
            " 69%|   | 596M/862M [04:22<03:46, 1.18Mb/s]\u001b[A\n",
            " 69%|   | 596M/862M [04:22<02:49, 1.57Mb/s]\u001b[A\n",
            " 69%|   | 599M/862M [04:23<02:02, 2.15Mb/s]\u001b[A\n",
            " 70%|   | 600M/862M [04:24<03:44, 1.17Mb/s]\u001b[A\n",
            " 70%|   | 601M/862M [04:24<02:47, 1.56Mb/s]\u001b[A\n",
            " 70%|   | 603M/862M [04:25<02:01, 2.14Mb/s]\u001b[A\n",
            " 70%|   | 604M/862M [04:26<03:40, 1.17Mb/s]\u001b[A\n",
            " 70%|   | 605M/862M [04:26<02:44, 1.56Mb/s]\u001b[A\n",
            " 70%|   | 607M/862M [04:27<01:58, 2.15Mb/s]\u001b[A\n",
            " 71%|   | 608M/862M [04:28<03:35, 1.18Mb/s]\u001b[A\n",
            " 71%|   | 609M/862M [04:28<02:40, 1.57Mb/s]\u001b[A\n",
            " 71%|   | 611M/862M [04:29<01:56, 2.16Mb/s]\u001b[A\n",
            " 71%|   | 612M/862M [04:30<03:31, 1.18Mb/s]\u001b[A\n",
            " 71%|   | 613M/862M [04:30<02:38, 1.57Mb/s]\u001b[A\n",
            " 71%|  | 615M/862M [04:31<01:54, 2.16Mb/s]\u001b[A\n",
            " 71%|  | 616M/862M [04:32<03:28, 1.18Mb/s]\u001b[A\n",
            " 72%|  | 617M/862M [04:32<02:35, 1.58Mb/s]\u001b[A\n",
            " 72%|  | 619M/862M [04:32<01:52, 2.16Mb/s]\u001b[A\n",
            " 72%|  | 620M/862M [04:34<03:24, 1.18Mb/s]\u001b[A\n",
            " 72%|  | 621M/862M [04:34<02:32, 1.58Mb/s]\u001b[A\n",
            " 72%|  | 624M/862M [04:34<01:50, 2.16Mb/s]\u001b[A\n",
            " 72%|  | 625M/862M [04:36<03:21, 1.18Mb/s]\u001b[A\n",
            " 73%|  | 626M/862M [04:36<02:30, 1.58Mb/s]\u001b[A\n",
            " 73%|  | 628M/862M [04:36<01:48, 2.16Mb/s]\u001b[A\n",
            " 73%|  | 629M/862M [04:38<03:17, 1.18Mb/s]\u001b[A\n",
            " 73%|  | 630M/862M [04:38<02:27, 1.58Mb/s]\u001b[A\n",
            " 73%|  | 632M/862M [04:38<01:46, 2.16Mb/s]\u001b[A\n",
            " 73%|  | 633M/862M [04:40<03:14, 1.18Mb/s]\u001b[A\n",
            " 74%|  | 634M/862M [04:40<02:24, 1.58Mb/s]\u001b[A\n",
            " 74%|  | 636M/862M [04:40<01:44, 2.16Mb/s]\u001b[A\n",
            " 74%|  | 637M/862M [04:42<03:10, 1.18Mb/s]\u001b[A\n",
            " 74%|  | 638M/862M [04:42<02:22, 1.58Mb/s]\u001b[A\n",
            " 74%|  | 640M/862M [04:42<01:42, 2.16Mb/s]\u001b[A\n",
            " 74%|  | 641M/862M [04:44<03:05, 1.19Mb/s]\u001b[A\n",
            " 74%|  | 642M/862M [04:44<02:18, 1.58Mb/s]\u001b[A\n",
            " 75%|  | 644M/862M [04:44<01:40, 2.17Mb/s]\u001b[A\n",
            " 75%|  | 645M/862M [04:46<03:03, 1.18Mb/s]\u001b[A\n",
            " 75%|  | 646M/862M [04:46<02:16, 1.58Mb/s]\u001b[A\n",
            " 75%|  | 649M/862M [04:46<01:38, 2.16Mb/s]\u001b[A\n",
            " 75%|  | 650M/862M [04:48<02:59, 1.18Mb/s]\u001b[A\n",
            " 75%|  | 651M/862M [04:48<02:14, 1.58Mb/s]\u001b[A\n",
            " 76%|  | 653M/862M [04:48<01:36, 2.16Mb/s]\u001b[A\n",
            " 76%|  | 654M/862M [04:50<02:56, 1.18Mb/s]\u001b[A\n",
            " 76%|  | 655M/862M [04:50<02:11, 1.58Mb/s]\u001b[A\n",
            " 76%|  | 657M/862M [04:50<01:34, 2.16Mb/s]\u001b[A\n",
            " 76%|  | 658M/862M [04:52<02:52, 1.18Mb/s]\u001b[A\n",
            " 76%|  | 659M/862M [04:52<02:08, 1.58Mb/s]\u001b[A\n",
            " 77%|  | 661M/862M [04:52<01:32, 2.16Mb/s]\u001b[A\n",
            " 77%|  | 662M/862M [04:54<02:49, 1.18Mb/s]\u001b[A\n",
            " 77%|  | 663M/862M [04:54<02:06, 1.58Mb/s]\u001b[A\n",
            " 77%|  | 665M/862M [04:54<01:31, 2.16Mb/s]\u001b[A\n",
            " 77%|  | 666M/862M [04:56<02:45, 1.18Mb/s]\u001b[A\n",
            " 77%|  | 667M/862M [04:56<02:03, 1.58Mb/s]\u001b[A\n",
            " 78%|  | 669M/862M [04:56<01:29, 2.16Mb/s]\u001b[A\n",
            " 78%|  | 670M/862M [04:58<02:42, 1.18Mb/s]\u001b[A\n",
            " 78%|  | 671M/862M [04:58<02:00, 1.58Mb/s]\u001b[A\n",
            " 78%|  | 674M/862M [04:58<01:27, 2.16Mb/s]\u001b[A\n",
            " 78%|  | 675M/862M [05:00<02:38, 1.18Mb/s]\u001b[A\n",
            " 78%|  | 676M/862M [05:00<01:58, 1.58Mb/s]\u001b[A\n",
            " 79%|  | 678M/862M [05:00<01:25, 2.16Mb/s]\u001b[A\n",
            " 79%|  | 679M/862M [05:02<02:35, 1.18Mb/s]\u001b[A\n",
            " 79%|  | 680M/862M [05:02<01:55, 1.58Mb/s]\u001b[A\n",
            " 79%|  | 682M/862M [05:02<01:23, 2.16Mb/s]\u001b[A\n",
            " 79%|  | 683M/862M [05:04<02:31, 1.18Mb/s]\u001b[A\n",
            " 79%|  | 684M/862M [05:04<01:53, 1.58Mb/s]\u001b[A\n",
            " 80%|  | 686M/862M [05:04<01:21, 2.16Mb/s]\u001b[A\n",
            " 80%|  | 687M/862M [05:06<02:28, 1.18Mb/s]\u001b[A\n",
            " 80%|  | 688M/862M [05:06<01:50, 1.58Mb/s]\u001b[A\n",
            " 80%|  | 690M/862M [05:06<01:19, 2.16Mb/s]\u001b[A\n",
            " 80%|  | 691M/862M [05:08<02:24, 1.18Mb/s]\u001b[A\n",
            " 80%|  | 692M/862M [05:08<01:47, 1.58Mb/s]\u001b[A\n",
            " 81%|  | 694M/862M [05:08<01:17, 2.16Mb/s]\u001b[A\n",
            " 81%|  | 695M/862M [05:10<02:21, 1.18Mb/s]\u001b[A\n",
            " 81%|  | 696M/862M [05:10<01:45, 1.58Mb/s]\u001b[A\n",
            " 81%|  | 699M/862M [05:10<01:15, 2.16Mb/s]\u001b[A\n",
            " 81%|  | 700M/862M [05:12<02:17, 1.18Mb/s]\u001b[A\n",
            " 81%|  | 700M/862M [05:12<01:42, 1.58Mb/s]\u001b[A\n",
            " 82%| | 703M/862M [05:12<01:13, 2.16Mb/s]\u001b[A\n",
            " 82%| | 704M/862M [05:14<02:14, 1.18Mb/s]\u001b[A\n",
            " 82%| | 705M/862M [05:14<01:39, 1.58Mb/s]\u001b[A\n",
            " 82%| | 707M/862M [05:14<01:11, 2.16Mb/s]\u001b[A\n",
            " 82%| | 708M/862M [05:16<02:10, 1.18Mb/s]\u001b[A\n",
            " 82%| | 709M/862M [05:16<01:37, 1.58Mb/s]\u001b[A\n",
            " 82%| | 711M/862M [05:16<01:09, 2.16Mb/s]\u001b[A\n",
            " 83%| | 712M/862M [05:18<02:07, 1.18Mb/s]\u001b[A\n",
            " 83%| | 713M/862M [05:18<01:34, 1.58Mb/s]\u001b[A\n",
            " 83%| | 715M/862M [05:18<01:07, 2.16Mb/s]\u001b[A\n",
            " 83%| | 716M/862M [05:20<02:03, 1.18Mb/s]\u001b[A\n",
            " 83%| | 717M/862M [05:20<01:31, 1.58Mb/s]\u001b[A\n",
            " 83%| | 719M/862M [05:20<01:06, 2.16Mb/s]\u001b[A\n",
            " 84%| | 720M/862M [05:22<02:00, 1.18Mb/s]\u001b[A\n",
            " 84%| | 721M/862M [05:22<01:29, 1.58Mb/s]\u001b[A\n",
            " 84%| | 724M/862M [05:22<01:04, 2.16Mb/s]\u001b[A\n",
            " 84%| | 724M/862M [05:24<01:56, 1.18Mb/s]\u001b[A\n",
            " 84%| | 725M/862M [05:24<01:26, 1.58Mb/s]\u001b[A\n",
            " 84%| | 728M/862M [05:24<01:02, 2.16Mb/s]\u001b[A\n",
            " 85%| | 729M/862M [05:26<01:52, 1.18Mb/s]\u001b[A\n",
            " 85%| | 730M/862M [05:26<01:24, 1.58Mb/s]\u001b[A\n",
            " 85%| | 732M/862M [05:26<01:00, 2.16Mb/s]\u001b[A\n",
            " 85%| | 733M/862M [05:28<01:49, 1.18Mb/s]\u001b[A\n",
            " 85%| | 734M/862M [05:28<01:21, 1.58Mb/s]\u001b[A\n",
            " 85%| | 736M/862M [05:28<00:58, 2.16Mb/s]\u001b[A\n",
            " 85%| | 737M/862M [05:30<01:45, 1.18Mb/s]\u001b[A\n",
            " 86%| | 738M/862M [05:30<01:18, 1.58Mb/s]\u001b[A\n",
            " 86%| | 740M/862M [05:30<00:56, 2.16Mb/s]\u001b[A\n",
            " 86%| | 741M/862M [05:32<01:42, 1.18Mb/s]\u001b[A\n",
            " 86%| | 742M/862M [05:32<01:16, 1.58Mb/s]\u001b[A\n",
            " 86%| | 744M/862M [05:32<00:54, 2.16Mb/s]\u001b[A\n",
            " 86%| | 745M/862M [05:34<01:38, 1.18Mb/s]\u001b[A\n",
            " 87%| | 746M/862M [05:34<01:13, 1.58Mb/s]\u001b[A\n",
            " 87%| | 748M/862M [05:34<00:52, 2.16Mb/s]\u001b[A\n",
            " 87%| | 749M/862M [05:36<01:35, 1.18Mb/s]\u001b[A\n",
            " 87%| | 750M/862M [05:36<01:10, 1.58Mb/s]\u001b[A\n",
            " 87%| | 753M/862M [05:36<00:50, 2.16Mb/s]\u001b[A\n",
            " 87%| | 754M/862M [05:38<01:31, 1.18Mb/s]\u001b[A\n",
            " 88%| | 755M/862M [05:38<01:08, 1.58Mb/s]\u001b[A\n",
            " 88%| | 757M/862M [05:38<00:48, 2.16Mb/s]\u001b[A\n",
            " 88%| | 758M/862M [05:40<01:28, 1.18Mb/s]\u001b[A\n",
            " 88%| | 759M/862M [05:40<01:05, 1.58Mb/s]\u001b[A\n",
            " 88%| | 761M/862M [05:40<00:46, 2.16Mb/s]\u001b[A\n",
            " 88%| | 762M/862M [05:42<01:24, 1.18Mb/s]\u001b[A\n",
            " 88%| | 763M/862M [05:42<01:02, 1.58Mb/s]\u001b[A\n",
            " 89%| | 765M/862M [05:42<00:44, 2.16Mb/s]\u001b[A\n",
            " 89%| | 766M/862M [05:44<01:21, 1.18Mb/s]\u001b[A\n",
            " 89%| | 767M/862M [05:44<01:00, 1.58Mb/s]\u001b[A\n",
            " 89%| | 769M/862M [05:44<00:42, 2.16Mb/s]\u001b[A\n",
            " 89%| | 770M/862M [05:46<01:17, 1.18Mb/s]\u001b[A\n",
            " 89%| | 771M/862M [05:46<00:57, 1.58Mb/s]\u001b[A\n",
            " 90%| | 773M/862M [05:46<00:41, 2.16Mb/s]\u001b[A\n",
            " 90%| | 774M/862M [05:48<01:14, 1.18Mb/s]\u001b[A\n",
            " 90%| | 775M/862M [05:48<00:55, 1.58Mb/s]\u001b[A\n",
            " 90%| | 778M/862M [05:48<00:39, 2.16Mb/s]\u001b[A\n",
            " 90%| | 779M/862M [05:50<01:10, 1.19Mb/s]\u001b[A\n",
            " 90%| | 780M/862M [05:50<00:51, 1.59Mb/s]\u001b[A\n",
            " 91%| | 782M/862M [05:50<00:36, 2.18Mb/s]\u001b[A\n",
            " 91%| | 783M/862M [05:52<01:06, 1.19Mb/s]\u001b[A\n",
            " 91%| | 784M/862M [05:52<00:49, 1.59Mb/s]\u001b[A\n",
            " 91%| | 786M/862M [05:52<00:34, 2.18Mb/s]\u001b[A\n",
            " 91%|| 787M/862M [05:54<01:03, 1.18Mb/s]\u001b[A\n",
            " 91%|| 788M/862M [05:54<00:47, 1.58Mb/s]\u001b[A\n",
            " 92%|| 790M/862M [05:54<00:33, 2.17Mb/s]\u001b[A\n",
            " 92%|| 791M/862M [05:56<01:00, 1.18Mb/s]\u001b[A\n",
            " 92%|| 792M/862M [05:56<00:44, 1.58Mb/s]\u001b[A\n",
            " 92%|| 794M/862M [05:56<00:31, 2.16Mb/s]\u001b[A\n",
            " 92%|| 795M/862M [05:58<00:56, 1.18Mb/s]\u001b[A\n",
            " 92%|| 796M/862M [05:58<00:41, 1.58Mb/s]\u001b[A\n",
            " 93%|| 798M/862M [05:58<00:29, 2.16Mb/s]\u001b[A\n",
            " 93%|| 799M/862M [06:00<00:53, 1.18Mb/s]\u001b[A\n",
            " 93%|| 800M/862M [06:00<00:39, 1.58Mb/s]\u001b[A\n",
            " 93%|| 803M/862M [06:00<00:27, 2.16Mb/s]\u001b[A\n",
            " 93%|| 804M/862M [06:01<00:49, 1.18Mb/s]\u001b[A\n",
            " 93%|| 806M/862M [06:02<00:25, 2.23Mb/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "...done. (363 seconds, 6 min)\n",
            "Extracting data from /root/web_data/embeddings/glove.6B/glove.6B.zip...\n",
            "   ...done.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/content/web/evaluate.py:336: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
            "  A = np.vstack(w.get(word, mean_vector) for word in X[:, 0])\n",
            "/content/web/evaluate.py:337: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
            "  B = np.vstack(w.get(word, mean_vector) for word in X[:, 1])\n",
            "Missing 24 words. Will replace them with mean vector\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Spearman correlation of scores on MEN 0.7374646969805517\n",
            "Spearman correlation of scores on WS353 0.5433255613304138\n",
            "Spearman correlation of scores on SIMLEX999 0.37050035710869067\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fW9y8I-PjrOR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}