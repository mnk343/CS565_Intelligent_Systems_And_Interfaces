# -*- coding: utf-8 -*-
"""CS565_Assignment_1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/162xNqoKN7RxDLAT39AmZByKuxIP9Tkb8
"""

from google.colab import drive
drive.mount('/content/drive')

"""# Question 1 Analysis of existing tools

### Pre-built libraries
"""

# 
# 
# NLTK ENGLISH
# with TreeBank Tokenizer
# 

from google.colab import drive
import nltk
file = open('corpus/en_wiki.txt')
write_sentence_file_object = open('/content/drive/My Drive/results/english_sentences_approach_1.txt', "w")
write_word_file_object = open('/content/drive/My Drive/results/english_words_approach_1.txt', "w")

corpus_text = file.read()
sentence_tokenize_english = nltk.tokenize.sent_tokenize(corpus_text)

for sentence in sentence_tokenize_english:
  write_sentence_file_object.write(sentence)
  write_sentence_file_object.write("\n")

  word_tokenize_english = nltk.tokenize.TreebankWordTokenizer().tokenize(sentence)
  for word in word_tokenize_english:
    write_word_file_object.write(word)
    write_word_file_object.write("\n")

write_word_file_object.close()
write_sentence_file_object.close()
file.close()

# 
# 
# NLTK ENGLISH
# with WordPunct tokenizer
# 

import nltk
file = open('corpus/en_wiki.txt')
write_sentence_file_object = open('/content/drive/My Drive/results/english_sentences_approach_2.txt', "w")
write_word_file_object = open('/content/drive/My Drive/results/english_words_approach_2.txt', "w")

corpus_text = file.read()
tokenizer = nltk.data.load('tokenizers/punkt/PY3/english.pickle') 
sentence_tokenize_english = tokenizer.tokenize(corpus_text) 

for sentence in sentence_tokenize_english:
  write_sentence_file_object.write(sentence)
  write_sentence_file_object.write("\n")

  word_tokenize_english = nltk.tokenize.WordPunctTokenizer().tokenize(sentence)
  for word in word_tokenize_english:
    write_word_file_object.write(word)
    write_word_file_object.write("\n")

write_word_file_object.close()
write_sentence_file_object.close()
file.close()

# 
# 
# SpaCy ENGLISH
# 
# 

import spacy
file = open('corpus/en_wiki.txt')
write_sentence_file_object = open('/content/drive/My Drive/results/english_sentences_spaCy_approach_1.txt', "w")
write_word_file_object = open('/content/drive/My Drive/results/english_words_spaCy_approach_1.txt', "w")

nlp = spacy.load("en_core_web_sm")


for line in file:
  line = line.rstrip()
  sentence_tokenize_english = nlp(line)
  for sentence in sentence_tokenize_english.sents:
      sentence = str(sentence)
      sentence = sentence.rstrip()
      write_sentence_file_object.write(sentence)
      write_sentence_file_object.write("\n")

      word_tokenize_english = nlp(sentence)
      for word in word_tokenize_english:
        word = str(word)
        if word != "":
          write_word_file_object.write(word)
          write_word_file_object.write("\n")

write_word_file_object.close()
write_sentence_file_object.close()
file.close()

# 
# 
# indicNLP HINDI
# 
# 

!pip install indic-nlp-library
from indicnlp.tokenize import sentence_tokenize, indic_tokenize

file = open('corpus/hi_wiki.txt')
write_sentence_file_object = open('/content/drive/My Drive/results/hindi_sentences_indicnlp.txt', "w")
write_word_file_object = open('/content/drive/My Drive/results/hindi_words_indicnlp.txt', "w")

sentences=sentence_tokenize.sentence_split(indic_string, lang='hi')

for line in file:
  sentence_tokenize_hindi = sentence_tokenize.sentence_split(line, lang='hi')

  for sentence in sentence_tokenize_hindi:
    write_sentence_file_object.write(sentence)
    write_sentence_file_object.write("\n")

    word_tokenize_hindi = indic_tokenize.trivial_tokenize(sentence)
    for word in word_tokenize_hindi:
      write_word_file_object.write(word)
      write_word_file_object.write("\n")

write_word_file_object.close()
write_sentence_file_object.close()
file.close()

"""## Custom Models 
### to attempt Word tokenization and sentence segmentation
"""

# 
# 
# Custom English
# 
# 

known_abbreviations = ["dr" , "mr" , "mrs" , "vs" , "etc" , "jr" , "sr", "prof"]
def check_if_known_abbreviation(sentence):
  last_word = sentence.split()[-1]
  last_word = last_word.lower()
  last_word = last_word[:-1]

  for abbr in known_abbreviations:
    if last_word == abbr:
      return True
  return False

def tokenize_words_from_sentence(sentence , file):
  word = ""
  is_single_quotes = 0
  sentence.rstrip()
  
  for char in sentence:
    if is_single_quotes == 1:
      if char != ' ':
        word += char
        file.write(word)
        file.write("\n")

      else:
        file.write(char)
        file.write("\n")
      
      word = ""
      is_single_quotes = 0
      continue
    is_single_quotes = 0
    if char == " ":
      word = word.strip()

      if( word != "" ):
        file.write(word)
        file.write("\n") 
      word = ""
      continue
    
    char = char.lower()
    if char == '\'':
      if word != "":
        file.write(word)
        file.write("\n") 
      word = "'"
      is_single_quotes = 1
    
    elif char == '.':
      word += char

      if check_if_known_abbreviation(word):
        file.write(word)
        file.write("\n") 
      else:
        word = word[:-1]
        word.split()
        if word != "":
          file.write(word)
          file.write("\n") 
        file.write(char)
        file.write("\n") 
      word = ""

    elif char == ',' or char=='?' or char == ';' or char == '!' or char == '"':
      if word != "":
        file.write(word)
        file.write("\n") 
      file.write(char)
      file.write("\n") 
      word = ""

    else:
      word += char
  
  if word != "":
      file.write(word)
      file.write("\n") 

file = open('corpus/en_wiki.txt')
write_sentence_file_object = open('/content/drive/My Drive/results/english_sentences_custom.txt', "w")
write_word_file_object = open('/content/drive/My Drive/results/english_words_custom.txt', "w")

for line in file:
  line = line.rstrip()
  sentence = ""
  count_quotes = 0
  
  for char in line:
    sentence += char  
    if count_quotes == 0 and char == '"':
      count_quotes += 1

    elif count_quotes != 0 and char=='"':
      count_quotes -= 1
   
    elif count_quotes == 0 and char == '.' and check_if_known_abbreviation(sentence):
      continue

    elif not (count_quotes != 0) and not (char != '.' and char != '!' and char != '?' and char != ':'):
      sentence = sentence.strip()
      write_sentence_file_object.write(sentence)
      write_sentence_file_object.write("\n")
      tokenize_words_from_sentence(sentence , write_word_file_object )
      sentence = ""
      
  sentence = sentence.strip()
  if sentence != "":
    write_sentence_file_object.write(sentence)
    write_sentence_file_object.write("\n")
    print(sentence)
    tokenize_words_from_sentence(sentence , write_word_file_object )
    sentence = ""

# 
# 
# Custom Hindi
# 
# 

def tokenize_words_from_sentence(sentence , file):
  word = ""
  sentence.rstrip()
  
  for char in sentence:

    if char == " ":
      word = word.strip()

      if( word != "" ):
        file.write(word)
        file.write("\n") 
      word = ""
      continue
    
    elif char=='(' or char == ')' or char== '.' or char == ',' or char=='?' or char == ';' or char == '!' or char == '"' or char == '|' or char == '\'' or char == '%' or char == '$':
      word = word.strip()
      if word != "":
        file.write(word)
        file.write("\n") 
      file.write(char)
      file.write("\n") 
      word = ""

    else:
      word += char
  
  if word != "":
      file.write(word)
      file.write("\n") 

file = open('hi_wiki.txt')
write_sentence_file_object = open('/content/drive/My Drive/results/hindi_sentences_custom.txt', "w")
write_word_file_object = open('/content/drive/My Drive/results/hindi_words_custom.txt', "w")

for line in file:
  line = line.rstrip()
  sentence = ""
  count_quotes = 0
  
  for char in line:
    sentence += char  
    if count_quotes == 0 and char == '"':
      count_quotes += 1

    elif count_quotes != 0 and char=='"':
      count_quotes -= 1

    elif not (count_quotes != 0) and not (char != '|' and char != '!' and char != '?' and char != ':'):
      sentence = sentence.strip()
      write_sentence_file_object.write(sentence)
      write_sentence_file_object.write("\n")
      tokenize_words_from_sentence(sentence , write_word_file_object )
      sentence = ""
      
  sentence = sentence.strip()
  if sentence != "":
    write_sentence_file_object.write(sentence)
    write_sentence_file_object.write("\n")
    print(sentence)
    tokenize_words_from_sentence(sentence , write_word_file_object )
    sentence = ""

"""## Frequency Distributions of models"""

# 
# 
# Plot Frequency Distribution
# 
# 

!pip install snowballstemmer
import snowballstemmer
import numpy as np 
from prettytable import PrettyTable
import matplotlib.pyplot as plt  
from nltk.stem import PorterStemmer 
stemmer = PorterStemmer() 
hindi_stemmer = snowballstemmer.HindiStemmer()
is_file_english = 1

val = input("Enter input file : ")
n = input("Enter the value of n (1 for unigram, 2 for bigram, 3 for trigram): ")
n = int(n)
is_stem = input("Enter Y to enable stemming: ")
is_stem = is_stem.lower()
if( is_stem == "y"):
  is_stem = 1
else:
  is_stem = 0 
if val[0] == 'h':
  is_file_english = 0
path_to_file = "/content/drive/My Drive/results/" + val
file = write_word_file_object = open(path_to_file, "r")

frequency_of_frequency = {}
for number in range(1,14):
  frequency_of_frequency[number] = 0

def generate_n_grams(n , list_ngrams):
  ngram = ""
  ctr = 1
  for line in file:
    line = line.rstrip()
    if line == "," or line == ":" or line == "?" or line == "." or line == "'" or line == "\"" or line =="(" or line == ")":
      continue

    ngram += line
    ngram += " "

    if ctr < n:
      ctr += 1
    else:
      list_ngrams.append(ngram.strip())
      ngram = ngram.split(' ',1)[1]

def plot_frequency_distribution ( list_ngrams, is_stem ):
  frequency = {} 
  rank = {}
  global is_file_english
  for ngram in list_ngrams:
    if is_stem == 1:
      if is_file_english == 0 :
        temp = ngram.split()
        for i in range(len(temp)):
          word = hindi_stemmer.stemWord(temp[i])
          temp[i] = word
        ngram = " ".join(temp)
      
      else:
        temp = ngram.split()
        for i in range(len(temp)):
          word = stemmer.stem(temp[i])
          temp[i] = word
        ngram = " ".join(temp)
      
    if ngram in frequency:
      frequency[ngram] += 1
    else:
      frequency[ngram] = 1
    
  rank_ngrams = sorted(frequency.items(), key = 
                      lambda kv:(kv[1], kv[0]) , reverse = True)

  ctr = 1
  data_for_plot = {}

  for pair in rank_ngrams:
    if (pair[1] < 11):
      frequency_of_frequency[pair[1]] += 1
    elif pair[1] >= 11 and pair[1] <= 50:
      frequency_of_frequency[11] += 1
    elif pair[1] >= 51 and pair[1] <= 100:
      frequency_of_frequency[12] += 1
    else:
      frequency_of_frequency[13] += 1
    
    rank[pair[0]] = ctr
    
    if ctr < 100 :
      data_for_plot[ctr] = pair[1]

    ctr += 1

  ranks = list(data_for_plot.keys()) 
  values = list(data_for_plot.values()) 
  figure = plt.figure(figsize = (10, 5)) 

  plt.bar(ranks, values, color ='coral', width=1) 
  plt.xlabel("Ranks of ngrams") 
  plt.ylabel("Frequency") 
  plt.title("Frequency distribution of ngrams with top 100 ranks") 
  plt.show() 

  table = PrettyTable()
  column_1 = ["1","2","3","4","5","6","7","8","9","10","11-50","51-100",">100"]
  table.field_names = ["Word Frequency", "Frequency of frequency"]
  for i in range(0,13):
    table.add_row([column_1[i] , frequency_of_frequency[i+1]])
  table.align["Word Frequency"] = "r"

  print(table)

  table = PrettyTable()

  table.field_names = ["Word", "Frequency"]
  for i in range(0,min(100,len(rank_ngrams))):
    table.add_row([rank_ngrams[i][0] ,rank_ngrams[i][1] ])
  print(table)

  print("Most frequent word: " + rank_ngrams[0][0] + " with frequency of " + str(rank_ngrams[0][1] ))
  print("Least frequent word: " + rank_ngrams[-1][0] + " with frequency of " + str(rank_ngrams[-1][1]) )
ngram_list = [] 

generate_n_grams(n, ngram_list)

frequency_ngram_list = {}
rank_ngram_list = {}

plot_frequency_distribution(ngram_list , is_stem)

"""# Question 2 Few Basic Questions"""

# 
# 
# Find Coverage of unigrams, bigrams,trigrams
# 
# 

import snowballstemmer
from nltk.stem import PorterStemmer 

stemmer = PorterStemmer() 
hindi_stemmer = snowballstemmer.HindiStemmer()

is_file_english = 1

val = input("Enter input file : ")
path_to_file = "/content/drive/My Drive/results/" + val
file = open(path_to_file, "r")

is_stem = input("Enter Y to enable stemming: ")
is_stem = is_stem.lower()
if is_stem == 'y':
  is_stem = 1
else:
  is_stem = 0

def generate_n_grams(n , list_ngrams):
  ngram = ""
  ctr = 1
  count_of_words = 0

  for line in file:
    count_of_words += 1
    
    line = line.rstrip()
    if line == "," or line == ":" or line == "?" or line == "." or line == "'" or line == "\"" or line =="(" or line == ")":
      continue

    ngram += line
    ngram += " "

    if ctr < n:
      ctr += 1
    else:
      list_ngrams.append(ngram.strip())
      ngram = ngram.split(' ',1)[1]
    
def frequency_ngram ( list_ngrams, frequency_list ):
  rank = {}
  for ngram in list_ngrams:
    if is_stem == 1:
      if is_file_english == 0 :
        temp = ngram.split()
        for i in range(len(temp)):
          word = hindi_stemmer.stemWord(temp[i])
          temp[i] = word
        ngram = " ".join(temp)

      else:
        temp = ngram.split()
        for i in range(len(temp)):
          word = stemmer.stem(temp[i])
          temp[i] = word
        ngram = " ".join(temp)

    if ngram in frequency_list:
      frequency_list[ngram] += 1
    else:
      frequency_list[ngram] = 1
    
  rank_ngrams = sorted(frequency_list.items(), key = 
                      lambda kv:(kv[1], kv[0]) , reverse = True)

  ctr = 1
  for pair in rank_ngrams:
    rank[pair[0]] = ctr
    ctr += 1

def find_coverage( ngram_list , frequency_ugram_list , percentage ):
  rank_ngrams = sorted(frequency_ugram_list.items(), key = 
                      lambda kv:(kv[1], kv[0]) , reverse = True)
  sum = 0
  number_of_ngrams = 0
  percentage /= 100
  
  for pair in rank_ngrams:
    sum += pair[1]
    coverage_so_far = sum / len(ngram_list)
    number_of_ngrams += 1

    if coverage_so_far >= percentage :
      return str(number_of_ngrams)
    
unigram_list = []
generate_n_grams(1, unigram_list)
file.seek(0,0)

bigram_list = []
generate_n_grams(2, bigram_list)
file.seek(0,0)

trigram_list = []
generate_n_grams(3, trigram_list)
file.seek(0,0)


frequency_unigram_list = {}
frequency_bigram_list = {}
frequency_trigram_list = {}

frequency_ngram(unigram_list , frequency_unigram_list)
frequency_ngram(bigram_list , frequency_bigram_list)
frequency_ngram(trigram_list , frequency_trigram_list)

print("Number of unigrams to cover 90% of corpora: " + find_coverage( unigram_list , frequency_unigram_list , 90 ))
print("Number of bigram to cover 80% of corpora: " + find_coverage( bigram_list , frequency_bigram_list , 80 ))
print("Number of trigram to cover 70% of corpora: " + find_coverage( trigram_list , frequency_trigram_list , 70 ))

import re , collections

vocabulary_size = 100000
number_of_iterations = 15000

val = input("Enter input file : ")
path_to_file = "/content/drive/My Drive/results/" + val
file = open(path_to_file, "r")

if( val [0] =='e'):
	val1 = "english"
else:
	val1 = "hindi"

writer_file = open("/content/drive/My Drive/results/"+val1 + "_bpe.txt", "w")

def generate_n_grams(n , list_ngrams):
  ngram = ""
  ctr = 1
  count_of_words = 0
  for line in file:
    count_of_words += 1
    
    line = line.rstrip()
    if line == "," or line == ":" or line == "?" or line == "." or line == "'" or line == "\"" or line =="(" or line == ")":
      continue

    ngram += line
    ngram += " "

    if ctr < n:
      ctr += 1
    else:
      list_ngrams.append(ngram.strip())
      ngram = ngram.split(' ',1)[1]

def frequency_ngram ( list_ngrams, vocab ):
	rank = {}
	global vocabulary_size
	frequency_list= {}
	for ngram in list_ngrams:
		if ngram in frequency_list:
			frequency_list[ngram] += 1
		else:
			frequency_list[ngram] = 1
		
	rank_ngrams = sorted(frequency_list.items(), key = 
											lambda kv:(kv[1], kv[0]) , reverse = True)

	for i in range(vocabulary_size):
		word = rank_ngrams[i][0]
		vocab_word = ""
		for char in word:
			vocab_word += char
			vocab_word += " "
		vocab_word += "~"
		vocab[vocab_word] = rank_ngrams[i][1]

def getstats ( vocab ) :
	pairs = collections . defaultdict( int )
	for word , freq in vocab .items ( ) :
	  symbols = word.split()
	  for i in range ( len(symbols) -1 ) :
	    pairs[ symbols [ i ] , symbols [ i + 1 ] ] += freq
	return pairs

def mergevocab ( pair , vin ) :
	vout = {}
	bigram = re . escape ( ' '.join ( pair ) )
	p = re.compile ( r'(?<!\S)' + bigram + r'(?!\S)' )
	for word in vin :
	  wout = p.sub ( ''. join ( pair ) , word )
	  vout [ wout ] = vin [ word ]
	return vout
  
unigram_list = []
generate_n_grams(1, unigram_list)
file.seek(0,0)

vocab = {}
frequency_ngram(unigram_list,vocab )

for i in range ( number_of_iterations ) :
  pairs = getstats ( vocab )
  best = max ( pairs , key= pairs . get )
  vocab = mergevocab ( best , vocab )

frequency_sub_tokens = {}

for key,value in vocab.items():
	sub_tokens = key.split()
	for sub_token in sub_tokens:
		if sub_token in frequency_sub_tokens:
			frequency_sub_tokens[sub_token] += value
		else:
			frequency_sub_tokens[sub_token] = value

rank_sub_tokens = sorted(frequency_sub_tokens.items(), key = 
										lambda kv:(kv[1], kv[0]) , reverse = True)

print("50 most frequent sub-tokens: ")
for i in range(50):
	if( rank_sub_tokens[-1*i][0]  == "~" ):
		continue
	print( rank_sub_tokens[i][0]  + " with frequency: " + str(rank_sub_tokens[i][1]) )

print("\n")
print("50 least frequent sub-tokens: ")
for i in range(50):
	if( rank_sub_tokens[-1*i][0]  == "~" ):
		continue
	print( rank_sub_tokens[-1*i][0]  + " with frequency: " + str(rank_sub_tokens[-1*i][1]) )
 
for line,freq in vocab.items():
	writer_file.write(line + "_" + str(freq))
	writer_file.write("\n")

"""# Question 3 Likelihood Ratio"""

# 
# 
# Likelihood Ratio Test to find collocations
# 
# 

import math

val = input("Enter input file : ")
path_to_file = "/content/drive/My Drive/results/" + val
file = open(path_to_file, "r")
filename = "likelihood_ratio_test";
if( val[0] == 'e' ):
  filename += "english.txt"
else:
  filename += "hindi.txt"

write_word_file_object = open('/content/drive/My Drive/results/' + filename , "w")

def auxiliary_value( k , n , x):
  return math.pow(1-x, n-k) * math.pow(x,k) 

def perform_likelihood_ratio_test( collocations_with_confidence , unigram_list , bigram_list, number_unigrams ):

  for bigram, frequency in bigram_list.items(): 
    bigram = bigram.rstrip()
    unigrams = bigram.split()
    if( len(unigrams) < 2):
      continue
    first_unigram = unigrams[0]
    second_unigram = unigrams[1]
    if second_unigram not in unigram_list or first_unigram not in unigram_list:
      continue

    p = unigram_list[second_unigram] / number_unigrams
    p1 = frequency / unigram_list[first_unigram]
    p2 = ( unigram_list[second_unigram] - frequency ) / ( number_unigrams - unigram_list[first_unigram] )
    val = auxiliary_value( frequency ,unigram_list[first_unigram] , p  ) 
    if val <= 0 :
      val = 1
      continue
    confidence_value = math.log( val ,10)

    val = auxiliary_value ( unigram_list[second_unigram] - frequency  , number_unigrams- unigram_list[first_unigram] , p )
    if val <= 0 :
      val = 1
      continue
    confidence_value += math.log( val ,10)
    
    val = auxiliary_value (  frequency , unigram_list[first_unigram] , p1 ) 
    if val <= 0 :
      val = 1
      continue
    # print(val)
    confidence_value -= math.log( val ,10)
    
    val = auxiliary_value (  unigram_list[second_unigram] - frequency, number_unigrams- unigram_list[first_unigram] , p2 ) 
    if val <= 0 :
      val = 1
      continue
    
    confidence_value -= math.log( val ,10)

    confidence_value *= -2
    collocations_with_confidence[bigram] = confidence_value

def generate_n_grams(n , list_ngrams):
  ngram = ""
  ctr = 1
  count_of_words = 0

  for line in file:
    count_of_words += 1
    
    line = line.rstrip()
    if line == "," or line == ":" or line == "?" or line == "." or line == "'" or line == "\"" or line =="(" or line == ")":
      continue

    ngram += line
    ngram += " "

    if ctr < n:
      ctr += 1
    else:
      list_ngrams.append(ngram.strip())
      ngram = ngram.split(' ',1)[1]

def frequency_ngram ( list_ngrams, frequency_list ):

  rank = {}
  for ngram in list_ngrams:
    if ngram in frequency_list:
      frequency_list[ngram] += 1
    else:
      frequency_list[ngram] = 1
    
  rank_ngrams = sorted(frequency_list.items(), key = 
                      lambda kv:(kv[1], kv[0]) , reverse = True)

  ctr = 1
  for pair in rank_ngrams:
    rank[pair[0]] = ctr
    ctr += 1

unigram_list = []
generate_n_grams(1, unigram_list)
file.seek(0,0)

bigram_list = []
generate_n_grams(2, bigram_list)

frequency_unigram_list = {}
frequency_bigram_list = {}

frequency_ngram(unigram_list , frequency_unigram_list)
frequency_ngram(bigram_list , frequency_bigram_list)

collocations_with_confidence = {}
perform_likelihood_ratio_test( collocations_with_confidence, frequency_unigram_list , frequency_bigram_list , len(unigram_list) )

collocation = sorted(collocations_with_confidence.items(), key = 
                      lambda kv:(kv[1], kv[0]) , reverse = True)

count_collocations = 0
for pair in collocation: 
    if( pair[1] > 50 ):
      count_collocations += 1
      write_word_file_object.write(str(pair[0]) + " " + str(pair[1]))
      write_word_file_object.write("\n")

print( "NUMBER OF BIGRAMS: " + str(len(bigram_list)))
print( "NUMBER OF COLLOCATIONS: " + str(count_collocations))
print( "RATIO: " +  str(count_collocations/len(bigram_list)) )

"""# Question 4 Morphological Analysis"""

#
#
# Morphological Analysis
#
#

!pip install polyglot        
!pip install pyicu           
!pip install Morfessor       
!pip install pycld2        
import polyglot
from polyglot.text import Text, Word
# !polyglot download morph2.wo
import random


val = input("Enter input file : ")
path_to_file = "/content/drive/My Drive/results/" + val
file = write_word_file_object = open(path_to_file, "r")

def generate_n_grams(n , list_ngrams):
  ngram = ""
  ctr = 1
  for line in file:
    line = line.rstrip()
    if line == "," or line == ":" or line == "?" or line == "." or line == "'" or line == "\"" or line =="(" or line == ")":
      continue
    ngram += line
    ngram += " "
    if ctr < n:
      ctr += 1
    else:
      list_ngrams.append(ngram.strip())
      ngram = ngram.split(' ',1)[1]

def frequency_distribution ( list_ngrams ):
  frequency = {} 
  rank = {}
  for ngram in list_ngrams:  
    if ngram in frequency:
      frequency[ngram] += 1
    else:
      frequency[ngram] = 1
    
  rank_ngrams = sorted(frequency.items(), key = 
                      lambda kv:(kv[1], kv[0]) , reverse = True)
  print("We first attempt Morphological analysis of 5 random words from 100 most frequent words: ")
  # for i in range(5):
  #   random_index = random.randint(1,1000)
    
  #   word = rank_ngrams[random_index][0]
  #   freq = rank_ngrams[random_index][1]

  #   print("Random Chosen Word: " + word +" with a frequency of: " + str(freq))
    
  #   morph = Text(word).words[0]
  #   # for morpheme in morph.morphemes:
  #   #   print(morpheme + " ")
  #   print(morph.morphemes)

  print("We now perform Morphological analysis of 5 random words from 100 least frequent words: ")
  i = 0
  while i < 5:
    print(i)
    random_index = random.randint(1,100000)
    
    word = rank_ngrams[-1 * random_index][0]
    flag = 1
    
    print(word)
    for j in word:
      if not (j.isalpha()) and j!= '\'' and j!='-':
        flag = 0
        break
    
    if word[0] == '\'' or word[-1] == '\'':
      flag = 0
    if flag == 0:
      continue
    i+=1
    
    freq = rank_ngrams[-1 * random_index][1]

    print("Random Chosen Word: " + word +" with a frequency of: " + str(freq))
    morph = Text(word).words[0]
    # for morpheme in morph.morphemes:
    #   print(morpheme + " ")
    print(morph.morphemes)

unigram_list = [] 

generate_n_grams(1, unigram_list)

frequency_ngram_list = {}
rank_ngram_list = {}

frequency_distribution(unigram_list)

"""# Question 5 Byte-Pair Encoding"""

val = input("Enter input file : ")
path_to_file = "/content/drive/My Drive/results/" + val
file = open(path_to_file, "r")

vocab = {}
for line in file:
  line = line.split("_")
  # print(line)
  if len(line)!=2:
    continue
  line[0] = line[0].rstrip()
  line[1] = line[1].rstrip()
  word = line[0]
  freq = int(line[1])
  vocab[word] = freq



frequency_sub_tokens = {}

for key,value in vocab.items():
	sub_tokens = key.split()
	for sub_token in sub_tokens:
		if sub_token in frequency_sub_tokens:
			frequency_sub_tokens[sub_token] += value
		else:
			frequency_sub_tokens[sub_token] = value

rank_sub_tokens = sorted(frequency_sub_tokens.items(), key = 
										lambda kv:(kv[1], kv[0]) , reverse = True)


while 1:
  index = 0
  val_with_sub_tokens = ""
  val = input("Enter word (press e to exit): ")
  for char in val:
    val_with_sub_tokens += char
    val_with_sub_tokens += " "
  val_with_sub_tokens += "~"
  if val=="e":
    break
  while index < len(rank_sub_tokens):
    temp = val_with_sub_tokens.split()
    l = 0
    is_sub_token_found=0
    merged_sub_tokens = ""
    temp_copy = []
    for value in temp:
      if l == 0 :
        l+=1
        merged_sub_tokens += value
        continue
      merged_sub_tokens += value
      
      if is_sub_token_found==0 and merged_sub_tokens == rank_sub_tokens[index][0]:
        temp_copy.append(merged_sub_tokens)
        is_sub_token_found = 1
        merged_sub_tokens = ""
        l=0
      else:
        merged_sub_tokens = merged_sub_tokens[:-1*len(value)]
        temp_copy.append(merged_sub_tokens)
        merged_sub_tokens = value
    temp_copy.append(merged_sub_tokens)
    if (is_sub_token_found==1):
      is_sub_token_found=0
      index=0
    else:
      index+=1
    val_with_sub_tokens = " ".join(temp_copy)

  print(val_with_sub_tokens)